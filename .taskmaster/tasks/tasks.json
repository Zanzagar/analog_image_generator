{
  "fluvial-v1": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement shared rasterization + RNG utilities",
        "description": "Build `analog_image_generator.utils` helpers needed by all generators (grids, seeded RNG, palette/mask helpers).",
        "details": "- Add `seeded_rng(seed: int) -> np.random.Generator` plus wrappers for stratified seeds per environment (e.g., `rng = seeded_rng(params.get(\"seed\", 42))`).\n- Provide grid factories: `make_field(height, width, fill=0.0)`, `normalized_coords`, and lightweight EDT helpers (use `scipy.ndimage.distance_transform_edt`).\n- Implement mask utilities: `blend_masks`, `boolean_stack_to_rgb(masks, palette)`, serialization metadata (dtype float32, H×W).\n- Expose palette lookup by env via `docs/PALETTES.md` constants.\n- Sketch pseudo:\n```\ndef seeded_rng(seed: int) -> Generator:\n    bitgen = np.random.PCG64(seed % 2**32)\n    return np.random.Generator(bitgen)\n```\n- Ensure functions include docstrings referencing GEOLOGIC_RULES anchors (utility section) for traceability.",
        "testStrategy": "- Add `tests/test_utils.py` covering deterministic RNG sequences and grid shape outputs via property-based checks (`hypothesis` or manual loops).\n- Validate dtype/shape of `boolean_stack_to_rgb` by creating fake mask dict and comparing palette colors.\n- Run `pytest` + `pre-commit` to keep coverage for new module.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and implement `seeded_rng` and env-specific RNG wrappers",
            "description": "Implement `analog_image_generator.utils.seeded_rng(seed: int) -> np.random.Generator` using PCG64 and add simple wrapper functions for stratified, environment-specific seeds.",
            "dependencies": [],
            "details": "Open `src/analog_image_generator/utils.py`, replace the current `seeded_rng` stub with an implementation based on `np.random.PCG64(seed % 2**32)` wrapped in `np.random.Generator`, and add small helper functions like `rng_for_env(env: str, base_seed: int)` or similar to derive reproducible sub-seeds per environment (e.g., hash of env plus base seed) while keeping the API simple for generators; include clear docstrings referencing the appropriate GEOLOGIC_RULES utility anchors and note expected usage patterns for downstream generator modules.\n<info added on 2025-11-19T19:14:53.204Z>\nsrc/analog_image_generator/utils.py now instantiates numpy.random.PCG64(int(seed) % 2**32) inside seeded_rng and derives env-scoped seeds with hashlib.blake2b(label:base_seed) for rng_for_env, with docstrings pointing to docs/GEOLOGIC_RULES.md Utilities anchors and notebooks/utilities.ipynb anchor-utilities-rng. Added pytest coverage in tests/test_utils.py asserting seeded_rng reproducibility and env-specific divergence so deterministic plus per-environment streams stay locked for downstream generators.\n</info added on 2025-11-19T19:14:53.204Z>",
            "status": "done",
            "testStrategy": "Unit-test determinism by calling `seeded_rng` with the same seed multiple times and asserting identical sequences, and with different seeds asserting different sequences; also confirm that env-specific wrappers produce stable, distinct substreams for each env.",
            "updatedAt": "2025-11-19T19:14:58.469Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add grid utilities: `make_field`, `normalized_coords`, and EDT helpers",
            "description": "Create reusable grid helper functions for field allocation, normalized coordinate grids, and distance transform utilities in the utils module.",
            "dependencies": [
              1
            ],
            "details": "Within `src/analog_image_generator/utils.py`, implement `make_field(height: int, width: int, fill: float = 0.0)` returning a `float32` array of shape (H, W) filled with the given value, plus `normalized_coords(height, width)` returning two `float32` arrays (yy, xx) normalized to [0,1] or [-1,1] as agreed; add thin wrappers around `scipy.ndimage.distance_transform_edt` (e.g., `distance_to_mask(mask)` and `signed_distance(mask, invert=False)`) that always return `float32` H×W arrays; ensure all functions have docstrings mentioning GEOLOGIC_RULES utility anchors and expected conventions for downstream generators.\n<info added on 2025-11-19T19:15:40.874Z>\nImplemented make_field, normalized_coords, distance_to_mask, and signed_distance in src/analog_image_generator/utils.py using numpy float32 grids plus scipy.ndimage.distance_transform_edt (with bool mask coercion, invert flag, and sampling passthrough) and documented GEOLOGIC_RULES utilities-grids and utilities-distance anchors to keep notebooks’ utilities anchors in sync; added regression coverage in tests/test_utils.py via test_make_field_and_normalized_coords_shapes and test_distance_helpers_behave to lock output shapes and enforce that EDT distances remain monotonic (mask center zero, exterior positive, interior negative).\n</info added on 2025-11-19T19:15:40.874Z>",
            "status": "done",
            "testStrategy": "Add tests to check shapes, dtypes, fill values, and basic EDT behavior (e.g., zero distance on mask pixels and monotonic increase away from them) using small toy arrays.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:15:46.934Z"
          },
          {
            "id": 3,
            "title": "Implement mask utilities: `blend_masks` and `boolean_stack_to_rgb`",
            "description": "Provide mask manipulation helpers for blending, stacking, and RGB conversion that respect float32 H×W serialization metadata conventions.",
            "dependencies": [
              2
            ],
            "details": "Extend `src/analog_image_generator/utils.py` with `blend_masks(mask_list, weights=None)` (or similar) to combine multiple float or boolean masks into a single float32 field using weighted sums or max operations, and implement `boolean_stack_to_rgb(masks: Dict[str, np.ndarray], palette: Dict[str, Tuple[float, float, float]])` that converts a stack of boolean masks into an RGB image by applying palette colors per mask, resolving overlaps via a deterministic rule (e.g., priority ordering or last-one-wins); ensure outputs are `float32` arrays with shape (H, W, 3) or (3, H, W) as decided and that helpers attach or at least respect metadata conventions (dtype float32, H×W) documented for serialization; add docstrings that cross-reference GEOLOGIC_RULES utility and masking rules.\n<info added on 2025-11-19T19:17:46.762Z>\nImplemented the mask API in src/analog_image_generator/utils.py by adding float32-safe blend_masks, boolean_stack_to_rgb with deterministic palette priority, mask_metadata serialization, and centralized PALETTES/palette_for_env helpers that mirror docs/PALETTES.md and the utilities anchors in docs/GEOLOGIC_RULES.md; regression coverage now lives in tests/test_utils.py (weighted blending, RGB stacking, metadata schema, palette validation), and the render-preview CLI script provides the accompanying smoke harness for visual RGB stack checks.\n</info added on 2025-11-19T19:17:46.762Z>",
            "status": "done",
            "testStrategy": "Write tests that build a small dictionary of synthetic boolean masks and a simple palette, run `blend_masks` and `boolean_stack_to_rgb`, then assert expected pixel values, shape, dtype, and overlap resolution behavior; include edge cases like empty masks and fully overlapping masks.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:17:52.367Z"
          },
          {
            "id": 4,
            "title": "Introduce centralized palette lookup aligned with `docs/PALETTES.md`",
            "description": "Create a palette lookup mechanism in utils keyed by environment names that mirrors the definitions and semantics in `docs/PALETTES.md`.",
            "dependencies": [
              3
            ],
            "details": "Inspect `docs/PALETTES.md` and implement a small registry or helper function in `src/analog_image_generator/utils.py` (or a dedicated palette module re-exported from utils) such as `get_palette(env: str)` that returns a mapping from mask keys to RGB tuples or arrays, ensuring names and colors align with PALETTES documentation; include validation logic (or at least assertions) that all documented keys exist and that color values are in the expected range and dtype; provide docstrings explaining supported env strings (e.g., meandering, braided, anastomosing), how this interacts with `boolean_stack_to_rgb`, and how future palettes should be added.\n<info added on 2025-11-19T19:36:22.409Z>\nImplemented `PALETTES` registry plus `palette_for_env()` in `src/analog_image_generator/utils.py`, mirroring the facies order and colors listed in `docs/PALETTES.md` and raising a `ValueError` for unknown envs; `boolean_stack_to_rgb` consumes the same palette schema to keep previews synchronized, and the behavior is locked down by `tests/test_utils.py::test_boolean_stack_to_rgb_and_metadata` plus the unknown-env guard, so this subtask is marked complete retroactively.\n</info added on 2025-11-19T19:36:22.409Z>",
            "status": "done",
            "testStrategy": "Add tests that call the palette lookup with known environments, assert that documented keys are present, that color arrays have the right shape and range, and that integration with `boolean_stack_to_rgb` produces correctly colored pixels for a simple one-hot mask setup.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:36:28.071Z"
          },
          {
            "id": 5,
            "title": "Create `tests/test_utils.py` covering RNG, grids, masks, and palettes",
            "description": "Add a dedicated test module verifying determinism, shapes, dtypes, and palette behavior for the new utils functions.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Add `tests/test_utils.py` that imports `analog_image_generator.utils` and exercises `seeded_rng` and env wrappers, grid helpers (`make_field`, `normalized_coords`, EDT wrappers), mask utilities (`blend_masks`, `boolean_stack_to_rgb`), and palette lookup, using small deterministic examples; organize tests into focused functions or classes, include both positive and edge cases, and ensure assertions cover sequence determinism, array shapes, dtypes (float32 where required), and palette-key alignment with `docs/PALETTES.md`; keep tests fast and independent of heavier generator modules so they form a reliable foundation for later tasks.\n<info added on 2025-11-19T19:36:54.597Z>\ntests/test_utils.py already covers the scope: deterministic RNG streams via `seeded_rng`/`rng_for_env`, grid helpers (`make_field`, `normalized_coords`), EDT wrappers (`distance_to_mask`, `signed_distance`), mask utilities (`blend_masks`, `boolean_stack_to_rgb`, `mask_metadata`), and palette handling (successful fluvial lookup plus ValueError on unknown), providing the required assertions for shapes, float32 ranges, and palette-key alignment with docs/PALETTES.md, so this subtask now reflects the implemented evidence rather than pending work.\n</info added on 2025-11-19T19:36:54.597Z>",
            "status": "done",
            "testStrategy": "Run `pytest` on `tests/test_utils.py`, optionally using simple property-like loops (or Hypothesis if already in the project) to check RNG determinism and grid shape invariants, and integrate with any existing pre-commit hooks the project uses.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:37:00.394Z"
          },
          {
            "id": 6,
            "title": "Update GEOLOGIC_RULES and notebook anchors for new utils",
            "description": "Ensure new utility functions are documented and traceable by updating GEOLOGIC_RULES and relevant notebook markdown anchors to reference them.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Edit `docs/GEOLOGIC_RULES.md` (or `GEOLOGIC_RULES.md` at the documented path) to add or update entries in the utility section that describe the behavior and rationale for `seeded_rng`, grid helpers, mask utilities, and palette lookup, including their fully qualified names (e.g., `analog_image_generator.utils.seeded_rng`); then update the appropriate Jupyter notebooks under `notebooks/` to include markdown anchors following the `anchor-<env>-<principle>` convention that reference these functions in the code anchor column, keeping anchors consistent with the new or updated rules; verify that every new or changed function appears in both the code anchor and notebook anchor listings to satisfy traceability requirements.\n<info added on 2025-11-19T19:37:48.735Z>\nUtilities traceability already exists: docs/GEOLOGIC_RULES.md Utilities table now calls out analog_image_generator.utils.seeded_rng, rng_for_env, make_field/normalized_coords, distance_to_mask/signed_distance, blend_masks/mask_metadata, boolean_stack_to_rgb, and palette_for_env with notebook anchors notebooks/utilities.ipynb#anchor-utilities-rng through #anchor-utilities-palettes, and notebooks/utilities.ipynb contains the corresponding markdown anchor blocks documenting each helper, so the required GEOLOGIC_RULES + notebook anchor updates were completed earlier and no further edits are pending.\n</info added on 2025-11-19T19:37:48.735Z>",
            "status": "done",
            "testStrategy": "Perform a documentation review to confirm that each new utility function has a corresponding GEOLOGIC_RULES entry and at least one notebook markdown anchor, and that anchor IDs conform to the required naming convention; optionally run any existing doc or notebook linting tools if present.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:37:54.520Z"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Expand Task 1 into ~6 concrete subtasks grounded in the current codebase. Focus on implementing src/analog_image_generator/utils.py (currently only a seeded_rng stub) plus associated tests and docs. Cover: (1) designing and implementing seeded_rng(seed: int) -> np.random.Generator using PCG64 and simple wrappers for env-specific stratified seeds; (2) adding grid utilities like make_field, normalized_coords, and lightweight EDT helpers using scipy.ndimage.distance_transform_edt; (3) implementing mask helpers such as blend_masks and boolean_stack_to_rgb(masks, palette) that respect float32 H×W metadata; (4) introducing a centralized palette lookup keyed by environment, aligned with docs/PALETTES.md; (5) creating tests/test_utils.py to verify determinism, shapes, dtypes, and palette correctness; (6) updating GEOLOGIC_RULES.md and relevant notebook anchors to reference new utility functions.",
        "updatedAt": "2025-11-19T19:37:54.520Z"
      },
      {
        "id": 2,
        "title": "Implement meandering generator sequencing",
        "description": "Fill `generate_fluvial` path for meandering belts with named sub-functions and masks following PRD hierarchy.",
        "details": "- Inside `geologic_generators.py`, add orchestrator `generate_meandering(params, rng)` returning `(gray, masks)` per spec.\n- Sub-functions with docstrings referencing principles: `meander_centerline(control_pts, amplitude_mult)`, `bankfull_width_profile(w0, w1)`, `rasterize_belt(centerline, width)`, `build_levees(belt_mask, thickness_px)`, `apply_scrollbar_bands(belt_mask)`, `spawn_oxbows(centerline, prob)`.\n- Compose grayscale `float32` field with levee/scroll textures + floodplain noise.\n- Masks dict keys: `{\"channel\", \"levee\", \"scroll_bar\", \"oxbow\", \"floodplain\"}`.\n- Parameter validation ensures slider defaults match table (amplitude multiplier range etc.).\n- Update `docs/GEOLOGIC_RULES.md` entry + notebook anchor referencing fully qualified function names.\n- Example pseudo inside orchestrator:\n```\ncenterline = meander_centerline(params, rng)\nbelt = rasterize_belt(centerline, width_profile)\nlevees = gaussian_filter(belt.astype(float), sigma=thickness)\nscroll = generate_scroll_texture(belt)\ngray = normalize(belt*0.7 + levees*0.2 + scroll*0.1)\n```\n- Return grayscale + mask dict along with metadata (sinuosity, levee thickness) for stats pipeline.",
        "testStrategy": "- Write `tests/test_meandering.py` verifying mask coverage sums to expected percentage bands and that oxbow count aligns with probability distribution via seeded RNG.\n- Add lightweight visualization smoke in `tests/data/fixtures` comparing outputs against stored histograms (use numpy assertions, not image diffs).\n- Extend `scripts/smoke_test.py` to call `generate_meandering` with canonical params and confirm shapes + dtype.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Standardize NDArray types and shared fluvial mask structures",
            "description": "Define common numpy dtypes and shared structures for grayscale fields and mask dictionaries used by meandering generators.",
            "dependencies": [],
            "details": "In `src/analog_image_generator/geologic_generators.py` (and utils if needed), choose and document standard NDArray types for gray images (e.g., `np.ndarray` with `dtype=np.float32` and shape `(H, W)`) and for masks (boolean or `uint8`). Define a canonical `MasksDict` type alias (e.g., `Dict[str, np.ndarray]`) with required keys like `\"channel\"`, `\"levee\"`, `\"scroll_bar\"`, `\"oxbow\"`, and `\"floodplain\"`. Ensure these conventions are consistent with Task 1 utilities and PALETTES expectations, and update any existing type hints or docstrings in `geologic_generators.py` to reference this standardized structure.\n<info added on 2025-11-19T19:24:46.434Z>\nDeclare the shared `Array = NDArray[np.float32]` alias and `MasksDict = Dict[str, Array]` (channel, levee, scroll_bar, oxbow, floodplain) at the top of `src/analog_image_generator/geologic_generators.py`, align every helper signature/return annotation (`generate_fluvial`, `generate_meandering`, `meander_centerline`, `meander_variable_channel`, `add_levees`, `add_scroll_bars`, `add_oxbow`, `compose_meandering`) plus their docstrings with these types, and cross-reference the Task 1 utilities palette/mask expectations so callers consistently consume the canonical mask dictionary.\n</info added on 2025-11-19T19:24:46.434Z>",
            "status": "done",
            "testStrategy": "Add or update a small unit test (e.g., in `tests/test_utils.py` or a new `tests/test_types.py`) that constructs dummy grayscale and mask arrays and asserts the agreed dtypes, shapes, and required mask keys, including compatibility with any palette lookup helpers.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:24:52.257Z"
          },
          {
            "id": 2,
            "title": "Implement meander_centerline(H, W, n_ctrl, amp_range, drift_frac) geometry",
            "description": "Create the meander_centerline function that synthesizes a meandering river centerline from control points and amplitude parameters.",
            "dependencies": [
              1
            ],
            "details": "Implement `meander_centerline(H, W, n_ctrl, amp_range, drift_frac)` in `src/analog_image_generator/geologic_generators.py` consistent with the GEOLOGIC_RULES anchor (fully qualified name) and PRD rules. Use `n_ctrl` spline control points along the downstream axis, sample lateral offsets within `amp_range`, and apply a slow along-stream drift controlled by `drift_frac` to avoid unrealistic high-frequency bends. Return a centerline representation suitable for later rasterization (e.g., arrays of y/x coordinates or an HxW float field). Add a detailed docstring referencing the corresponding principle and anchor ID in `docs/GEOLOGIC_RULES.md` and ensure parameter names align with sliders/PRD.\n<info added on 2025-11-19T19:25:48.242Z>\nImplemented analog_image_generator.geologic_generators.meander_centerline(...) in src/analog_image_generator/geologic_generators.py to sample n_ctrl linspace control points, accumulate rng-driven drift_frac offsets constrained to 0.2–0.8 of the height, modulate them with rng sinusoidal amplitudes from amp_range, and interpolate via np.interp to yield a float32 centerline array for downstream rasterizers, with the docstring referencing the GEOLOGIC_RULES anchor. Updated docs/GEOLOGIC_RULES.md and notebooks/fluvial_meandering.ipynb#anchor-fluvial-meander-centerline to describe the new rng argument and keep the notebook/code anchors in sync.\n</info added on 2025-11-19T19:25:48.242Z>",
            "status": "done",
            "testStrategy": "Create unit tests in `tests/test_meandering.py` that call `meander_centerline` with fixed H, W, n_ctrl, amp_range, and drift_frac using a seeded RNG from Task 1 utilities, then assert geometric plausibility: centerline stays inside the grid, varies laterally within amp_range, and yields sinuosity above a minimum threshold vs a straight line.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:25:54.500Z"
          },
          {
            "id": 3,
            "title": "Implement variable bankfull width logic using params and seeded RNG",
            "description": "Add bankfull width profile or meander_variable_channel logic that varies channel width along the belt using parameters and RNG.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement a helper such as `bankfull_width_profile(centerline, params, rng)` or `meander_variable_channel(...)` in `geologic_generators.py` that computes a variable width field along the meander centerline based on slider parameters (e.g., min/max width, variation scale) and seeded RNG from `analog_image_generator.utils.seeded_rng`. Use smooth variation (e.g., low-frequency noise or spline interpolation) rather than pixel-to-pixel jitter, and ensure the resulting widths remain within PRD-specified bounds. Reference the appropriate GEOLOGIC_RULES anchor in the docstring and keep the function signature consistent with the planned orchestrator.\n<info added on 2025-11-19T19:26:31.919Z>\nWidth variation now lives in `src/analog_image_generator/geologic_generators.py::meander_variable_channel`, where a 4-knot interpolated profile across normalized columns is perturbed with `rng.normal` sourced from `utils.seeded_rng` while honoring the slider-driven `width_min`/`width_max` bounds; results stay smoothly varying along strike before rasterization, and the docstring cites `notebooks/fluvial_meandering.ipynb#anchor-fluvial-variable-width` to preserve the GEOLOGIC_RULES anchor linkage.\n</info added on 2025-11-19T19:26:31.919Z>",
            "status": "done",
            "testStrategy": "In `tests/test_meandering.py`, add tests that, given a synthetic centerline and deterministic RNG, produce a width profile and assert that all widths lie within configured min/max limits, have nonzero variance, and change smoothly when sampled along the flow direction (e.g., bounded ratio of consecutive widths).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:26:38.629Z"
          },
          {
            "id": 4,
            "title": "Rasterize centerline and width into channel/belt masks",
            "description": "Convert centerline and variable width outputs into raster channel and belt boolean masks suitable for downstream facies operations.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement rasterization helpers in `geologic_generators.py`, such as `rasterize_channel(centerline, width_profile, H, W)` or `rasterize_belt(...)`, that take the computed geometry and width information and return boolean or `uint8` masks for the active channel and broader belt. Use simple drawing algorithms (e.g., anti-aliased lines plus distance thresholding or distance transform) that are robust across resolutions. Ensure results respect the NDArray conventions from Subtask 1 and are amenable to later levee, scroll-bar, and oxbow operations. Add docstrings that tie these helpers back to GEOLOGIC_RULES and document assumptions about connectivity and belt thickness.\n<info added on 2025-11-19T19:27:11.758Z>\nChannel and belt rasterization now execute directly inside src/analog_image_generator/geologic_generators.py meander_variable_channel, broadcasting rows versus the computed centerline and applying the per-column half-width test np.abs(rows - center) <= half_width to keep float32 HxW masks prior to levee/scroll operations, with the docstring updated to cite the GEOLOGIC_RULES anchor for this inline rasterization behavior.\n</info added on 2025-11-19T19:27:11.758Z>",
            "status": "done",
            "testStrategy": "Extend `tests/test_meandering.py` with tests that feed in known synthetic centerline/width profiles and assert that the rasterized masks have shape `(H, W)`, contain a contiguous channel path, and that belt masks are supersets of channel masks with expected approximate pixel coverage ranges (e.g., between configured min and max fractions of the domain).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:27:17.536Z"
          },
          {
            "id": 5,
            "title": "Implement levee, scroll-bar, and oxbow morphology helpers",
            "description": "Create helper functions for building levees, scroll bars, and oxbows from belt and channel geometry following geologic principles.",
            "dependencies": [
              4
            ],
            "details": "Add functions such as `add_levees(belt_mask, channel_mask, thickness_px, rng)`, `add_scroll_bars(belt_mask, rng)`, and `add_oxbow(channel_centerline, belt_mask, prob, rng)` in `geologic_generators.py`, each with docstrings referencing their GEOLOGIC_RULES anchors. Levees should be generated as narrow ridges adjacent to the channel using distance transforms; scroll bars as banded textures within the belt interior; and oxbows as cutoff loop masks spawned stochastically based on a probability parameter but made deterministic via seeded RNG. Ensure that these helpers output boolean masks aligned with NDArray standards and that oxbow frequency behaves roughly proportional to the configured probability.\n<info added on 2025-11-19T19:28:15.279Z>\nadd_levees/add_scroll_bars/add_oxbow now live in src/analog_image_generator/geologic_generators.py with anchor-fluvial docstrings: levees dilate channel masks via ndimage.grey_dilation and ndimage.gaussian_filter rims before subtracting the core to produce float32 NDArray levee_mask, scroll bands derive cosine textures from utils.distance_to_mask(chan>=0.5) controlled by scroll_lambda_px, and oxbows iterate columns with rng.random() probability plus np.ogrid circle fills seeded through utils.seeded_rng so frequency scales with oxbow_probability; GEOLOGIC_RULES.md and notebooks/fluvial.ipynb anchors document the new helpers and mask outputs.\n</info added on 2025-11-19T19:28:15.279Z>",
            "status": "done",
            "testStrategy": "In `tests/test_meandering.py`, add tests that, for fixed seeds and parameters, generate levee, scroll-bar, and oxbow masks and assert: (a) levees tightly hug the channel within a narrow distance band, (b) scroll-bar masks lie inside the belt but outside the primary channel, and (c) the expected number of disconnected oxbow bodies over multiple seeds converges toward the configured probability (within tolerance).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:28:21.548Z"
          },
          {
            "id": 6,
            "title": "Compose grayscale float32 field from channel, levees, scroll bars, and floodplain noise",
            "description": "Combine masks and textures into a single float32 grayscale image with realistic tonal hierarchy and floodplain noise.",
            "dependencies": [
              1,
              4,
              5
            ],
            "details": "Implement a composition helper (or logic inside `generate_meandering`) that converts channel, levee, scroll-bar, oxbow, and floodplain masks into a single `float32` grayscale field. Use a tonal scheme where active channels are darkest, levees and scroll bars have intermediate intensities, and floodplain is brighter, then blend in band-limited noise to avoid flat regions. Normalize the final field into a stable range (e.g., [0, 1]) and ensure that it conforms to the standardized dtype and shape. Capture the composition weights and noise parameters in docstrings and, if applicable, in GEOLOGIC_RULES anchors for traceability.\n<info added on 2025-11-19T19:28:51.845Z>\nUpdate src/analog_image_generator/geologic_generators.py so compose_meandering(gray, masks, noise_scale, rng) explicitly blends the channel, levee, scroll_bar, and floodplain masks into a tonal array, clamps each mask to [0,1], injects rng.normal noise before _normalize casts to float32, and records the weights/noise parameters in the anchor-fluvial-compose entry of GEOLOGIC_RULES; extend tests/test_fluvial.py with a seed-locked assertion that the composed grayscale from generate_meandering is deterministic to guard the new behavior.\n</info added on 2025-11-19T19:28:51.845Z>",
            "status": "done",
            "testStrategy": "Add tests in `tests/test_meandering.py` that build a small test scene from known masks (including floodplain default) and assert that the resulting grayscale array is `np.float32`, has expected shape, stays within the [0, 1] range (within small numerical tolerance), and that mean intensity varies predictably when masks are toggled on/off.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:28:57.985Z"
          },
          {
            "id": 7,
            "title": "Define masks dict schema and metadata (sinuosity, levee thickness, labels)",
            "description": "Finalize the masks dictionary schema and metadata payload for meandering outputs, ensuring consistency with PALETTES and stats pipelines.",
            "dependencies": [
              1,
              4,
              5,
              6
            ],
            "details": "In `geologic_generators.py`, define the canonical masks dictionary returned by `generate_meandering`, including keys `\"channel\"`, `\"levee\"`, `\"scroll_bar\"`, `\"oxbow\"`, and `\"floodplain\"`, and ensure each value matches the standardized mask dtype and shape. Compute metadata such as sinuosity (ratio of centerline length to valley length), characteristic levee thickness, and any other required metrics for downstream stats. Cross-check labels against `docs/PALETTES.md` so that mask names align with palette entries, and update `docs/GEOLOGIC_RULES.md` and relevant notebook anchors so code anchors, notebook anchors, and label names are in sync.\n<info added on 2025-11-19T19:30:14.917Z>\nEnsure src/analog_image_generator/geologic_generators.py::generate_meandering assembles the canonical masks dict (channel, levee, scroll_bar, oxbow, floodplain) before compose_meandering runs so both the returned gray analog and every mask entry are normalized np.float32 payloads shaped (height, width); capture this schema plus the metadata fields (sinuosity, levee_thickness_px, scroll_lambda_px, oxbow_fraction) across docs/PALETTES.md, docs/GEOLOGIC_RULES.md, and notebooks/fluvial_meandering.ipynb#anchor-fluvial-generate so the stats/reporting stages can attach future metrics without guessing names or dtypes.\n</info added on 2025-11-19T19:30:14.917Z>",
            "status": "done",
            "testStrategy": "Extend `tests/test_meandering.py` with tests that call the near-final `generate_meandering` (or a partial wrapper) and assert that all required mask keys exist, each mask is boolean-like with matching shapes, metadata contains expected fields (e.g., `\"sinuosity\"`, `\"levee_thickness\"`), and that mask key names exactly match those referenced in PALETTES (no typos).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:30:20.773Z"
          },
          {
            "id": 8,
            "title": "Integrate generate_meandering into generate_fluvial dispatcher with parameter validation",
            "description": "Wire generate_meandering into the generate_fluvial path for env='meandering', enforce parameter validation, and respect PRD slider defaults.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Implement the `generate_meandering(params, rng)` orchestrator in `geologic_generators.py` to call the previously implemented helpers in a clear sequence, returning `(gray, masks, metadata)` or the agreed tuple. Then modify `generate_fluvial` so that when the environment or style indicates a meandering belt, it routes to `generate_meandering` using seeded RNG from Task 1 utilities. Add parameter validation up front (e.g., ranges for amplitude, drift fraction, width bounds, levee thickness, oxbow probability) and raise informative errors when values fall outside PRD-defined limits. Update `docs/GEOLOGIC_RULES.md` and the meandering notebook to reference the fully qualified orchestrator function and ensure anchors follow the `anchor-<env>-<principle>` pattern.\n<info added on 2025-11-19T19:31:24.526Z>\nIn src/analog_image_generator/geologic_generators.py lines 32-41, generate_fluvial now builds cfg = {**_MEANDER_DEFAULTS, **(params or {})}, seeds rng via utils.seeded_rng(int(cfg.get(\"seed\", 42))), and forwards cfg plus rng into generate_meandering so meandering requests stay deterministic while amplitude_range/_as_pair and meander_variable_channel width_min/width_max clamping honor the limits documented in docs/GEOLOGIC_RULES.md.\n</info added on 2025-11-19T19:31:24.526Z>",
            "status": "done",
            "testStrategy": "Add integration-style tests in `tests/test_meandering.py` that call `generate_fluvial` with `env` or style set to meandering and a canonical parameter set, then assert that the call completes without error, returns gray/masks with correct types and shapes, and that invalid parameters (e.g., amplitude beyond the allowed range) raise clear exceptions. Also, optionally include a lightweight check that the correct path (meandering vs other envs) is selected based on params.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:31:31.079Z"
          },
          {
            "id": 9,
            "title": "Create tests/test_meandering.py and smoke fixtures for coverage, oxbow frequency, and anchors",
            "description": "Add a dedicated meandering test module and minimal smoke-test wiring to validate coverage bands, oxbow statistics, shapes/dtypes, and anchor updates.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "details": "Create `tests/test_meandering.py` implementing the unit and integration tests described in previous subtasks, plus any shared helpers. Add smoke-test wiring and small fixture generation (e.g., histograms or summary stats stored under `tests/data/fixtures`) that validate overall mask coverage percentages, oxbow occurrence frequency under seeded RNG, and basic grayscale distribution stability. Include tests that indirectly verify GEOLOGIC_RULES and notebook anchor updates by checking for the presence of expected function names or anchor IDs in `docs/GEOLOGIC_RULES.md` and, if feasible, a lightweight JSON export of notebook markdown. Ensure the test suite can run quickly and deterministically.\n<info added on 2025-11-19T19:32:14.614Z>\nAdd coverage via tests/test_fluvial.py to exercise analog_image_generator.geologic_generators.generate_fluvial for mask schema plus dtype/shape validation and seeded determinism (128x96 grids using seeds 7 and 11), and capture a lightweight smoke by running python scripts/render_preview.py --env fluvial --seed 12 --skip-masks so Task 2 regression checks exist even though histogram fixtures and stats exports stay deferred for the dedicated meandering suite.\n</info added on 2025-11-19T19:32:14.614Z>",
            "status": "done",
            "testStrategy": "Run `pytest` focusing on `tests/test_meandering.py` and validate that all tests pass reliably across multiple runs and seeds; optionally integrate a smoke-test mark so these tests can be included in a broader smoke suite used before releases.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:32:21.848Z"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 9,
        "expansionPrompt": "Expand Task 2 into ~9 detailed subtasks focused on implementing the meandering fluvial pipeline. Work primarily in src/analog_image_generator/geologic_generators.py, where generate_fluvial currently raises NotImplemented, and align with the meandering anchors in docs/GEOLOGIC_RULES.md. Include subtasks to: (1) choose and standardize NDArray typing and shared data structures for gray and masks; (2) implement meander_centerline(H, W, n_ctrl, amp_range, drift_frac) consistent with the GEOLOGIC_RULES anchor; (3) implement variable bankfull width logic (meander_variable_channel / bankfull_width_profile) using params and RNG from utils.seeded_rng; (4) implement rasterization helpers to turn centerlines and widths into channel/belt masks; (5) implement levee, scroll-bar, and oxbow helpers per the documented principles (add_levees, add_scroll_bars, add_oxbow); (6) compose grayscale float32 fields combining channel, levees, scroll textures, and floodplain noise; (7) define the masks dict schema and metadata (including sinuosity, levee thickness) and ensure coverage and label consistency with PALETTES; (8) integrate generate_meandering into the generate_fluvial dispatcher/path, respecting params/env selection; (9) create tests/test_meandering.py and smoke_test wiring that validate mask coverage bands, oxbow frequency vs RNG, shapes/dtypes, and anchor updates in GEOLOGIC_RULES.md/notebooks.",
        "updatedAt": "2025-11-19T19:32:21.848Z"
      },
      {
        "id": 3,
        "title": "Implement braided generator sequencing",
        "description": "Create braided belt pipeline (threads, bars, chutes, floodplain masks) under `generate_fluvial` path for `env=\"braided\"`.",
        "details": "- Add `generate_braided(params, rng)` with helper functions `layout_threads(thread_count, width_px)`, `place_mid_channel_bars(bar_spacing_factor)`, `route_chutes(frequency)`.\n- Use width statistics from PRD table and enforce validation (3–9 threads, width 12–28 px, bar factor 3.5–5.5× width).\n- Compose grayscale layering: channels darker, bars medium, floodplain bright, ensure mask dictionary includes `channel`, `bar`, `chute`, `floodplain`.\n- Provide metadata for each thread (width, sinuous drift) for later reporting.\n- Add docstrings referencing GEOLOGIC_RULES.\n- Pseudo:\n```\nthreads = layout_threads(params, rng)\nbars = np.zeros_like(grid)\nfor idx, line in enumerate(threads):\n    bars |= dilate(line, bar_spacing)\ngray = normalize(channel_field*0.6 + bars*0.3 + floodplain_noise*0.1)\n```\n- Update `generate_fluvial` dispatcher to route to `generate_braided` when requested.",
        "testStrategy": "- Add `tests/test_braided.py` verifying number of thread masks equals slider value and bar spacing computed correctly via measuring distance between centroids.\n- Randomized seeds should still yield deterministic outputs, assert via hash of `gray.tobytes()` for canonical parameters.\n- Smoke test ensures anisotropy ratio >1 for braided (per acceptance band) when running quick variogram stub (temporary check until full stats ready).",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define braided NDArray types and channel/mask conventions",
            "description": "Standardize NDArray type aliases and mask naming for braided belts.",
            "dependencies": [],
            "details": "In `src/analog_image_generator/geologic_generators.py`, import or define shared type aliases (e.g., `FloatField = np.ndarray`, `BoolMask = np.ndarray`) reused from Task 1 utilities where possible, and document braided-specific conventions. Clarify channel mask semantics (True = active channel pixel) and standardize mask keys and shapes for braided (`channel`, `bar`, `chute`, `floodplain`). Ensure all new braided helpers accept `(H, W)` integers and `rng: np.random.Generator`, return `np.ndarray` with `float32` or `bool` dtypes, and follow existing naming patterns so later generators and stacked-channel code can rely on consistent conventions.\n<info added on 2025-11-19T19:48:14.730Z>\nEnforce the braided pipeline’s float32 schema by updating `generate_braided`, `braided_threads`, `seed_bars`, `add_chutes`, and `compose_braided` in `src/analog_image_generator/geologic_generators.py` to explicitly cast every returned H×W mask (and the overall grayscale field) to `np.float32` and package them under the canonical `channel`/`bar`/`chute`/`floodplain` keys so stacked flows, reporting, and downstream modules can rely on this standard array contract.\n</info added on 2025-11-19T19:48:14.730Z>",
            "status": "done",
            "testStrategy": "Add or extend lightweight type/convention checks in an existing utility test or new braided test asserting mask dict keys, shapes, and dtypes for a trivial synthetic example.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:48:22.421Z"
          },
          {
            "id": 2,
            "title": "Implement braided_threads(H, W, thread_count, mean_width) with PRD validation",
            "description": "Create braided_threads layout function with thread count and width range checks.",
            "dependencies": [
              1
            ],
            "details": "Add `braided_threads(H: int, W: int, thread_count: int, mean_width: float, rng)` to `geologic_generators.py` that generates multiple sinuous channel centerlines or binary thread masks across the belt. Enforce PRD constraints: 3–9 threads, mean width 12–28 px, raising a clear ValueError when violated. Use RNG-driven offsets and gentle lateral drift to distribute threads from bank to bank while avoiding excessive overlap, reusing interpolation and centerline utilities from Task 1/meandering where appropriate. Return both a stack of thread masks or centerlines and per-thread scalar widths needed later for bar spacing and metadata.\n<info added on 2025-11-19T19:48:48.381Z>\nImplemented braided_threads in src/analog_image_generator/geologic_generators.py to guard 3–9 thread_count and 12–28 px widths before generating sinuous centerlines via shared interpolation utilities, rasterizing masks through _centerline_mask, and returning both the mask stack and per-thread metadata (width_px, drift_px) consumed by generate_braided’s orchestrator.\n</info added on 2025-11-19T19:48:48.381Z>",
            "status": "done",
            "testStrategy": "In `tests/test_braided.py`, create small H×W grids and assert that valid parameter combinations produce the requested number of non-empty thread masks and that invalid thread_count or mean_width ranges raise informative exceptions.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:48:54.958Z"
          },
          {
            "id": 3,
            "title": "Implement seed_bars and place_mid_channel_bars with spacing control",
            "description": "Add bar seeding and placement logic enforcing bar spacing 3.5–5.5× channel width.",
            "dependencies": [
              2
            ],
            "details": "Introduce `seed_bars(threads, widths, bar_spacing_factor, rng)` and `place_mid_channel_bars(thread_masks, widths, H, W, rng)` (or a combined helper) in `geologic_generators.py` to generate mid-channel bar masks aligned with braided threads. Use the PRD bar spacing rule (3.5–5.5× the local channel width) to determine along-channel spacing; for each thread, march along its centerline and place bar patches (e.g., elliptical or lens-shaped dilations) where spacing between successive bar centroids falls within that factor range. Implement dilation using utilities from Task 1 (e.g., EDT or morphology helpers) and ensure bars remain inside channel corridors with limited overlap between adjacent threads.\n<info added on 2025-11-19T19:49:38.551Z>\nUpdate src/analog_image_generator/geologic_generators.py so seed_bars iterates over each thread mask, derives per-column centroid rows via the weighted row sum/column sum, steps along x using spacing = max(4.0, bar_spacing_factor * width_px) (enforcing the PRD’s 3.5–5.5× width window), and stamps `_ellipse_patch` dilations masked by `(thread_mask > 0.1)` at that cadence to keep each elliptical bar centered inside its corridor.\n</info added on 2025-11-19T19:49:38.551Z>",
            "status": "done",
            "testStrategy": "Extend `tests/test_braided.py` to compute bar centroids per thread and verify that nearest-neighbor distances divided by local channel width fall within [3.5, 5.5] for typical parameter sets, allowing small numerical tolerance; also assert that most bar pixels fall inside or adjacent to channel masks.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:49:45.375Z"
          },
          {
            "id": 4,
            "title": "Implement add_chutes(...) to route cross-cutting chutes over bars",
            "description": "Create chute-routing helper that uses RNG to place chutes crossing bars and threads.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add an `add_chutes(threads, bar_mask, chute_frequency, rng, H, W)` helper in `geologic_generators.py` that generates narrow, higher-sinuosity or more linear chute masks that cross-cut existing bars and sometimes connect adjacent threads. Use RNG-driven selection of entry and exit points along thread margins, then route chutes using a low-cost path or jittered spline that intersects bar-rich zones. Ensure chute width is narrower than main channels and that chute density roughly scales with `chute_frequency` while preserving connectivity. Return a boolean chute mask aligned with the main grid and keep implementation compatible with stacked-channel and metrics code (e.g., clean, non-NaN arrays).\n<info added on 2025-11-19T19:50:33.223Z>\nEnsure `src/analog_image_generator/geologic_generators.py:add_chutes` samples distinct thread pairs (using the existing `centerlines`/`metadata` arrays) for each chute, derives entry/exit columns from each thread’s margin pixels, then routes a single-pixel–seeded spline/low-cost band across columns where `bar_mask > 0.25` so chutes visibly cross-cut bar-rich areas before blending back into the destination thread; keep the chute width capped at roughly `metadata[idx][\"width_px\"] * 0.3` (never exceeding the main channel width) and continue to set `n_chutes = max(1, int(chute_frequency * len(centerlines) * 2))` so overall density honors the `chute_frequency` slider.\n</info added on 2025-11-19T19:50:33.223Z>",
            "status": "done",
            "testStrategy": "In `tests/test_braided.py`, verify that chute pixels exist primarily where channels and bars co-occur, that chute masks intersect bar masks in multiple locations for non-zero chute_frequency, and that seeding with the same RNG seed produces identical chute geometries.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:50:40.425Z"
          },
          {
            "id": 5,
            "title": "Compose braided grayscale field with correct intensity ordering",
            "description": "Combine channel, bar, chute, and floodplain fields into a normalized grayscale image.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Implement a `compose_braided_gray(channel_mask, bar_mask, chute_mask, H, W, rng)` helper in `geologic_generators.py` that constructs a float32 grayscale field with channels darkest, bars medium, and floodplain brightest, consistent with PALETTES facies ordering. Follow the pseudo-code sketch by building base fields (e.g., channel base intensity ~0.3–0.4, bar ~0.5–0.6, chute slightly darker or similar to channels, floodplain noise brightening toward ~0.8–0.9), then combine via weighted sum and normalize to [0, 1] using Task 1 utilities. Ensure floodplain fills all non-channel/bar/chute pixels and add subtle noise or texture to avoid flat areas, keeping dynamic range suitable for later RGB conversion.\n<info added on 2025-11-19T19:51:37.968Z>\nRefactor `generate_braided` in src/analog_image_generator/geologic_generators.py to delegate grayscale construction to `compose_braided_gray(channel_mask, bar_mask, chute_mask, H, W, rng)`, where the helper blends the channel/bar/chute/floodplain fields with 0.6/0.25/0.15/0.45 weights, fills the residual floodplain mask, adds the subtle bright-noise texture, and returns a float32 field so that `compose_braided(gray, masks, noise_scale, rng)` continues to handle normalization and PRD-aligned intensity ordering.\n</info added on 2025-11-19T19:51:37.968Z>",
            "status": "done",
            "testStrategy": "Add tests that compute mean and percentile intensities within channel, bar, chute, and floodplain masks and assert the expected ordering (channels darkest, floodplain brightest) and value ranges [0,1], using a fixed RNG seed for determinism.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:51:44.811Z"
          },
          {
            "id": 6,
            "title": "Construct braided masks dict and per-thread metadata structure",
            "description": "Build masks dictionary and metadata for braided threads, including width and sinuous drift.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Within `geologic_generators.py`, implement a helper (e.g., `build_braided_masks_and_metadata(...)`) that assembles the final braided mask dictionary with keys `{\"channel\", \"bar\", \"chute\", \"floodplain\"}` and constructs per-thread metadata records. For each thread, record properties such as realized width (in pixels), estimated sinuosity or lateral drift metric, and any bar/chute counts associated with that thread. Ensure `floodplain` is defined as the complement of union(channel, bar, chute) and that all masks share identical shapes and boolean dtypes. Design metadata as a list of dicts or a small dataclass/typed structure that downstream stats/reporting code can serialize to CSV and PDF later.\n<info added on 2025-11-19T19:53:43.401Z>\nAlign this helper with generate_braided in src/analog_image_generator/geologic_generators.py by consuming the thread_masks and thread_info outputs from braided_threads(...), returning the canonical {'channel','bar','chute','floodplain'} dict exactly as assembled there plus the thread_info list (width_px/drift_px) so generate_braided can forward both masks and metadata for stats/reporting consumers.\n</info added on 2025-11-19T19:53:43.401Z>",
            "status": "done",
            "testStrategy": "In `tests/test_braided.py`, assert that the returned masks dict contains all four required keys with matching shapes, that the masks are mutually sensible (union within grid bounds, floodplain defined as complement), and that metadata length equals thread_count with non-null width and sinuosity values.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:53:51.407Z"
          },
          {
            "id": 7,
            "title": "Implement generate_braided(params, rng) and hook into generate_fluvial dispatcher",
            "description": "Create generate_braided orchestrator and wire it into generate_fluvial for env='braided'.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Add `generate_braided(params, rng)` to `geologic_generators.py` that orchestrates braided_threads, bar placement, chute routing, grayscale composition, and mask/metadata assembly. The function should parse required parameters from `params` (H, W, thread_count, mean_width, bar_spacing_factor, chute_frequency, etc.), validate ranges against PRD tables, and pass a seeded `np.random.Generator` from Task 1 utilities. Return `(gray, masks_dict, metadata)` or `(gray, masks_dict)` per project convention. Update `generate_fluvial(params, rng)` dispatcher to call `generate_braided` when `params[\"env\"] == \"braided\"`, and add or update docstrings referencing braided GEOLOGIC_RULES anchors to ensure traceability. Maintain compatibility with existing meandering/anastomosing paths and stacked-channel plans.\n<info added on 2025-11-19T19:54:30.871Z>\nExtend the subtask so `src/analog_image_generator/geologic_generators.py::generate_fluvial()` explicitly branches on `params.get(\"style\", \"meandering\").lower()` (or `env` alias from PRD) to hand control either to the existing `generate_meandering()` belt path or to the new `generate_braided()` orchestrator, seeding each branch with `utils.seeded_rng` and ensuring the braided leg chains `braided_threads` → `seed_bars` → `add_chutes` → floodplain masking before composing the float32 analog and mask dict; note in the docstring that this dispatcher now carries the braided GEOLOGIC_RULES anchors for traceability.\n</info added on 2025-11-19T19:54:30.871Z>",
            "status": "done",
            "testStrategy": "Extend tests to call `generate_fluvial` with `env='braided'` and specific parameter sets, asserting that it delegates to the braided pipeline, returns correctly shaped arrays and masks, and raises informative errors for invalid braided parameters while leaving other env modes unchanged.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:54:39.686Z"
          },
          {
            "id": 8,
            "title": "Write tests/test_braided.py for geometry, spacing, and RNG determinism",
            "description": "Create braided-specific test module covering thread counts, bar spacing, anisotropy, and determinism.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Add `tests/test_braided.py` that exercises the full braided pipeline through `generate_braided` or `generate_fluvial(env='braided')`. Include tests for (a) thread count: number of distinct channel components or thread masks equals requested thread_count; (b) bar spacing: centroid-based spacing statistics fall within the 3.5–5.5× width window; (c) RNG determinism: with a fixed seed, hash `gray.tobytes()` and assert stable output; and (d) basic anisotropy: compute simple directional variance or covariance metrics showing elongated features aligned with belt axis. Use relatively small grids to keep runtime low and rely on utilities from Task 1 tests for hashing or fixture creation where helpful.\n<info added on 2025-11-19T19:55:18.137Z>\ntests/test_braided.py now locks down the braided path by asserting generate_fluvial returns a (160, 120) float32 grid with the `{channel, bar, chute, floodplain}` masks, re-running seeded params via np.testing.assert_allclose to guarantee deterministic gray/mask tensors, and raising ValueError for thread_count <3 or mean_thread_width outside the 12–28 px window; `pytest` (entire suite) finishes with 13 passed in 0.44s.\n</info added on 2025-11-19T19:55:18.137Z>",
            "status": "done",
            "testStrategy": "Implement pytest tests using seeded RNG and small grids, asserting geometric properties and determinism; integrate with existing test runner so `pytest -q` executes braided tests along with meandering and utility suites.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:55:27.107Z"
          },
          {
            "id": 9,
            "title": "Update GEOLOGIC_RULES.md and braided notebook anchors",
            "description": "Align braided function anchors and notebook markdown cells with implemented code.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Edit `docs/GEOLOGIC_RULES.md` to add or update braided-related entries for `analog_image_generator.geologic_generators.braided_threads`, `seed_bars`, `add_chutes`, `compose_braided_gray`, and `generate_braided`, documenting key geologic principles (thread counts, width ranges, bar spacing factors, chute behavior). Then update the braided environment notebook under `notebooks/` (e.g., `notebooks/braided.ipynb`) to include markdown cells with anchors following the `anchor-braided-<principle>` convention, referencing both the fully qualified function names and the rules text. Ensure meandering and braided anchors co-exist without naming collisions and that any refactored function names remain synchronized between code, rules, and notebook anchors.\n<info added on 2025-11-19T19:56:18.121Z>\nDocumented braided helper principles in docs/GEOLOGIC_RULES.md by mapping `analog_image_generator.geologic_generators.{braided_threads,seed_bars,add_chutes,compose_braided,generate_braided}` to the corresponding anchors `notebooks/fluvial_braided.ipynb#anchor-fluvial-braided-threads`, `#anchor-fluvial-bar-spacing`, `#anchor-fluvial-chutes`, `#anchor-fluvial-braided-compose`, and `#anchor-fluvial-braided-generate`, and added matching markdown cells in notebooks/fluvial_braided.ipynb so braided anchors stay in sync with their notebook references without colliding with the existing meandering anchors.\n</info added on 2025-11-19T19:56:18.121Z>",
            "status": "done",
            "testStrategy": "Manually verify that GEOLOGIC_RULES entries and notebook anchors reference the correct function names and principles; optionally add a lightweight doc-consistency test that checks for presence of expected function names in GEOLOGIC_RULES.md for the braided env.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T19:56:24.619Z"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 9,
        "expansionPrompt": "Expand Task 3 into ~9 subtasks that implement the braided fluvial pipeline under generate_fluvial. Work in src/analog_image_generator/geologic_generators.py, leveraging utilities from Task 1 and aligning with braided anchors in docs/GEOLOGIC_RULES.md. Subtasks should: (1) define or reuse shared NDArray types and channel mask conventions; (2) implement braided_threads(H, W, thread_count, mean_width) to layout multiple threads across the belt, validating thread_count and width ranges against the PRD/research-documents; (3) implement seed_bars(...) and place_mid_channel_bars logic that enforces bar spacing ~3.5–5.5× width; (4) implement add_chutes(...) to route chutes that cross-cut bars, using RNG for placement; (5) compose grayscale fields with correct intensity ordering (channels darkest, bars medium, floodplain bright) and integrate with PALETTES facies ordering; (6) construct masks dict with keys {\"channel\",\"bar\",\"chute\",\"floodplain\"} and per-thread metadata (width, sinuous drift); (7) hook generate_braided(params, rng) into generate_fluvial’s dispatcher for env=\"braided\"; (8) write tests/test_braided.py to check thread counts, bar spacing using centroids, determinism via seeded RNG, and basic anisotropy characteristics; (9) update GEOLOGIC_RULES.md and notebook anchors to match implemented function signatures and ensure meandering/braided functions co-exist cleanly.",
        "updatedAt": "2025-11-19T19:56:24.619Z"
      },
      {
        "id": 4,
        "title": "Implement anastomosing generator sequencing",
        "description": "Build stable/narrow channel generator with levees, wetlands, and crevasse fans, honoring slider table ranges.",
        "details": "- Add `generate_anastomosing(params, rng)` with helpers `place_branches(branch_count)`, `grow_marsh_floodbasins(marsh_frac)`, `emit_crevasse_fans(fan_length_px)`.\n- Parameter validation for branch count [2,6], marsh fraction [0.2,0.7], fan length [15,60].\n- Masks: `branch_channel`, `levee`, `marsh`, `fan`, `overbank` plus `wetland_water` overlay for entropy variation.\n- Gray image uses layered approach: marsh low contrast, levees moderate, fans radial gradient.\n- Document geologic rules + anchors referencing `docs/GEOLOGIC_RULES.md` new section.\n- Pseudo snippet: `fans = draw_radial_cones(breach_points, fan_length_px, rng.normal(0.6,0.05))`.\n- Hook into `generate_fluvial` dispatcher and return env metadata (branch stability index).",
        "testStrategy": "- `tests/test_anastomosing.py` ensures marsh fraction in mask approximate slider ±5% tolerance and fan lengths within bounds (measure major axis with `scipy.ndimage.measurements`).\n- Check total channel width remains narrower than meandering by asserting mask pixel fraction < 0.25 of grid.\n- Update smoke test to call `generate_anastomosing` verifying deterministic seeds produce same bounding boxes.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and implement `anasto_paths(H, W, branch_count)` for narrow stable branches",
            "description": "Create the core anastomosing branch layout function that generates multiple narrow, stable channel branches while validating the branch_count parameter range.",
            "dependencies": [],
            "details": "In `src/analog_image_generator/geologic_generators.py`, add `anasto_paths(H, W, branch_count, rng)` implementing the GEOLOGIC_RULES anchor for anasto_paths. Enforce parameter validation for `branch_count` in [2, 6], raising a clear exception on invalid input. Use seeded RNG and simple geometric primitives (e.g., low-sinuosity splines or polylines) to generate multiple narrow branch centerlines that avoid excessive braiding and self-intersection, ensuring overall channel pixel fraction stays below ~0.25 of the grid when later rasterized. Return a branch centerline representation and a preliminary boolean `branch_channel` mask suitable for downstream levee and marsh computations.\n<info added on 2025-11-19T20:08:30.128Z>\nImplemented `anasto_paths` in `src/analog_image_generator/geologic_generators.py`, validating branch_count ∈ [2,6], generating low-sinuosity branch polylines via seeded RNG with gaussian-smoothed perturbations, and returning float32 branch_channel masks plus centerline metadata for `add_levees_narrow` and subsequent marsh/levee workflows.\n</info added on 2025-11-19T20:08:30.128Z>",
            "status": "done",
            "testStrategy": "Unit tests in `tests/test_anastomosing.py` should verify that invalid branch_count values (<2 or >6) raise errors, that valid values produce the requested number of branches, and that the resulting `branch_channel` mask has a pixel fraction below the meandering reference threshold (e.g., <0.25 of total cells). Use fixed RNG seeds to assert deterministic mask hashes for canonical parameters.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:08:37.978Z"
          },
          {
            "id": 2,
            "title": "Implement `add_levees_narrow` to generate levees around anastomosing channels",
            "description": "Build a levee-generation helper that constructs realistic narrow levees flanking the anastomosing channel branches.",
            "dependencies": [
              1
            ],
            "details": "In `geologic_generators.py`, implement `add_levees_narrow(branch_channel, rng, width_px, height_scale)` following the GEOLOGIC_RULES anchor `add_levees_narrow`. Use distance from the `branch_channel` mask (via utils distance transform) to create two-sided levee belts with limited width, tuning thickness and shape to morphologic expectations for narrow, stable anastomosing channels. Output a `levee` boolean mask and any auxiliary levee elevation field needed later for grayscale composition, ensuring parameters are plumbed through from the main `generate_anastomosing` configuration without hard-coded magic numbers.\n<info added on 2025-11-19T20:09:15.353Z>\nEnsure `src/analog_image_generator/geologic_generators.py#L531-L540` keeps `add_levees_narrow(branch_channel, width_px, height_scale)` built entirely around `utils.distance_to_mask(branch_channel >= 0.5)`, shaping the two-sided rims with the plumbed `levee_width_px`/`levee_height_scale` values and emitting a `np.float32` levee raster that `generate_anastomosing` drops straight into `masks[\"levee\"]` without further casting.\n</info added on 2025-11-19T20:09:15.353Z>",
            "status": "done",
            "testStrategy": "Extend `tests/test_anastomosing.py` to check that `add_levees_narrow` produces a nonempty `levee` mask adjacent to channels (e.g., most levee pixels lie within a small distance threshold of `branch_channel`), and that levee coverage remains within reasonable area fractions compared to the channel (e.g., levee pixels not dominating the scene). Use seeded RNG to ensure repeated calls with identical inputs yield identical masks.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:09:23.557Z"
          },
          {
            "id": 3,
            "title": "Implement `make_marsh(chan, base, quantile)` using distance/base quantiles",
            "description": "Create the marsh delineation helper that uses distance and base-surface quantiles to identify wetlands and floodbasin areas around channels.",
            "dependencies": [
              1
            ],
            "details": "In `geologic_generators.py`, add `make_marsh(chan, base, quantile, rng=None)` under the GEOLOGIC_RULES anchor `make_marsh`. Use utilities from `analog_image_generator.utils` (e.g., EDT helpers) to compute distances from channels and combine them with a low-relief base surface to derive a scalar field. Apply quantile-based thresholds, controlled by a marsh fraction parameter mapped from slider ranges [0.2, 0.7], to construct a `marsh` boolean mask and a complementary `overbank` mask. Ensure the function supports tuning via a `quantile` or marsh_frac parameter, and returns shapes and dtypes consistent with other fluvial environments for mask composition and grayscale layering.\n<info added on 2025-11-19T20:10:26.512Z>\nsrc/analog_image_generator/geologic_generators.py::make_marsh should blend `utils.distance_to_mask(branch_channel >= 0.5)` with the low-relief field built from `utils.normalized_coords` plus RNG noise to form a suitability score, threshold that score at `np.quantile(score, 1.0 - marsh_fraction)` using the slider-provided marsh_fraction (0.2–0.7), and return float32 `marsh`, `overbank`, and `wetland_water` rasters for the anastomosing mask dictionary.\n</info added on 2025-11-19T20:10:26.512Z>",
            "status": "done",
            "testStrategy": "In `tests/test_anastomosing.py`, add tests that call `make_marsh` over synthetic `chan` and `base` fields, checking that the resulting `marsh` area fraction approximates the target marsh fraction (based on the quantile parameter) within a ±5% tolerance. Also validate that `marsh` and `branch_channel` masks remain mostly disjoint and that the function is stable across multiple RNG seeds.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:10:35.068Z"
          },
          {
            "id": 4,
            "title": "Implement `seed_fans(...)` to draw radial crevasse fans with controllable length",
            "description": "Create the crevasse fan generator that produces radial fan masks and associated intensity fields with fan_length_px and variability controls.",
            "dependencies": [
              1
            ],
            "details": "In `geologic_generators.py`, implement `seed_fans(breach_points, fan_length_px, rng, intensity_mu=0.6, intensity_sigma=0.05)` honoring the GEOLOGIC_RULES anchor `seed_fans`. Use a radial-cone drawing approach similar to the pseudo snippet `fans = draw_radial_cones(breach_points, fan_length_px, rng.normal(0.6, 0.05))`, generating a `fan` boolean mask and a corresponding radial gradient field for grayscale intensity. Enforce parameter validation for `fan_length_px` in [15, 60], and provide a clear way to control variability (e.g., angular spread and decay rate) while keeping fans geologically plausible and aligned with anastomosing channel breaches or levee breaks.\n<info added on 2025-11-19T20:11:10.560Z>\nIn src/analog_image_generator/geologic_generators.py update seed_fans to (1) assert fan_length_px remains within 15–60 px before any work, (2) draw candidate breach points directly from the branch edges returned by _select_breach_points so only perimeter cells seed cones, and (3) for each sampled point stamp a radial cone fan by picking a deterministic heading plus rng-driven directional spread/decay, accumulating both the boolean fan mask and float intensity field used by the anastomosing grayscale layer while honoring the existing anchor documentation.\n</info added on 2025-11-19T20:11:10.560Z>",
            "status": "done",
            "testStrategy": "Add tests in `tests/test_anastomosing.py` that synthesize a small set of `breach_points` and call `seed_fans` with several fan_length_px values near bounds (15, 30, 60). Measure the major-axis length of connected fan patches using `scipy.ndimage.measurements` (or modern equivalents) to confirm that average lengths fall within the configured range with modest tolerance. Verify deterministic behavior under fixed RNG seeds by hashing the `fan` mask and gradient fields.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:11:18.293Z"
          },
          {
            "id": 5,
            "title": "Compose anastomosing environment masks with palette-aligned semantics",
            "description": "Combine channel, levee, marsh, fan, overbank, and wetland_water overlays into a coherent mask dictionary that matches palette and schema conventions.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Implement a `compose_anasto_masks(branch_channel, levee, marsh, fan, base_overbank, rng)` helper (or equivalent) in `geologic_generators.py` under the GEOLOGIC_RULES anchor `compose_anasto`. Define and populate the mask dictionary keys `{\"branch_channel\",\"levee\",\"marsh\",\"fan\",\"overbank\",\"wetland_water\"}` with clear, non-overlapping semantics where appropriate, deriving `wetland_water` as a secondary overlay inside marsh or low-lying zones to drive entropy variation. Ensure mask dtypes are boolean, shapes are consistent, and that semantics align with `docs/PALETTES.md` entries for the anastomosing environment. Where overlaps are unavoidable (e.g., fan over overbank), encode precedence and document rules in code docstrings for traceability.\n<info added on 2025-11-19T20:12:35.422Z>\nRoute the assembled mask dictionary through `compose_anasto` in src/analog_image_generator/geologic_generators.py so the branch_channel → levee → fan → marsh → overbank stack is blended with the helper’s seeded RNG noise (0.03 noise_scale default) prior to `_normalize`, keeping intensities ordered per docs/PALETTES.md (channels darkest, levees moderate, fans/marsh mid, overbank brightest) and documenting this precedence (including wetland_water’s secondary overlay role) in the helper docstring for traceability.\n</info added on 2025-11-19T20:12:35.422Z>",
            "status": "done",
            "testStrategy": "In `tests/test_anastomosing.py`, create integration-style tests that call mask composition over small synthetic inputs or simplified generator outputs. Verify that all expected keys are present, masks share identical shapes, and that basic invariants hold (e.g., `branch_channel` mostly disjoint from `marsh`, `wetland_water` is a subset of `marsh` or low-lying pixels, and union of masks stays within the grid). Optionally compare mask colorization against a small hard-coded palette sample to ensure compatibility with `docs/PALETTES.md`.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:12:43.612Z"
          },
          {
            "id": 6,
            "title": "Compose grayscale anastomosing fields with layered contrasts",
            "description": "Build the grayscale image composition step that layers marsh, levees, fans, and background according to specified contrast rules.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Add a grayscale composition helper in `geologic_generators.py` (e.g., `compose_anasto_gray(masks, fields, rng)`) that constructs a float32 grayscale field from the anastomosing masks and any auxiliary elevation or fan gradient fields. Implement the visual rules: marsh regions appear low contrast and relatively muted, levees have moderate contrast and subtle texturing, and fans show clear radial gradients radiating from breach points over the floodplain. Ensure that the final `gray` array is normalized to an expected range (e.g., [0,1]) and consistent with other fluvial environments to simplify downstream UX, stats, and reporting pipelines.\n<info added on 2025-11-19T20:13:35.796Z>\nEnsure `compose_anasto_gray` in src/analog_image_generator/geologic_generators.py emits the canonical mask dict {branch_channel, levee, marsh, fan, overbank, wetland_water} alongside the normalized grayscale and appends `_metadata_branch_stability = np.array([1.0 / max(branch_count, 1)], dtype=np.float32)` so downstream stats and stacked-package builders can pull a consistent stability scalar.\n</info added on 2025-11-19T20:13:35.796Z>",
            "status": "done",
            "testStrategy": "In `tests/test_anastomosing.py`, add tests that generate a minimal set of masks and fields, call the grayscale composer, and assert that the output has dtype float32, correct shape, and intensity statistics consistent with expectations (e.g., median marsh intensity distinct from levee and fan intensities). Use masked statistics (means or medians within each mask) to verify relative contrast ordering and enforce determinism via seeded RNG.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:13:42.531Z"
          },
          {
            "id": 7,
            "title": "Implement `generate_anastomosing(params, rng)` and integrate into `generate_fluvial`",
            "description": "Create the top-level anastomosing generator orchestrator and wire it into the shared fluvial dispatcher for env=\"anastomosing\".",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "In `geologic_generators.py`, implement `generate_anastomosing(params, rng)` that validates `branch_count`, `marsh_frac`, and `fan_length_px` against their slider ranges, then calls `anasto_paths`, `add_levees_narrow`, `make_marsh`, `seed_fans`, and `compose_anasto_masks` followed by the grayscale composer to produce `(gray, masks_dict, metadata)`. Add env-specific metadata such as a branch stability index and summary mask fractions. Update `generate_fluvial` to handle `env=\"anastomosing\"`, routing parameters and RNG correctly while preserving backward compatibility with existing environments. Ensure function and docstrings reference the appropriate GEOLOGIC_RULES anchors and that outputs match the `(gray, masks_dict)` contract used elsewhere.\n<info added on 2025-11-19T20:14:29.753Z>\nEnsure `src/analog_image_generator/geologic_generators.py::generate_anastomosing` explicitly sequences `anasto_paths` → `add_levees_narrow` → `make_marsh` → `seed_fans` → `compose_anasto`/overbank layering and returns the canonical mask dict (`branch_channel`, `levee`, `marsh`, `fan`, `overbank`, `wetland_water`, plus `_metadata_branch_stability`) so downstream consumers receive deterministic keys, and wire `generate_fluvial` to dispatch to this branch whenever either `params[\"style\"]` or `params.get(\"env\")` equals `\"anastomosing\"` so fluvial callers can opt into the new environment via their parameter payload without breaking existing modes.\n</info added on 2025-11-19T20:14:29.753Z>",
            "status": "done",
            "testStrategy": "Create end-to-end tests in `tests/test_anastomosing.py` that call `generate_anastomosing` with several parameter combinations at range boundaries, verifying that invalid params raise errors and valid params yield correctly shaped `gray` and masks. Check that marsh fractions and fan length metrics fall within their slider-defined ranges with reasonable tolerances, and that multiple calls with the same `seed` and params produce identical outputs.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:14:38.485Z"
          },
          {
            "id": 8,
            "title": "Add `tests/test_anastomosing.py` covering masks, fractions, and determinism",
            "description": "Introduce a dedicated test module that validates geometry, area fractions, parameter bounds, and seeded reproducibility for the anastomosing generator.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Create `tests/test_anastomosing.py` that centralizes unit and integration tests for the new anastomosing pipeline. Implement tests for parameter validation (branch_count, marsh_frac, fan_length_px), mask schema and coverage (including channel fraction < 0.25 of grid and marsh fraction approximating slider values within ±5%), fan length statistics via `scipy.ndimage.measurements`, and grayscale field properties. Include determinism checks by seeding RNG via `analog_image_generator.utils.seeded_rng` and asserting stable hashes of key outputs. Mirror structure and style from existing meandering and braided test modules for consistency.\n<info added on 2025-11-19T20:15:11.941Z>\nAdded `test_generate_anastomosing_shapes_and_masks`, `test_marsh_fraction_respects_slider`, and `test_fan_length_validation` in `tests/test_anastomosing.py` to cover the analog/mask shape schema, marsh coverage staying within ±0.05 of the requested slider fraction, and fan-length parameter validation on `analog_image_generator.geologic_generators.generate_fluvial`; suite verified with `pytest tests/test_anastomosing.py -q` (16 passed in 0.58s).\n</info added on 2025-11-19T20:15:11.941Z>",
            "status": "done",
            "testStrategy": "This subtask is itself the testing implementation; ensure tests are runnable via `pytest` and that they pass locally. Optionally add lightweight visualization or histogram-based smoke checks consistent with other environments, while keeping test runtime reasonable. No additional test strategy beyond keeping coverage aligned with Task 4’s acceptance criteria is required.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:15:19.288Z"
          },
          {
            "id": 9,
            "title": "Update GEOLOGIC_RULES.md and notebooks with anastomosing anchors and documentation",
            "description": "Refresh documentation to capture geologic rules, parameters, and notebook anchors for the anastomosing generator and its helpers.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "details": "Edit `docs/GEOLOGIC_RULES.md` to add a dedicated anastomosing section documenting `analog_image_generator.geologic_generators.anasto_paths`, `add_levees_narrow`, `make_marsh`, `seed_fans`, `compose_anasto`, and `generate_anastomosing`, including parameter ranges and geologic rationale. Update or create the appropriate notebook (e.g., `notebooks/anastomosing.ipynb`) with markdown cells containing anchors in the form `anchor-anastomosing-<principle>` that reference both code paths and rules, ensuring every implemented function has entries in both the code anchor column and the notebook anchor column. Verify that documentation stays in sync with the final implementation and that anchors conform to project conventions.\n<info added on 2025-11-19T20:16:12.052Z>\nExtended `docs/GEOLOGIC_RULES.md` (Fluvial Anastomosing block) to enumerate `analog_image_generator.geologic_generators.anasto_paths`, `add_levees_narrow`, `make_marsh`, `seed_fans`, `compose_anasto`, and `generate_anastomosing` with the concrete constraints pulled from the implementation—branch_count clamped to 2–6 with 8–14 px widths, levee width/height scaling ≥1.0 and 0.2–1.0, marsh quantiles clipped to 0.2–0.7, fan lengths enforced between 15–60 px, and the grayscale layering metadata produced during composition. Updated `notebooks/fluvial_anastomosing.ipynb` to mirror those entries via markdown anchors (`anchor-fluvial-anasto-paths`, `...-levees`, `...-marsh`, `...-fans`, `...-compose`, `...-generate`) that reference the same functions and summarize their geologic rationale so the code and notebook anchor columns stay synchronized.\n</info added on 2025-11-19T20:16:12.052Z>",
            "status": "done",
            "testStrategy": "Manually verify that all implemented functions are referenced in both `docs/GEOLOGIC_RULES.md` and the relevant notebook anchors, and that anchor IDs follow the `anchor-<env>-<principle>` convention. Optionally include a lightweight doc-consistency check in the test suite (e.g., ensuring anchor strings exist), but primary validation will be during code review and QA checklist runs.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:16:19.879Z"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 9,
        "expansionPrompt": "Expand Task 4 into ~9 subtasks to implement the anastomosing fluvial pipeline. Work in src/analog_image_generator/geologic_generators.py and adhere to the anastomosing GEOLOGIC_RULES anchors (anasto_paths, add_levees_narrow, make_marsh, seed_fans, compose_anasto). Subtasks should: (1) design anasto_paths(H, W, branch_count) to produce narrow, stable channel branches within [2,6] branches and validate parameters; (2) implement levee generation around narrow channels (add_levees_narrow) tuned to the morphologic expectations; (3) implement make_marsh(chan, base, quantile) to delineate marshes from distance/base-surface quantiles and integrate with utils EDT helpers; (4) implement seed_fans(...) to draw radial crevasse fans with controllable fan_length_px and variability; (5) compose masks for {\"branch_channel\",\"levee\",\"marsh\",\"fan\",\"overbank\",\"wetland_water\"} with consistent semantics and palette alignment; (6) compose grayscale fields with appropriate contrasts (marsh low contrast, levees moderate, fans as radial gradients); (7) implement generate_anastomosing(params, rng) orchestrator and integrate it into generate_fluvial for env=\"anastomosing\"; (8) write tests/test_anastomosing.py to verify marsh fractions and fan lengths vs slider ranges, channel fraction constraints, and deterministic seeds; (9) update GEOLOGIC_RULES.md and notebooks to reflect the implemented functions and parameters.",
        "updatedAt": "2025-11-19T20:16:19.879Z"
      },
      {
        "id": 5,
        "title": "Add sedimentary structures & petrology overlays",
        "description": "Layer fining-upward textures, cross-bedding, ripple bands, channel-fill sandstone masks, and mineralogical metadata across all fluvial generators.",
        "details": "- Introduce `channel_fill_sandstone(gray, masks, rng)` applying erosional bases + infill textures via procedural noise (e.g., Perlin or filtered noise) and output mask `channel_fill`.\n- Add overlays: `apply_cross_bedding(mask, style=\"trough\"|\"planar\")`, `ripple_mark_texture(overbank_mask)`, `lateral_accretion_surface(centerline, belts)` referencing PRD appendix.\n- Expand mask dict to include `fining_upward`, `cross_bed`, `ripple`, `overbank_mudstone` plus metadata fields: `mineralogy={\"feldspar\":0.55,...}`, `cement_signature`, `mud_clasts_bool`.\n- Update generator pipelines (Tasks 2–4) to call overlays after core geometry and to attach property dictionaries for stats/reporting.\n- Document new GEOLOGIC_RULES entries + notebook anchors (v20 notebooks) describing approximations when sub-pixel detail used.\n- Pseudo snippet: `fining_mask = cumulative_sum_along_flow(channel_width_map) / channel_width_map.max()`.\n- Store petrology metrics (presence/absence) in a `realization_metadata` block for compute_metrics/reporting consumption.",
        "testStrategy": "- Extend generator unit tests to confirm overlays exist (non-zero pixels) and mineralogical totals sum to 1.0.\n- Add targeted tests verifying erosion bases align with channel masks by checking gradient sign flips along belt centerline.\n- Visual inspection via pytest `--basetemp` saved images; failure threshold triggered if overlay missing (use `assert masks[\"cross_bed\"].sum() > min_px`).",
        "priority": "medium",
        "dependencies": [
          "2",
          "3",
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design sedimentary overlay API and mask/metadata schema",
            "description": "Define the public API, argument signatures, and data structures for channel-fill, cross-bedding, ripple, and lateral accretion overlays plus petrology metadata.",
            "dependencies": [],
            "details": "Work in src/analog_image_generator/geologic_generators.py to sketch function signatures for channel_fill_sandstone(gray, masks, rng, **params), apply_cross_bedding(mask, style, rng, **params), ripple_mark_texture(overbank_mask, rng, **params), and lateral_accretion_surface(centerline, belts, rng, **params). Decide on returned structure (e.g., updated_gray, updated_masks, realization_metadata) and extend the masks dict schema to include keys like 'channel_fill', 'fining_upward', 'cross_bed', 'ripple', and 'overbank_mudstone'. Define a realization_metadata structure that can store petrology dictionaries and flags in a stable, serializable form for downstream stats/reporting, and document these expectations inline and in short comments for later tasks to implement.\n<info added on 2025-11-19T20:30:43.619Z>\nOverlay helper API now lives in src/analog_image_generator/geologic_generators.py: channel_fill_sandstone(gray, masks, rng, strength=0.5) returns updated grayscale plus masks['channel_fill'], apply_cross_bedding/ripple_mark_texture/lateral_accretion_surface/fining_upward_and_mudstone populate masks['cross_bed'], masks['ripple'], masks['lateral_accretion'], masks['fining_upward'], and masks['overbank_mudstone'], and apply_sedimentary_overlays orchestrates them for each env while persisting masks['realization_metadata'] via _petrology_metadata with feldspar-quartz-clay fractions, cement_signature, and mud_clasts_bool so the mask schema and petrology metadata now match the PRD design.\n</info added on 2025-11-19T20:30:43.619Z>",
            "status": "done",
            "testStrategy": "Peer-review or quick design review in a notebook or PR, ensuring the API is consistent with existing generate_* generator patterns and that mask/metadata keys match PRD naming conventions.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:30:51.053Z"
          },
          {
            "id": 2,
            "title": "Implement channel_fill_sandstone with erosional bases and infill textures",
            "description": "Implement channel_fill_sandstone to carve erosional bases beneath channels and infill them with procedurally textured sandstone, returning a channel_fill mask.",
            "dependencies": [
              1
            ],
            "details": "In geologic_generators.py, implement channel_fill_sandstone(gray, masks, rng, **params) to detect active channel or belt masks, derive an erosional base surface (e.g., using distance-to-channel combined with a smoothed random field), and apply a fining-upward sandstone infill texture using Perlin-like or filtered noise. Update the gray field in-place or return a modified copy, and create a boolean mask 'channel_fill' indicating channel-fill sandstone pixels. Ensure the function is deterministic for a given RNG, supports configurable texture amplitude and vertical thickness parameters, and leaves non-channel areas unchanged. Keep the implementation vectorized with NumPy and compatible with existing gray and masks shapes and dtypes.\n<info added on 2025-11-19T20:31:26.166Z>\nImplemented src/analog_image_generator/geologic_generators.py::channel_fill_sandstone to grab the first available channel/branch mask, derive a normalized utils.distance_to_mask surface blended with gaussian-filtered rng.normal noise for the erosional base, reblend gray[channel_mask] in a deterministic (Generator-seeded) fashion via strength-weighted np.clip logic, and cache the resulting float32 channel_fill mask back into masks for downstream overlays.\n</info added on 2025-11-19T20:31:26.166Z>",
            "status": "done",
            "testStrategy": "Add focused tests (later in tests/test_structures.py) that call channel_fill_sandstone with synthetic channel masks and verify that: (a) channel_fill is non-empty only where channels exist, (b) gray values show a distinct texture inside the channel region versus outside, and (c) repeated runs with the same seed produce identical outputs.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:31:34.425Z"
          },
          {
            "id": 3,
            "title": "Implement cross-bedding, ripple textures, and lateral accretion overlays",
            "description": "Add apply_cross_bedding, ripple_mark_texture, and lateral_accretion_surface helpers that generate structured sedimentary overlays tied to existing facies masks and centerlines.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement apply_cross_bedding(mask, style, rng, **params) to overlay oriented cross-bed laminae within channel_fill or bar masks using banded noise or line patterns, respecting style options 'trough' and 'planar'. Implement ripple_mark_texture(overbank_mask, rng, **params) to add shallow-amplitude, short-wavelength ripples on exposed overbank or floodplain regions. Implement lateral_accretion_surface(centerline, belts, rng, **params) to derive inclined accretion surfaces based on along-flow distance or curvature of the centerline, returning an additional mask (e.g., 'lateral_accretion') and optional gray modulation. Ensure these helpers only modify pixels inside the supplied masks, keep output arrays aligned with the global grid, and are parameterized for later tuning via PRD sliders.\n<info added on 2025-11-19T20:33:05.279Z>\nIn src/analog_image_generator/geologic_generators.py, redesign apply_cross_bedding, ripple_mark_texture, and lateral_accretion_surface so each helper consumes the live masks emitted by the generators (use masks[\"channel_fill\"] or _first_available(\"channel\",\"branch_channel\",\"bar\") for laminae, masks[\"overbank\"] or masks[\"floodplain\"] for ripples, and the channel/branch masks plus optional centerline geometry for lateral surfaces), derives orientation/frequency from that geometry, clamps overlays strictly to those masks, and returns both the overlay field and any gray modulation needed for later slider tuning. Update apply_sedimentary_overlays in the same file to include an env_defaults map (meandering/anastomosing → planar laminae with longer ripple wavelengths tied to levee width, braided → trough bands with higher ripple frequency and steeper accretion gradients), call the helpers with those defaults for every generate_meandering/generate_braided/generate_anastomosing invocation, and blend the returned overlays into the analog plus masks dict so cross_bed, ripple, and lateral_accretion entries are populated consistently across environments.\n</info added on 2025-11-19T20:33:05.279Z>",
            "status": "done",
            "testStrategy": "Extend generator tests to ensure that when cross-bedding and ripple helpers are applied to simple synthetic masks they produce non-uniform, anisotropic patterns with correct orientation, and that lateral_accretion_surface masks lie within the channel/belt footprint and follow the centerline trajectory (checked by sampling along centerline indices).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:33:13.368Z"
          },
          {
            "id": 4,
            "title": "Add fining-upward and overbank mudstone masks with grayscale modulation",
            "description": "Introduce fining-upward and overbank_mudstone masks and adjust grayscale textures to express vertical fining trends and muddy overbank facies.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Using the pseudo-snippet fining_mask = cumulative_sum_along_flow(channel_width_map) / channel_width_map.max(), implement a helper that computes a normalized fining-upward index along the flow direction within channels or belts, and derive a boolean mask 'fining_upward' plus a continuous field that modulates gray values (e.g., lighter upward, darker downward). Similarly, derive an 'overbank_mudstone' mask from existing overbank/floodplain masks and apply subtle grayscale adjustments to indicate muddier, lower-contrast textures. Ensure the new masks are added to the masks dict consistently for all fluvial environments and that modulation does not destroy existing contrast budgets or violate PALETTES assumptions.\n<info added on 2025-11-19T20:33:54.228Z>\nAdded `fining_upward_and_mudstone()` in `src/analog_image_generator/geologic_generators.py:726` using `utils.distance_to_mask(channel_mask >= 0.5)` plus Gaussian smoothing/noise-dithered floodplain blur to emit normalized fining and mudstone overlays, then updated `apply_sedimentary_overlays()` (same file:774-789) to invoke it so every fluvial generator now populates `fining_upward` and `overbank_mudstone` masks with light/dark gray modulation that preserves existing PALETTES contrast budgets.\n</info added on 2025-11-19T20:33:54.228Z>",
            "status": "done",
            "testStrategy": "Create tests that verify fining_upward values increase monotonically along the defined flow coordinate within channels and that overbank_mudstone is a subset of overbank/floodplain masks. Use gray histograms to confirm that fining-upward modulation produces a measurable gradient while keeping values within [0,1] or the expected float range.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:34:01.498Z"
          },
          {
            "id": 5,
            "title": "Define petrology metadata schema and attach mineralogy, cement, and mud clast flags",
            "description": "Design and implement a petrology metadata block capturing mineralogy fractions, cement signatures, and mud clast presence, and attach it to realization_metadata.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Define a realization_metadata['petrology'] structure that includes a normalized mineralogy dict (e.g., {'quartz': x, 'feldspar': y, 'lithics': z}), a cement_signature field (string or small enum-like code), and a mud_clasts_bool flag (or similar) indicating mud clast presence. Implement helpers to generate default or environment-specific petrology profiles for meandering, braided, and anastomosing realizations, and ensure mineralogy fractions are normalized to sum to 1.0 within numerical tolerance. Wire this metadata into the outputs of the fluvial generators or the overlay pipeline so that compute_metrics and reporting can read it consistently, without yet implementing full metrics logic from Task 8.\n<info added on 2025-11-19T20:34:41.391Z>\nConstrain src/analog_image_generator/geologic_generators.py:_petrology_metadata to emit a mineralogy block containing only feldspar/quartz/clay fractions normalized to 1.0 (≤1e-3 tolerance) alongside cement_signature and mud_clasts_bool, and ensure apply_sedimentary_overlays assigns the helper’s dict to masks['realization_metadata'] for each fluvial generator run so downstream compute_metrics/reporting always receive the updated schema.\n</info added on 2025-11-19T20:34:41.391Z>",
            "status": "done",
            "testStrategy": "Add tests that create example realization_metadata structures and assert that mineralogy fractions sum to 1.0 within a small epsilon, that required keys (mineralogy, cement_signature, mud_clasts_bool) are present, and that generators populate petrology metadata for each realization without raising errors.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:34:49.252Z"
          },
          {
            "id": 6,
            "title": "Integrate sedimentary overlays into meandering, braided, and anastomosing pipelines",
            "description": "Update generate_meandering, generate_braided, and generate_anastomosing pipelines to call overlay helpers after core geometry and facies construction, returning expanded masks and metadata.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "In geologic_generators.py, modify generate_meandering, generate_braided, and generate_anastomosing so that after they build core geometry, facies masks, and base grayscale images, they invoke channel_fill_sandstone, apply_cross_bedding, ripple_mark_texture, lateral_accretion_surface, and the fining-upward/overbank helpers. Ensure each generator updates its masks dict with 'channel_fill', 'fining_upward', 'cross_bed', 'ripple', 'overbank_mudstone', and any lateral accretion masks, and that realization_metadata is enriched with petrology information in a consistent schema across environments. Maintain backward compatibility for callers by preserving the (gray, masks, metadata) return pattern and validating that overlays respect environment-specific parameters (e.g., stronger cross-bedding in braided bars than in overbank).\n<info added on 2025-11-19T20:35:45.477Z>\nEnsure src/analog_image_generator/geologic_generators.py keeps generate_meandering (~137), generate_braided (~176), and generate_anastomosing (~221) calling apply_sedimentary_overlays(analog, masks, rng, env=\"<env>\") immediately after compose_* so each environment consistently inherits the shared channel_fill/cross_bed/ripple/fining_upward/overbank_mudstone overlays and realization_metadata emitted by the helper.\n</info added on 2025-11-19T20:35:45.477Z>",
            "status": "done",
            "testStrategy": "Extend tests in tests/test_meandering.py, tests/test_braided.py, and tests/test_anastomosing.py to assert that the new mask keys exist and have non-zero coverage for typical parameter sets, and that realization_metadata includes the petrology fields for each environment. Use seeded RNGs to ensure overlays remain deterministic for fixed seeds.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:35:53.208Z"
          },
          {
            "id": 7,
            "title": "Update PALETTES and GEOLOGIC_RULES with new facies and overlay principles",
            "description": "Extend PALETTES and GEOLOGIC_RULES documentation, including anchors, to cover channel-fill sandstone, fining-upward trends, cross-beds, ripples, overbank mudstone, and petrology metadata.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Edit docs/PALETTES.md to assign or confirm stable palette entries for channel_fill, fining_upward, cross_bed, ripple, overbank_mudstone, and any new masks used by sedimentary overlays so that future visualizations and reports use consistent colors and legends. Update docs/GEOLOGIC_RULES.md to describe conceptual rules for erosional bases, channel-fill textures, cross-bedding styles, ripple formation, fining-upward behavior, and associated petrology assumptions, and add code anchors referencing fully qualified functions in analog_image_generator.geologic_generators.*. Add or update notebook markdown anchors in the appropriate notebooks (e.g., notebooks/fluvial_v20.ipynb) following the anchor-<env>-<principle> convention so that each new or modified principle is traceable from code to documentation.\n<info added on 2025-11-19T20:39:18.909Z>\ndocs/GEOLOGIC_RULES.md now carries Fluvial sedimentary overlay rows for `analog_image_generator.geologic_generators.channel_fill_sandstone`, `.apply_cross_bedding`, `.ripple_mark_texture`, `.lateral_accretion_surface`, `.fining_upward_and_mudstone`, and `.apply_sedimentary_overlays`, each tied to the updated facies ordering references, and notebooks/fluvial_meandering.ipynb, notebooks/fluvial_braided.ipynb, and notebooks/fluvial_anastomosing.ipynb already cite the same anchors.\n</info added on 2025-11-19T20:39:18.909Z>",
            "status": "done",
            "testStrategy": "Manually verify that all new or modified functions have corresponding entries in GEOLOGIC_RULES.md and matching notebook anchors, and run any existing documentation or legend-related tests (if present) to ensure no palette keys are missing. Optionally run a small notebook cell to render a legend image using the new PALETTES entries for visual confirmation.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:39:26.461Z"
          },
          {
            "id": 8,
            "title": "Extend unit tests for fluvial generators and shared sedimentary overlays",
            "description": "Add and update tests to validate overlay masks, petrology normalization, and spatial alignment for meandering, braided, and anastomosing generators plus shared helpers.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Update tests/test_meandering.py, tests/test_braided.py, and tests/test_anastomosing.py to include checks that channel_fill, fining_upward, cross_bed, ripple, and overbank_mudstone masks exist and are non-empty for representative parameter sets. Add a new tests/test_structures.py module (or similar) that exercises channel_fill_sandstone, apply_cross_bedding, ripple_mark_texture, and lateral_accretion_surface on controlled synthetic inputs to verify spatial alignment (e.g., erosional bases always lie below channels along a gradient) and pattern orientation. Include assertions that petrology mineralogy sums to 1.0, that mud_clasts_bool is correctly set based on environment, and that overlays respect RNG determinism. Keep tests fast and avoid heavy plotting, relying on simple numeric summaries and small grids.\n<info added on 2025-11-19T20:44:44.380Z>\nAdded tests/test_overlays.py for channel_fill_sandstone alignment, cross_bedding style divergence, fining_upward gradients, and petrology normalization plus extended tests/test_fluvial.py, tests/test_braided.py, and tests/test_anastomosing.py to require channel_fill/cross_bed/ripple/fining_upward/overbank_mudstone masks with normalized mineralogy metadata; pytest: 20 passed in 0.73s (16 generator + 4 overlay tests).\n</info added on 2025-11-19T20:44:44.380Z>",
            "status": "done",
            "testStrategy": "Run the fluvial test suite (pytest tests/test_meandering.py tests/test_braided.py tests/test_anastomosing.py tests/test_structures.py) and ensure all assertions around overlay presence, mineralogy normalization, and spatial relationships pass with deterministic outcomes for fixed seeds.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:44:52.890Z"
          },
          {
            "id": 9,
            "title": "Add sedimentary overlay smoke tests to scripts/smoke_test.py",
            "description": "Extend the smoke test script to generate quick visual and numeric checks for sedimentary structures and petrology overlays across all fluvial environments.",
            "dependencies": [
              6,
              8
            ],
            "details": "Modify scripts/smoke_test.py (or the existing smoke-test entry point) to run generate_meandering, generate_braided, and generate_anastomosing with typical parameter sets and the new overlays enabled, saving representative gray images or thumbnails and summarizing key mask coverages (channel_fill, fining_upward, cross_bed, ripple, overbank_mudstone) plus petrology summaries. Optionally emit simple text or JSON statistics such as overlay pixel fractions and mineralogy breakdowns to facilitate quick visual and quantitative sanity checks. Ensure the smoke tests are lightweight enough to run as part of a pre-release check without excessive runtime or storage overhead.\n<info added on 2025-11-19T20:46:01.573Z>\nUpdate scripts/smoke_test.py so _run_smoke(style) forwards style=meandering|braided|anastomosing into generate_fluvial with a fixed 128×128 canvas, asserts analog shape, and calls a shared _assert_required_masks over REQUIRED_MASKS = (channel_fill, cross_bed, ripple, fining_upward, overbank_mudstone, realization_metadata) to guarantee each overlay map or metadata dict exists. Emit the human-readable ✓ smoke passed for style=<style> lines after each call so CI/devs can quickly eyeball that all three environments are covered before tagging smoke as complete.\n</info added on 2025-11-19T20:46:01.573Z>",
            "status": "done",
            "testStrategy": "Run the smoke test script manually (e.g., python scripts/smoke_test.py) and visually inspect a small subset of saved images or textual summaries to confirm that added structures and metadata appear plausible and do not crash the pipeline; consider integrating this script into any existing smoke-test CI job if available.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:46:08.940Z"
          },
          {
            "id": 10,
            "title": "Document approximations, limitations, and notebook anchors for sedimentary overlays",
            "description": "Capture modeling approximations and limitations for erosional bases, textures, and petrology in notebooks and GEOLOGIC_RULES utility sections, with proper anchors for review.",
            "dependencies": [
              6,
              7,
              8,
              9
            ],
            "details": "Update relevant v20 fluvial notebooks in the notebooks/ directory to include markdown cells explaining how erosional bases, channel-fill sandstone textures, cross-beds, ripples, fining-upward patterns, and overbank mudstone are approximated numerically (e.g., sub-pixel assumptions, noise model simplifications, lack of fully resolved laminae). Add explicit notes on petrology simplifications such as assumed mineralogy ranges and cement types. Ensure that each principle has a corresponding anchor ID of the form anchor-<env>-<principle> and that GEOLOGIC_RULES.md references these anchors in the notebook anchor column. Clarify which aspects are intended for qualitative realism versus quantitative petrophysical fidelity so reviewers and users can interpret metrics and reports correctly.\n<info added on 2025-11-19T20:47:16.473Z>\nDocumented `anchor-sedimentary-approximations` markdown cells in `notebooks/fluvial_meandering.ipynb`, `notebooks/fluvial_braided.ipynb`, and `notebooks/fluvial_anastomosing.ipynb` that spell out the distance-to-mask gradient heuristics, Gaussian-smoothed lamination noise, feldspar/quartz/clay mineralogy presets, kaolinite-vs-calcite cement toggles, and an explicit reminder that these overlays target qualitative realism rather than petrophysical fidelity.\n</info added on 2025-11-19T20:47:16.473Z>",
            "status": "done",
            "testStrategy": "Open the updated notebooks and GEOLOGIC_RULES.md to confirm that all new or modified sedimentary overlay functions and petrology assumptions are documented with matching anchors, and optionally have a reviewer or geologist skim the documentation to validate clarity and adequacy of the stated approximations and limitations.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:47:25.589Z"
          }
        ],
        "complexity": 9,
        "recommendedSubtasks": 10,
        "expansionPrompt": "Expand Task 5 into ~10 subtasks that introduce cross-cutting sedimentary-structure and petrology overlays across all fluvial generators. Work primarily in src/analog_image_generator/geologic_generators.py (or a small adjunct module if needed) and update GEOLOGIC_RULES.md accordingly. Subtasks should: (1) design the API and data structures for channel_fill_sandstone(gray, masks, rng) and additional overlay helpers (apply_cross_bedding, ripple_mark_texture, lateral_accretion_surface); (2) implement channel-fill logic with erosional bases and infill textures using procedural noise (e.g., Perlin-like or filtered random fields); (3) implement cross-bedding masks/styles and ripple textures tied to existing facies masks; (4) add fining-upward and overbank_mudstone masks and corresponding grayscale modulation; (5) define and attach petrology metadata (mineralogy dict, cement_signature, mud_clasts_bool) to realization metadata; (6) update meandering, braided, and anastomosing generator pipelines (Tasks 2–4) to call these overlays after core geometry and facies are built, expanding mask dicts consistently; (7) ensure PALETTES and GEOLOGIC_RULES entries are extended to cover new facies and overlays with anchors; (8) implement or extend tests in tests/test_meandering.py, tests/test_braided.py, tests/test_anastomosing.py, and potentially a shared tests/test_structures.py to validate overlay presence, mineralogical normalization, and spatial alignment (e.g., erosion bases under channels); (9) add smoke-test hooks in scripts/smoke_test.py to visually/quantitatively validate added structures; (10) document approximations and limitations in notebooks and GEOLOGIC_RULES utility sections for traceability.",
        "updatedAt": "2025-11-19T20:47:25.589Z"
      },
      {
        "id": 6,
        "title": "Implement stacked channel package builder",
        "description": "Create `stacked_channels.py` that composes multi-package stratigraphy with relief controls and boundary masks for stats/reporting.",
        "details": "- Add module with functions `sequence_packages(package_specs, base_grid_shape, rng)` and `cut_erosional_surface(previous_stack, relief_px, style)`.\n- Packages track metadata: style (meander/braided/anasto), thickness_px, erosion_depth, boundary masks per package.\n- Provide API `build_stacked_fluvial(params)` used by `generate_fluvial` when `params[\"mode\"] == \"stacked\"` to assemble outputs from Tasks 2–5.\n- Manage toggles between single belt and stacked workflow without duplicating generator logic: pass per-package params to generator functions and accumulate masks (with package IDs for stats/reporting).\n- Pseudo loop:\n```\nstack = np.zeros((n_packages, H, W), dtype=float32)\nfor idx, spec in enumerate(package_specs):\n    gray, masks = GENERATOR_MAP[spec.style](spec.params, rng)\n    stack[idx] = apply_relief(gray, current_surface, spec.erosion)\n```\n- Export boundary masks: `upper_surface_mask`, `erosion_surface_mask`, `package_id_map`.\n- Update docs + notebook anchors describing stacked workflow and slider mapping.\n- Ensure metadata recorded for stats (erosional relief, package mix).",
        "testStrategy": "- Create `tests/test_stacked_channels.py` verifying package count matches slider, erosion depth reduces stratigraphy thickness accordingly, and mask union stays within grid.\n- Add property test ensuring toggling between single vs stacked yields identical single-belt outputs when `package_count=1`.\n- Update smoke script to build 2-package stack and confirm boundary masks exist.",
        "priority": "medium",
        "dependencies": [
          "2",
          "3",
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design stacked channel package spec and metadata structures",
            "description": "Define data structures to represent stacked channel package specifications and per-package metadata for fluvial styles.",
            "dependencies": [],
            "details": "Introduce a clear data model in src/analog_image_generator/stacked_channels.py for package_specs and resulting metadata, likely using TypedDicts or dataclasses to capture fields such as style (meander, braided, anasto), thickness_px, erosion_depth_px, seed, and any per-package parameter overrides. Include structures to hold per-package boundary masks and a stable package_id so downstream stats and reporting can reference packages consistently. Ensure types and docstrings align with existing generator parameter conventions in geologic_generators.py.",
            "status": "done",
            "testStrategy": "Add lightweight unit checks that constructing PackageSpec instances with typical and edge-case values works, and that metadata containers serialize to JSON-compatible dicts for stats/reporting.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:58:57Z"
          },
          {
            "id": 2,
            "title": "Define generator dispatch map for fluvial styles in stacked_channels",
            "description": "Create a GENERATOR_MAP that routes styles to existing fluvial generator functions.",
            "dependencies": [
              1
            ],
            "details": "In stacked_channels.py, import the public generator entry points from geologic_generators.py (e.g., generate_meandering, generate_braided, generate_anastomosing) and build a GENERATOR_MAP dictionary keyed by style strings such as 'meander', 'braided', and 'anasto'. Ensure each callable accepts (params, rng) and returns (gray, masks_dict) consistent with Tasks 2–4. Document expected mask keys and gray dtypes so sequence_packages and relief routines can treat all styles uniformly.",
            "status": "done",
            "testStrategy": "Write a small test that iterates over supported style keys, calls the mapped generator with minimal dummy params and a seeded RNG (using test doubles or simple fixtures), and asserts that each returns a float32 gray array and a non-empty mask dict.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:58:57Z"
          },
          {
            "id": 3,
            "title": "Implement sequence_packages to build 3D gray stack and per-package masks",
            "description": "Implement sequence_packages(package_specs, base_grid_shape, rng) to loop over packages, call generators, and accumulate stacked outputs.",
            "dependencies": [
              1,
              2
            ],
            "details": "Add sequence_packages(package_specs, base_grid_shape, rng) in stacked_channels.py that allocates a stack array of shape (n_packages, H, W) and iterates over ordered package_specs. For each spec, use GENERATOR_MAP[spec.style] to obtain (gray, masks), resize or crop to base_grid_shape if needed, and store gray into the stack index. Track per-package masks and metadata in parallel lists or dicts keyed by package_id. Ensure that generator parameters are cloned and updated per spec to avoid mutating shared dicts, and that sequence order is deterministic given the same RNG seed and package_specs.",
            "status": "done",
            "testStrategy": "Create tests that construct 2–3 synthetic PackageSpec entries with fixed seeds, call sequence_packages, assert the stack has expected shape and dtype, verify the number of slices equals len(package_specs), and confirm per-package metadata and mask collections line up with package_ids.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:58:57Z"
          },
          {
            "id": 4,
            "title": "Design and implement apply_relief functions for vertical stacking",
            "description": "Create apply_relief or equivalent helpers that map individual package gray images into a vertically stacked representation with relief.",
            "dependencies": [
              1,
              3
            ],
            "details": "Introduce one or more helper functions (e.g., apply_relief(gray, current_surface, relief_px, erosion_style)) that take a single package gray array and an accumulated surface or elevation model and return an updated representation reflecting erosional relief and thickness. Decide whether to represent relief as a separate elevation grid or implicitly via mask blending; implement logic to offset or attenuate underlying packages according to relief_px and thickness_px. Ensure interfaces integrate cleanly with sequence_packages so each package slice is transformed consistently before insertion into the stack.",
            "status": "done",
            "testStrategy": "Add tests that feed simple synthetic gray fields (e.g., constant ramps) into apply_relief with known relief_px values and verify that resulting surfaces honor expected elevation ordering, thickness adjustments, and do not introduce NaNs or out-of-bounds indices.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:58:57Z"
          },
          {
            "id": 5,
            "title": "Implement cut_erosional_surface and integrate with stacking loop",
            "description": "Implement cut_erosional_surface(previous_stack, relief_px, style) and wire it into the package sequencing workflow.",
            "dependencies": [
              3,
              4
            ],
            "details": "In stacked_channels.py, implement cut_erosional_surface(previous_stack, relief_px, style) to compute an erosional surface given the accumulated stack so far, using style-dependent logic when needed (e.g., different relief textures for braided vs meandering). Have this function return both an updated stack or surface representation and erosion masks that can be used as boundary masks. Integrate calls to cut_erosional_surface into the sequence_packages loop so that for each new package, erosion is applied to previously deposited packages before the new package is added, tracking erosion_depth in metadata.",
            "status": "done",
            "testStrategy": "Write tests that construct a small synthetic previous_stack with two packages, call cut_erosional_surface with a moderate relief_px, and assert that a portion of upper voxels are removed or modified while lower packages remain intact, and that returned erosion masks are boolean and within grid bounds.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:58:57Z"
          },
          {
            "id": 6,
            "title": "Export boundary masks and package_id_map for stats and reporting",
            "description": "Define and compute upper_surface_mask, erosion_surface_mask, package_id_map, and aggregated metadata outputs.",
            "dependencies": [
              3,
              4,
              5
            ],
            "details": "Extend stacked_channels.py to derive final boundary products after stacking: an upper_surface_mask marking the topmost exposed cells, an erosion_surface_mask marking erosional contacts, and a package_id_map assigning each pixel in the final composite to a package_id or background. Aggregate per-package metadata (style, thickness_px, erosion_depth_px, relief_px, etc.) into a structured record suitable for downstream stats and reporting APIs. Ensure these outputs have consistent shapes with base_grid_shape and are returned from the main stacking API along with the composite gray image and masks_dict.",
            "status": "done",
            "testStrategy": "Add tests that run a small 3-package scenario and verify that exactly one package_id is assigned per non-background pixel, that upper_surface_mask and erosion_surface_mask are boolean arrays matching the grid shape, and that metadata entries exist for each package_id referenced in the map.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:58:57Z"
          },
          {
            "id": 7,
            "title": "Implement build_stacked_fluvial API and single vs stacked mode toggle",
            "description": "Create build_stacked_fluvial(params) to route between single-belt and stacked workflows without duplicating generator logic.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6
            ],
            "details": "In stacked_channels.py, implement build_stacked_fluvial(params) that inspects params[\"mode\"] and package_count-related fields to decide whether to call existing single-belt generators directly or to construct package_specs and invoke sequence_packages. Reuse the same generator entry points wired into GENERATOR_MAP so no generator logic is duplicated. Ensure the function returns (gray, masks, metadata) with a compatible shape and mask schema to existing single-belt outputs, and that when package_count == 1 in stacked mode, the result is equivalent to the single-belt path within numerical tolerance.",
            "status": "done",
            "testStrategy": "Add tests that compare outputs of build_stacked_fluvial with mode='single' and mode='stacked' and package_count=1 using the same base params and seed, asserting that gray arrays match within a small tolerance and that masks have identical coverage and keys.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:58:57Z"
          },
          {
            "id": 8,
            "title": "Wire stacked mode into generate_fluvial and parameter plumbing",
            "description": "Integrate build_stacked_fluvial into geologic_generators.generate_fluvial so callers can select stacked workflows via params and sliders.",
            "dependencies": [
              7
            ],
            "details": "Modify geologic_generators.py to import build_stacked_fluvial and update generate_fluvial to recognize a mode key (e.g., params.get('mode', 'single')) plus new stacked parameters like package_count and erosional_relief_px. Update the function to route fluvial environments to either existing single-belt generators or the stacked builder as appropriate, ensuring the public API remains backward compatible. Adjust any existing parameter validation, docstrings, and env-to-slider mappings so the new stacked options integrate cleanly.",
            "status": "done",
            "testStrategy": "Extend or add unit tests around generate_fluvial that call it with both legacy single-belt params and new stacked-mode params, asserting that previous behaviors are unchanged and that stacked mode returns appropriately shaped outputs and metadata without raising errors.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:58:57Z"
          },
          {
            "id": 9,
            "title": "Create unit tests for stacked_channels behaviors and invariants",
            "description": "Add tests/test_stacked_channels.py to verify package counts, erosion-thickness relationships, mask unions, and single-belt equivalence.",
            "dependencies": [
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "details": "Create a new test module tests/test_stacked_channels.py that exercises the main stacking APIs with several configurations. Include tests confirming that the number of stacked slices equals package_count, that increasing erosion_depth_px reduces net preserved thickness, that unions of boundary masks remain within the grid extents, and that when package_count == 1 stacked outputs match single-belt results from generate_fluvial. Use deterministic seeded RNG and assert on shapes, dtypes, and simple statistics rather than strict pixel equality when appropriate.",
            "status": "done",
            "testStrategy": "Implement pytest cases covering: multi-package stacking with varied styles, erosion vs thickness monotonicity, mask union containment, and equivalence for the package_count=1 case against a reference single-belt run.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:58:57Z"
          },
          {
            "id": 10,
            "title": "Extend smoke tests and documentation for stacked workflows and anchors",
            "description": "Update smoke tests, GEOLOGIC_RULES, and notebooks to document stacked channel workflows, anchors, and slider mappings.",
            "dependencies": [
              7,
              8,
              9
            ],
            "details": "Edit scripts/smoke_test.py to include at least one small stacked configuration (e.g., 2–3 packages with mixed styles) and verify that boundary masks, package_id_map, and metadata are present and sane. Update docs/GEOLOGIC_RULES.md to describe the stacked channel package principles, including package_count, erosional_relief_px, and style mix, and add or update anchor IDs using the anchor-<env>-<principle> pattern referencing fully qualified function names like analog_image_generator.stacked_channels.build_stacked_fluvial. Ensure relevant environment notebooks under notebooks/ add markdown anchors describing the stacked workflow and slider mapping consistent with the code.",
            "status": "done",
            "testStrategy": "Run the updated smoke_test script to ensure it completes without errors for the stacked configuration, and manually or via simple checks confirm that the new documentation anchors reference existing code symbols and that notebook cells execute to completion.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T20:58:57Z"
          }
        ],
        "complexity": 9,
        "recommendedSubtasks": 10,
        "expansionPrompt": "Expand Task 6 into ~10 subtasks focused on building a stacked channel package system. Create a new module src/analog_image_generator/stacked_channels.py and integrate it with existing generators in geologic_generators.py. Subtasks should: (1) design data structures for package_specs and per-package metadata (style, thickness_px, erosion_depth, boundary masks, IDs); (2) implement sequence_packages(package_specs, base_grid_shape, rng) to loop over packages, call the appropriate fluvial generator (meandering/braided/anasto), and accumulate grayscale and masks into a stack; (3) implement cut_erosional_surface(previous_stack, relief_px, style) and any helper routines needed to carve erosional surfaces and apply relief; (4) design and implement apply_relief or equivalent functions that map individual package outputs into a vertically stacked representation; (5) implement build_stacked_fluvial(params) that decides between single-belt vs stacked workflows (mode toggle) without duplicating generator logic and routes to sequence_packages; (6) define outputs such as upper_surface_mask, erosion_surface_mask, package_id_map, and aggregate metadata needed by stats/reporting; (7) integrate stacked mode into generate_fluvial so callers can choose between single and stacked workflows; (8) write tests/tests_stacked_channels.py to verify package counts, erosion-thickness relationships, mask unions within grid bounds, and equivalence to single-belt outputs when package_count=1; (9) extend scripts/smoke_test.py to exercise a small stacked configuration and validate boundary masks; (10) update GEOLOGIC_RULES.md and notebooks to document stacked workflows, anchors, and slider mappings (package_count, erosional_relief_px, style mix).",
        "updatedAt": "2025-11-19T20:58:57Z"
      },
      {
        "id": 7,
        "title": "Build ipywidgets sliders, previews, and param batch (Interactive v20a)",
        "description": "Implement `interactive.py` to expose slider schemas per env, render sequential previews, show live β/D/H, and save lightweight param batches.",
        "details": "- Define `build_sliders(env)` returning dicts with `min`, `max`, `step`, `default`, `units`, `source`, matching PRD table (use validated defaults from research PDFs).\n- Compose ipywidgets UI: slider panel, preview button, seed input, stacked toggle, package mix multi-select.\n- `preview_sequence(env, params, seeds)` should run generator/stacked builder, capture intermediate thumbnails (centerline, masks, final gray, color + legend), and compute quick β/D/H by calling lightweight subset of stats (Task 8) or placeholder until ready.\n- Add `run_param_batch(env, slider_configs, seeds, output_dir)` saving grayscale + color PNGs.\n- Provide sequential preview layout via `ipywidgets.HBox`/`VBox` with Matplotlib or Pillow for inline display.\n- Ensure slider defaults validated against `research-documents` references, storing citation metadata for tooltips.\n- Document usage in `notebooks/v20a_interactive_rebuild.ipynb` (markdown cell referencing anchor IDs).",
        "testStrategy": "- Add `tests/test_interactive.py` verifying slider definitions match required ranges and default values, using pure-Python tests (no widget rendering) by asserting dictionaries.\n- Use `pytest` with `ipywidgets` event loop to simulate slider change -> preview call (mock generator to avoid heavy compute) and ensure `preview_sequence` returns layout objects.\n- Manual notebook smoke: run `v20a_interactive_rebuild.ipynb` to confirm live β/D/H updates; capture screenshot for documentation.",
        "priority": "medium",
        "dependencies": [
          "2",
          "3",
          "4",
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design slider schema and implement build_sliders(env) in interactive.py",
            "description": "Define a structured slider configuration schema and implement build_sliders(env) to return environment-specific slider definitions.",
            "dependencies": [],
            "details": "In src/analog_image_generator/interactive.py, define a clear Python data structure (e.g., TypedDict or dataclass plus plain dicts) for slider configs with fields: name/key, label, min, max, step, default, units, source, citation_ids, and any env tags. Implement build_sliders(env: str) to return an ordered mapping of parameter keys to these config dicts for each supported env (e.g., meandering, braided, anastomosing, stacked). Seed initial ranges and defaults from the current PRD tables and research PDFs summaries (using existing constants or helper modules if available), but keep lookup logic separated so values can later be regenerated from a single source of truth. Ensure build_sliders is pure (no widget construction) and safe to call in tests and non-notebook contexts.",
            "status": "done",
            "testStrategy": "Add unit tests in tests/test_interactive.py that call build_sliders for each env and assert required keys exist on every slider config, that numeric fields are within expected broad bounds, and that the function is side-effect free (no ipywidgets imported or created).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:11:38Z"
          },
          {
            "id": 2,
            "title": "Implement ipywidgets-based UI factory using slider configs",
            "description": "Create composable ipywidgets UI components for sliders, seed input, stacked toggle, and package mix selectors.",
            "dependencies": [
              1
            ],
            "details": "In interactive.py, add a function such as build_interactive_ui(env: str) or build_controls(env: str, slider_configs: Mapping) that uses ipywidgets (IntSlider, FloatSlider, Text/IntText, ToggleButtons/Checkbox, SelectMultiple, Button, etc.) to construct the interactive control panel. Map each slider config from build_sliders(env) to a corresponding widget, wiring labels, ranges, defaults, units in descriptions, and tooltip text from source/citation metadata. Include additional widgets for RNG seed input, stacked vs single-belt toggle, package_count and package mix multi-select controls, and a Preview button. Return a structured object (e.g., a dataclass or dict) containing both the widgets and a top-level VBox/HBox layout so notebooks can embed the panel easily without tightly coupling to notebook code.",
            "status": "done",
            "testStrategy": "Write tests that construct the UI using a fake or real slider_configs dict and assert that the correct number and types of widgets are created, their min/max/default values match the configs, and that build_interactive_ui returns a structure that can be introspected without requiring a running Jupyter front-end (e.g., checking attributes, not rendering).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:11:38Z"
          },
          {
            "id": 3,
            "title": "Implement preview_sequence(env, params, seeds) with generator/stacked integration",
            "description": "Wire preview_sequence to call the appropriate generators, capture intermediate artifacts, and return an ipywidgets layout of preview images.",
            "dependencies": [
              1,
              2
            ],
            "details": "In interactive.py, implement preview_sequence(env: str, params: Mapping, seeds: Sequence[int]) to call the correct fluvial generator entry points (e.g., analog_image_generator.geologic_generators.generate_fluvial or a build_stacked_fluvial helper) based on env and the stacked toggle/package parameters. For each requested seed, run the generator to collect intermediate artifacts such as centerline representation, masks_dict, grayscale image, and a composited RGB image with color legend using existing visualization utilities or lightweight matplotlib/Pillow helpers in a separate, non-notebook-dependent module. Convert these artifacts into small inline preview images (e.g., via matplotlib FigureCanvas or PIL-to-Image widgets) and arrange them into a sequential HBox/VBox layout that shows centerline → masks → gray → color for each seed. Return both the widget layout and a structured data bundle (e.g., dict of arrays or paths) so callers can optionally reuse the raw outputs.",
            "status": "done",
            "testStrategy": "In tests/test_interactive.py, create minimal or mocked generator functions (monkeypatched into interactive.py) that return small numpy arrays or dummy masks. Call preview_sequence with a small list of seeds and assert that it invokes the mocked generators the expected number of times, that the return value includes a widget container and structured artifacts, and that it does not raise in a non-notebook environment. Avoid heavy image generation by using tiny arrays and skipping actual rendering when running under tests (e.g., using a headless matplotlib backend).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:11:38Z"
          },
          {
            "id": 4,
            "title": "Add lightweight β/D/H metrics adapter for previews",
            "description": "Provide a fast adapter that computes or fetches β/D/H metrics for each preview run using existing stats code or a focused helper.",
            "dependencies": [
              3
            ],
            "details": "Introduce a small helper in interactive.py or a dedicated stats adapter module (e.g., interactive_metrics.py) that accepts the generator outputs (centerline, masks, gray) and computes β/D/H metrics using a lightweight subset of stats.compute_metrics or a dedicated fast path that avoids full-report overhead. The adapter should handle the case where the full stats module or Task 8 is not yet complete by either importing a minimal metrics function or using a clearly marked placeholder that returns NaN or approximate values while logging a warning. Integrate this adapter into preview_sequence so each preview seed has associated β/D/H values that can be displayed next to or overlaid on the preview widgets (e.g., as Label widgets or text areas). Design the interface so that when stats.compute_metrics is fully implemented, only the adapter needs updating, not the UI code.",
            "status": "done",
            "testStrategy": "Add tests that call the metrics adapter directly with small synthetic inputs and verify that it either delegates to stats.compute_metrics when available or returns a well-structured placeholder result when not. In preview_sequence tests, assert that the returned data includes β/D/H fields for each seed and that failures in metrics computation are handled gracefully without breaking widget construction (e.g., by catching exceptions and substituting NaN values).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:11:38Z"
          },
          {
            "id": 5,
            "title": "Implement run_param_batch(env, slider_configs, seeds, output_dir) for PNG and metadata export",
            "description": "Create a batch execution function that runs generators over parameter/seed grids and saves grayscale/color PNGs plus minimal metadata.",
            "dependencies": [
              1,
              3,
              4
            ],
            "details": "In interactive.py, implement run_param_batch(env: str, slider_configs: Mapping, seeds: Sequence[int], output_dir: PathLike, params_overrides: Optional[Sequence[Mapping]] = None) that iterates over seeds and one or more parameter sets derived from the current slider configurations. For each run, call the same underlying generator or stacked builder used by preview_sequence, capture grayscale and colorized images, and save them as PNGs with deterministic filenames encoding env, seed, and a concise parameter hash. Additionally, serialize minimal JSON or CSV sidecar metadata per run (or one aggregated file) capturing env, seed, slider values, β/D/H metrics from the adapter, and citation/source tags used to define the parameters. Ensure the function creates output_dir if needed and is safe to call from both notebooks and scripts without relying on ipywidgets. Keep I/O logic separated from UI so it can be tested headlessly and used by reporting pipelines.",
            "status": "done",
            "testStrategy": "Write tests that use a temporary directory (pytest tmp_path) and a mocked generator that returns small arrays; call run_param_batch with a couple of seeds and simple slider_configs, then assert that the expected PNG files and metadata file(s) are created, that filenames include env and seed, and that metadata records contain parameter values and β/D/H fields. Use small images or dummy arrays to keep tests fast and independent of real rendering complexity.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:11:38Z"
          },
          {
            "id": 6,
            "title": "Validate slider ranges/defaults against PRD and research documents and attach citation metadata",
            "description": "Add validation utilities that check slider configs against PRD tables and research-documents and enrich configs with citation/source metadata.",
            "dependencies": [
              1
            ],
            "details": "Implement a validation layer in interactive.py or a small helper module (e.g., slider_validation.py) that, given the slider configs from build_sliders(env), cross-checks min/max/default/step values against a structured representation of the PRD tables and any available research-documents summaries (e.g., JSON or constants already extracted from PDFs). The validator should raise clear exceptions or warnings when discrepancies exceed allowed tolerances, and it should populate each slider config with source fields and citation IDs (e.g., DOI, paper name, page/figure references) that can be used in ipywidgets tooltips. Expose a function like validate_slider_configs(env, slider_configs) and call it from build_sliders or immediately after in UI constructors so all interactive usage benefits from consistent validation and traceability.",
            "status": "done",
            "testStrategy": "Add tests that construct synthetic PRD/research reference data and slider_configs, then run validate_slider_configs to confirm that matching values pass and that intentional mismatches raise informative errors or warnings. Also test that citation metadata is injected into slider configs as expected and that build_sliders(env) either returns already-validated configs or fails early when validation detects a problem.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:11:38Z"
          },
          {
            "id": 7,
            "title": "Add tests for interactive slider data, preview plumbing, and batch execution",
            "description": "Create tests/test_interactive.py to cover slider definitions, UI wiring, preview_sequence behavior, metrics adapter integration, and run_param_batch.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Populate tests/test_interactive.py with a focused test suite that treats slider configurations as pure data and exercises the main public entry points in interactive.py without expensive rendering. Include tests that: (a) verify build_sliders(env) returns only serializable, well-typed config dicts; (b) check that the UI factory returns the expected widgets and that changes in a widget’s value propagate into parameter dicts used for previews; (c) confirm preview_sequence returns a widget container plus structured artifacts, using mocked generators and metrics adapters; and (d) ensure run_param_batch writes PNGs and metadata files correctly to a temporary directory using mocked generators. Where necessary, use monkeypatching or lightweight fixtures to avoid importing Jupyter-specific modules beyond ipywidgets and to keep tests deterministic and fast.",
            "status": "done",
            "testStrategy": "Run pytest against tests/test_interactive.py, using fixtures to set a headless matplotlib backend and small dummy arrays. Validate that all tests pass without requiring a live notebook kernel, and consider adding minimal golden-data checks for the shapes or hashes of dummy outputs if stable enough.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:11:38Z"
          },
          {
            "id": 8,
            "title": "Update v20a interactive notebook anchors and GEOLOGIC_RULES documentation for interactive.py",
            "description": "Revise notebooks/v20a_interactive_rebuild.ipynb and GEOLOGIC_RULES.md to document the new interactive sliders, previews, batch functions, and anchor mappings.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Open notebooks/v20a_interactive_rebuild.ipynb and add or update markdown cells that describe how to import and use the new interactive functions (build_sliders, UI factory, preview_sequence, run_param_batch), including example code snippets and explanation of β/D/H display. Ensure each principle or behavior implemented in interactive.py has a corresponding notebook anchor ID following the anchor-<env>-<principle> convention and that these anchors reference fully qualified function names like analog_image_generator.interactive.build_sliders. In docs/GEOLOGIC_RULES.md, add or update rows that map geologic or UX principles to the new interactive functions and notebook anchors, keeping anchor IDs and terminology in sync. Verify that the notebook’s checklist and any task references reflect Task 7’s completion criteria and that documentation aligns with PRD and slider validation rules.",
            "status": "done",
            "testStrategy": "Manually open notebooks/v20a_interactive_rebuild.ipynb in Jupyter to confirm that anchors render correctly, example cells execute using interactive.py, and that links between GEOLOGIC_RULES.md and notebook anchors are consistent. Optionally, add a lightweight documentation test that checks for the presence of expected anchor IDs and function name references in GEOLOGIC_RULES.md using a simple pytest that reads the file as text.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:11:38Z"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 8,
        "expansionPrompt": "Expand Task 7 into ~8 subtasks that implement interactive sliders, previews, and param batching in src/analog_image_generator/interactive.py, which currently only contains NotImplemented stubs. Subtasks should: (1) define a structured schema for slider configs (min, max, step, default, units, source, citations) and implement build_sliders(env) to populate them from PRD tables and research-documents; (2) wire up ipywidgets-based UI elements (sliders, seed input, stacked toggle, package mix selectors) in a composable form that can be imported into notebooks; (3) implement preview_sequence(env, params, seeds) to call generate_fluvial or build_stacked_fluvial, collect intermediate artifacts (centerline, masks, gray, RGB legend view) and return a layout built from ipywidgets HBox/VBox; (4) add a lightweight adapter that computes β/D/H metrics for previews via a small subset of stats.compute_metrics or a dedicated fast path; (5) implement run_param_batch(env, slider_configs, seeds, output_dir) to execute batches and save grayscale/color PNGs and minimal metadata; (6) ensure that slider defaults and ranges are validated against PRD and research-documents, and attach citation/source metadata; (7) write tests/test_interactive.py that validate slider definitions as pure data, and use minimal/mocked generators to exercise preview_sequence return types without heavy image generation; (8) update notebooks/v20a_interactive_rebuild.ipynb markdown anchors and GEOLOGIC_RULES references to document usage and traceability.",
        "updatedAt": "2025-11-19T21:11:38Z"
      },
      {
        "id": 8,
        "title": "Implement full Phase 1 & 2 metrics pipeline",
        "description": "Fill `stats.py` compute_metrics covering variograms, entropy, fractal dimension, PSD anisotropy, topology, and property metadata hooks.",
        "details": "- Implement helpers: `compute_variogram(gray, directions)`, `fit_power_law(beta)`, `two_segment_fit(lags)`, `entropy(gray)`, `fractal_dimension(beta)`, `psd_anisotropy(gray)`, `topology_metrics(mask)` using NumPy/SciPy.\n- Accept `gray`, `masks`, `env`, and optional `metadata` (from Tasks 2–6) to append petrology + sedimentary flags.\n- Ensure outputs include PRD fields: `beta_iso`, `beta_dir_*`, `anisotropy_ratio`, `beta_seg1`, `beta_seg2`, `h0`, `entropy_global`, `D`, `SFI`, `psd_aspect`, `psd_theta`, `area_*`, `compact_*`, `conn_*`, mineralogical tags, QA flags for property presence, stacked package metadata.\n- Provide quick metric adapter for interactive preview (β/D/H) to avoid recomputation.\n- Validate outputs against acceptance bands (raise warnings when outside expected ranges, e.g., braided PSD aspect >2.0) and propagate to reporting.\n- Document formulas + references in GEOLOGIC_RULES + notebooks (Phase 1 & 2 sections).",
        "testStrategy": "- Add `tests/test_stats.py` with synthetic arrays (analytic slopes) verifying β estimates within tolerance and PSD aspect detection of anisotropic Gaussian field.\n- Include fixtures for mask topology (simple polygons) to verify area/compactness/connectivity calculations.\n- Run regression tests comparing computed metrics to stored JSON fixtures generated from deterministic seeds; fail if deviation > tolerance.\n- Integrate into `scripts/smoke_test.py` to compute metrics for each env and assert required keys exist.",
        "priority": "medium",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design metrics result schema and QA flag structure",
            "description": "Define a flat, typed metrics result schema for compute_metrics covering all PRD fields, QA flags, and stacked package metadata.",
            "dependencies": [],
            "details": "Review the PRD, reporting requirements, and existing CSV/PDF expectations to enumerate all required keys (e.g., beta_iso, beta_dir_*, anisotropy_ratio, beta_seg1, beta_seg2, h0, entropy_global, D, SFI, psd_aspect, psd_theta, area_*, compact_*, conn_*, mineralogical tags, property presence flags, stacked package metadata). Specify units, types, and default missing-value conventions (e.g., NaN or None), and design a canonical flat dict layout plus any lightweight internal container (e.g., TypedDict or dataclass) used inside stats.py. Capture QA thresholds (e.g., braided PSD aspect > 2.0) and define corresponding boolean or categorical QA fields that downstream reporting can consume consistently.",
            "status": "done",
            "testStrategy": "Add a unit test that instantiates a dummy metrics dict via a small helper and asserts that all required PRD keys exist with correct types, units documented in docstrings, and stable key ordering for CSV export.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:15:47Z"
          },
          {
            "id": 2,
            "title": "Implement compute_variogram(gray, directions) and lag binning helpers",
            "description": "Create variogram computation utilities for grayscale images, including lag vector generation and directional binning.",
            "dependencies": [
              1
            ],
            "details": "Implement compute_variogram(gray, directions) in stats.py using NumPy (and SciPy if helpful) to compute semi-variograms for a set of directions (e.g., 0°, 45°, 90°, 135°). Add helper functions to generate lag offsets, accumulate squared differences, and bin by lag distance with appropriate normalization and masking of NaNs. Ensure support for isotropic aggregation as well as per-direction outputs so that later fitting can derive beta_iso and beta_dir_* values. Make the implementation efficient enough for interactive use on typical grid sizes by avoiding Python loops where possible.",
            "status": "done",
            "testStrategy": "Write tests that generate synthetic Gaussian random fields and simple linear gradient fields, compute variograms for a small set of directions, and verify symmetry, monotonicity of semi-variance with lag, and expected scaling trends within loose numerical tolerances.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:15:47Z"
          },
          {
            "id": 3,
            "title": "Implement power-law and two-segment variogram slope fitting helpers",
            "description": "Fit power-law and two-segment models to variogram data to estimate beta, beta_seg1, beta_seg2, and h0.",
            "dependencies": [
              2
            ],
            "details": "Add fit_power_law() to estimate a single-slope beta from log-log variogram versus lag data using robust linear regression or SciPy curve fitting, including safeguards against noisy or sparse bins. Implement two_segment_fit() to split the lag range into two regimes (e.g., using a breakpoint search or fixed fraction of max lag) and fit separate slopes beta_seg1 and beta_seg2 plus an intercept-based characteristic scale h0. Ensure both helpers return diagnostics (R^2, number of points used, valid flag) so compute_metrics can set QA flags when fits are unreliable.",
            "status": "done",
            "testStrategy": "In tests, construct synthetic power-law variograms with known slopes (single and two-segment), pass them into the fitting helpers, and assert recovered betas and h0 values are within specified relative tolerance bands, including edge cases with noisy or truncated data.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:15:47Z"
          },
          {
            "id": 4,
            "title": "Implement entropy(gray) and fractal_dimension(beta) calculations",
            "description": "Compute global entropy of grayscale textures and derive fractal dimension from variogram slopes.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement entropy(gray) using histogram- or probability-based Shannon entropy over the grayscale image, handling NaNs or masked regions as needed and normalizing bins consistently across tests and environments. Implement fractal_dimension(beta) using the chosen geologic relationship between variogram slope and fractal dimension (e.g., D = f(beta) as specified in PRD and GEOLOGIC_RULES), supporting both isotropic beta and directional or segmented variants if needed. Ensure both functions are numerically stable and documented with explicit formulas and parameter choices.",
            "status": "done",
            "testStrategy": "Create tests using simple synthetic fields: uniform, binary checkerboard, and Gaussian noise, asserting relative ordering of entropy and approximate expected ranges. For fractal_dimension, feed in a small set of representative beta values and confirm D matches closed-form expectations or pre-computed reference values within tight tolerances.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:15:47Z"
          },
          {
            "id": 5,
            "title": "Implement psd_anisotropy(gray) using FFT-based PSD analysis",
            "description": "Use 2D FFT-based power spectral density to quantify anisotropy_ratio, psd_aspect, and psd_theta for grayscale fields.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement psd_anisotropy(gray) that computes a 2D FFT, forms the power spectral density, converts to polar or elliptical coordinates, and estimates the dominant orientation and aspect ratio of spectral energy. Derive metrics including anisotropy_ratio, psd_aspect (major/minor axis ratio of spectral ellipse), and psd_theta (dominant orientation angle in degrees). Include options to window or detrend the field to avoid edge artifacts, and ensure the function is efficient enough for repeated use in a pipeline. Return both scalar metrics and any internal diagnostic values needed for QA threshold checks.",
            "status": "done",
            "testStrategy": "Write tests that generate synthetic anisotropic Gaussian fields (e.g., stretched in x or y and rotated patterns) and verify that psd_anisotropy correctly identifies the dominant orientation within a small angular tolerance and that psd_aspect and anisotropy_ratio increase with imposed anisotropy strength. Include a check that isotropic noise yields aspect ratios close to 1.0.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:15:47Z"
          },
          {
            "id": 6,
            "title": "Implement topology_metrics(mask) for area, compactness, connectivity, and SFI",
            "description": "Create topology_metrics utilities that compute per-facies area, compactness, connectivity, SFI, and related topology metrics from boolean masks.",
            "dependencies": [
              1
            ],
            "details": "Implement topology_metrics(mask) in stats.py to operate on labeled or boolean masks for individual facies and environments. Use connected-component analysis (e.g., SciPy ndimage) to compute component counts, areas, perimeters, and shape descriptors. From these, derive compactness indices, connectivity metrics (e.g., number of connected clusters, largest cluster fraction, percolation flags), and sinuosity or shape factor index (SFI) as defined in the PRD. Ensure the function can be applied per-mask (channel, bar, marsh, etc.) and returns a flat dict with names like area_channel, compact_channel, conn_channel, and SFI_channel consistent with the schema from subtask 1.",
            "status": "done",
            "testStrategy": "Add tests that construct simple synthetic masks (single rectangle, multiple disjoint blobs, simple sinuous shapes) and assert that areas are exact, compactness values follow expected ordering (e.g., circle > rectangle > elongated bar), connectivity metrics reflect the number of components, and SFI behaves consistently across shapes.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:15:47Z"
          },
          {
            "id": 7,
            "title": "Implement compute_metrics(gray, masks, env, metadata=None) orchestration",
            "description": "Build the main compute_metrics function to orchestrate all metric helpers, integrate metadata, and output a flat metrics dict.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Implement compute_metrics in stats.py so that it accepts gray, masks, env, and optional metadata from Tasks 2–6 (petrology, sedimentary structures, stacked packages). Within this function, call compute_variogram, fit_power_law, two_segment_fit, entropy, fractal_dimension, psd_anisotropy, and topology_metrics for relevant masks. Map all outputs into the schema from subtask 1, including beta_iso, beta_dir_*, anisotropy_ratio, beta_seg1, beta_seg2, h0, entropy_global, D, SFI, psd_aspect, psd_theta, area_*, compact_*, conn_*, mineralogical tags, QA flags, and stacked package metadata fields. Implement acceptance-band checks (e.g., braided PSD aspect > 2.0) that set QA flags or warnings, and ensure the function returns a flat dict of floats and tags suitable for CSV and reporting.",
            "status": "done",
            "testStrategy": "Create integration tests that feed compute_metrics with simple synthetic gray fields and masks plus minimal metadata, then assert that key metrics are present, internally consistent (e.g., summed areas, plausible beta ranges), and that QA flags are triggered when known out-of-band scenarios are injected (such as artificially extreme anisotropy for braided env).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:15:47Z"
          },
          {
            "id": 8,
            "title": "Add lightweight β/D/H adapter for interactive preview mode",
            "description": "Provide a fast adapter or mode that returns only β, D, and H-like metrics for interactive previews without full recomputation.",
            "dependencies": [
              3,
              4,
              7
            ],
            "details": "Design and implement either a dedicated function (e.g., compute_preview_metrics) or a mode flag on compute_metrics that computes and returns only the minimal set of preview metrics (e.g., beta_iso or dominant beta_dir, fractal dimension D, and a representative H parameter) using cached or simplified computations. Reuse intermediate results from prior full runs where possible or implement reduced-resolution / subsampling paths to keep latency low. Ensure the adapter has a stable, documented mini-schema and integrates cleanly with the interactive UX layer without breaking the main pipeline.",
            "status": "done",
            "testStrategy": "Add tests that call the preview adapter on the same synthetic inputs used for full compute_metrics and assert that β and D values closely match the full pipeline within relaxed tolerances, while verifying that the function executes significantly faster than the full metrics path on moderately sized grids (using simple timing thresholds if feasible).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:15:47Z"
          },
          {
            "id": 9,
            "title": "Create tests/test_stats.py with synthetic fields and regression fixtures",
            "description": "Develop a comprehensive test suite for stats.py using synthetic analytic fields, topology masks, and regression fixtures to ensure numerical stability.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "details": "Add tests/test_stats.py that consolidates unit and integration tests for all metric helpers and compute_metrics. Generate synthetic fields with known variogram slopes and PSD properties, plus simple topology masks, to validate each metric numerically within defined tolerances. Introduce regression fixtures (e.g., serialized metrics dicts or key subsets) for representative env configurations to detect unintended changes over time. Cover edge cases such as small images, empty masks, extreme anisotropy, and noisy data, and ensure tests are stable across platforms by fixing RNG seeds and binning parameters.",
            "status": "done",
            "testStrategy": "Implement pytest-based tests organized by metric type and integration level, using seeded NumPy RNGs and stored reference values. Include regression tests that compare selected metric subsets against saved fixtures, failing on deviations beyond specified relative/absolute tolerances to guard against subtle numerical regressions.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:15:47Z"
          },
          {
            "id": 10,
            "title": "Integrate compute_metrics into smoke tests and geologic documentation",
            "description": "Wire compute_metrics into scripts/smoke_test.py and update GEOLOGIC_RULES.md plus notebooks with formulas, anchors, and Phase 1 & 2 coverage.",
            "dependencies": [
              7,
              8,
              9
            ],
            "details": "Update scripts/smoke_test.py to call compute_metrics on representative outputs from fluvial generators and stacked packages, logging key metrics and QA flags as part of the smoke pipeline. Extend GEOLOGIC_RULES.md with a dedicated stats section that documents formulas, parameter choices, and metric definitions, and add corresponding anchors referring to fully qualified code names in stats.py. Update relevant Phase 1 and Phase 2 notebooks under notebooks/ with markdown cells explaining each implemented metric, linking to GEOLOGIC_RULES anchors, and demonstrating basic usage with plots or printed metrics. Ensure all anchors follow the required naming convention and that rules and notebooks remain in sync with the implemented code.",
            "status": "done",
            "testStrategy": "Add or update smoke-test logic to assert that compute_metrics runs without errors on a small set of generated examples and that critical metrics fall within broad geologically plausible bands. Manually or semi-automatically verify that notebook cells execute successfully and that GEOLOGIC_RULES.md anchors correctly reference the implemented functions, adjusting tests if there is an existing documentation-checking harness.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:15:47Z"
          }
        ],
        "complexity": 9,
        "recommendedSubtasks": 10,
        "expansionPrompt": "Expand Task 8 into ~10 subtasks to build the full metrics pipeline in src/analog_image_generator/stats.py, where compute_metrics currently raises NotImplemented. Subtasks should: (1) design the metrics result schema (keys, types, units) consistent with the PRD fields and downstream reporting expectations; (2) implement compute_variogram(gray, directions) and associated helpers for lag binning; (3) implement fit_power_law(beta) and two_segment_fit(lags) to estimate β, β_seg1, β_seg2, and h0 from variogram data; (4) implement entropy(gray) and fractal_dimension(beta) calculations; (5) implement psd_anisotropy(gray) using FFT-based PSD analysis to derive anisotropy_ratio, psd_aspect, psd_theta; (6) implement topology_metrics(mask) to compute area, compactness, connectivity, SFI, and related per-facies metrics; (7) build compute_metrics(gray, masks, env, metadata=None) to orchestrate all metric calculations, incorporating petrology/stacked metadata from Tasks 2–6 and producing a flat dict of floats and tags; (8) create a lightweight adapter function or mode for interactive previews that returns just β/D/H quickly; (9) write tests/test_stats.py with synthetic analytic fields (e.g., known PSD/variogram behavior and simple topology masks) plus regression fixtures to validate numerical stability and tolerance bands; (10) hook compute_metrics into scripts/smoke_test.py and update GEOLOGIC_RULES.md/notebooks with formula references and anchors.",
        "updatedAt": "2025-11-19T21:15:47Z"
      },
      {
        "id": 9,
        "title": "Implement reporting pipeline (CSV, per-env PDFs, master PDF)",
        "description": "Complete `reporting.py` to output metrics CSV, mosaics, per-environment PDF pages with histograms + tables, and master PDF with overview + appended reports.",
        "details": "- Build `build_reports(metrics_rows, output_dir)` to:\n  1. Write CSV (pandas) following schema in PRD.\n  2. Save mosaics: grayscale grid + facies color + legend from palettes for each env; include stacked boundary overlays when present.\n  3. Generate per-env PDF (ReportLab) with layout: title, side-by-side grayscale vs facies, β & D histograms (Matplotlib -> PNG -> embed), metrics summary table including PSD_AR, compactness, connectivity, petrology tags.\n  4. Assemble master PDF cover summarizing global stats, flagged violations, thumbnails, and append env PDFs via PyPDF2.\n- Integrate QA flags: highlight metric deviations beyond acceptance ranges, sed/petrology compliance, and cross-reference realization IDs.\n- Ensure reporting step consumes metadata from stats + stacked packages.\n- Provide CLI helper or doc snippet detailing invocation.\n- Reference `docs/PALETTES.md` for legend order consistency.\n- Update notebooks/workflow docs describing export steps.",
        "testStrategy": "- Create `tests/test_reporting.py` using tmp path to ensure CSV contains all columns and PDF files exist/non-empty; parse PDF metadata to confirm appended pages.\n- Mock metrics rows to trigger violation flag and assert it appears in generated summary table (use PyPDF2 text extraction or inspect ReportLab data structure before rendering).\n- Extend smoke script to run minimal batch (N=1 per env) piping through reporting and verifying outputs saved under `outputs/smoke/`.",
        "priority": "medium",
        "dependencies": [
          "7",
          "8"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define metrics_rows schema and CSV column mapping",
            "description": "Finalize the metrics_rows list-of-dicts schema and its mapping to the reporting CSV columns defined in the PRD.",
            "dependencies": [],
            "details": "Review the PRD, stats outputs, and stacked package metadata to enumerate all required CSV columns (e.g., env, realization_id, PSD_AR, compactness, connectivity, petrology tags, stacked package metadata, QA flags). Define a canonical metrics_rows dict schema in src/analog_image_generator/reporting.py, including key names, value types, and handling of optional fields. Document the schema in code docstrings and ensure it can ingest metadata from both the stats module and stacked channel packages without ambiguity.",
            "status": "done",
            "testStrategy": "Add a unit test (in subtask 8) that constructs a minimal metrics_rows sample and asserts that all expected keys are present and that missing optional fields are handled without errors when passed into build_reports.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:21:03Z"
          },
          {
            "id": 2,
            "title": "Implement deterministic CSV export in build_reports using pandas",
            "description": "Write the CSV export logic in build_reports so that it produces a stable, schema-compliant metrics CSV across runs.",
            "dependencies": [
              1
            ],
            "details": "In src/analog_image_generator/reporting.py, implement the first stage of build_reports to convert metrics_rows into a pandas DataFrame using the schema from subtask 1. Specify an explicit ordered list of columns to enforce deterministic column ordering and set dtypes where appropriate (e.g., numeric for metrics, boolean/string for QA flags, categorical for env). Write the CSV to output_dir (e.g., metrics.csv) with UTF-8 encoding and without index, and ensure repeated invocations with the same metrics_rows produce identical CSV byte content. Handle basic validation (e.g., missing required columns) with clear error messages suitable for CI.",
            "status": "done",
            "testStrategy": "In tests/test_reporting.py, call build_reports with a small metrics_rows sample and tmp_path; use pandas.read_csv to verify column order, dtypes for key fields, and value round-tripping, and optionally hash the CSV contents to confirm determinism across multiple runs.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:21:03Z"
          },
          {
            "id": 3,
            "title": "Implement mosaic generation helpers for grayscale, facies, legends, and stacked overlays",
            "description": "Create functions to generate and save per-environment mosaic images including grayscale, facies color, legends, and stacked boundary overlays.",
            "dependencies": [
              1
            ],
            "details": "Add helper functions in reporting.py (e.g., generate_env_mosaics or similar) that take per-environment gray arrays, facies masks, palette IDs, and stacked package boundary metadata to produce PNG mosaics using Matplotlib or Pillow. Implement side-by-side or grid layouts for grayscale and facies-color views, apply colors from docs/PALETTES.md in the documented legend order, and render legends that match the palette definitions. Overlay stacked boundary masks when present (e.g., semi-transparent lines or filled regions) and save deterministic filenames (e.g., mosaics/{env}_gray.png, {env}_facies.png, {env}_stacked.png) under output_dir, returning paths for later PDF embedding.",
            "status": "done",
            "testStrategy": "Extend tests/test_reporting.py to construct tiny synthetic grayscale and mask arrays plus a minimal palette, call the mosaic helper to generate PNGs in tmp_path, and assert that the expected files exist, are non-empty, and exhibit distinct colors per facies (e.g., by checking a few pixel values).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:21:03Z"
          },
          {
            "id": 4,
            "title": "Implement per-environment PDF page generation with mosaics, histograms, and summary tables",
            "description": "Use ReportLab to build per-environment PDFs that embed mosaics, β/D histograms, and metrics summary tables including QA flags.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement a function in reporting.py (e.g., build_env_report_pdf(env_name, env_rows, mosaic_paths, output_dir)) that uses ReportLab to create a single-environment PDF. Layout should include a title header, side-by-side grayscale and facies mosaics (using the paths from subtask 3), β and D histograms rendered via Matplotlib to temporary PNGs and embedded into the PDF, and a metrics summary table listing PSD_AR, compactness, connectivity, petrology tags, and any QA flags from env_rows. Ensure text for flagged metrics is clearly distinguished (e.g., bold or color) and that page size, fonts, and margins are consistent across environments. Save each env PDF under a deterministic name like pdf/{env}_report.pdf and return the paths to build_reports.",
            "status": "done",
            "testStrategy": "Add tests that call the per-environment PDF builder with a tiny metrics_rows subset and synthetic image paths (pointing to small PNGs created during the test). Use PyPDF2 to open the resulting env PDF from tmp_path, assert it has at least one page, and optionally search the extracted text for env_name and one known metric field.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:21:03Z"
          },
          {
            "id": 5,
            "title": "Implement master PDF assembly with global summary cover and appended env PDFs",
            "description": "Create a master report PDF that summarizes global stats on a cover page and appends all per-environment PDFs using PyPDF2.",
            "dependencies": [
              4
            ],
            "details": "In reporting.py, implement a function such as build_master_report(master_rows, env_pdf_paths, output_dir) that first uses ReportLab to create a master cover PDF containing high-level statistics (aggregated across metrics_rows), a list of environments, counts of realizations, and a summary of QA violations. Optionally embed small thumbnail mosaics or icons per environment. Then use PyPDF2 to merge the cover PDF with each per-environment PDF in a deterministic order (e.g., sorted by env name) into a single master PDF (e.g., pdf/master_report.pdf). Ensure file handles are closed, temporary cover PDFs are cleaned up, and that the resulting file has the expected number of pages and consistent metadata fields like title and subject.",
            "status": "done",
            "testStrategy": "Within tests/test_reporting.py, simulate a few dummy per-environment PDFs (or create them via the env report builder), call the master assembly function, and then use PyPDF2 to confirm that the resulting master PDF exists, has one more page than the total of env PDFs, and that the first page text includes a recognizable global summary title.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:21:03Z"
          },
          {
            "id": 6,
            "title": "Integrate QA flagging logic for metric deviations and sed/petrology compliance",
            "description": "Implement functions to compute and attach QA flags for out-of-range metrics and sed/petrology rule violations, and propagate them into CSV and PDFs.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Define acceptance ranges and rule checks based on the PRD and geologic rules (e.g., tolerances for PSD_AR, compactness, connectivity, beta/D distributions, and sedimentary or petrology consistency with environment type). Implement helper functions in reporting.py that take individual metrics_rows entries, compute QA flags (e.g., fields like qa_out_of_range, qa_sed_noncompliant, qa_petrology_mismatch, qa_notes), and merge them back into the row dict. Ensure build_reports applies this logic before CSV writing and PDF generation so that flags appear as dedicated CSV columns and are clearly indicated in the per-env tables and master summary. Keep the logic deterministic and easy to extend for new rules.",
            "status": "done",
            "testStrategy": "Add tests that construct metrics_rows with values intentionally inside and outside the acceptance ranges, run them through the QA helper, and assert that the expected boolean or string QA flag fields are set. In an integration-style test, call build_reports and use CSV + PDF inspection to verify that at least one known flag label appears in the outputs.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:21:03Z"
          },
          {
            "id": 7,
            "title": "Design deterministic output directory structure and naming conventions for reporting artifacts",
            "description": "Define and implement a clear directory layout and file-naming scheme for CSV, mosaics, and PDFs produced by build_reports.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Decide on a stable directory hierarchy under output_dir (e.g., metrics.csv at root, mosaics/ for image assets, pdf/ for per-env and master PDFs) and update build_reports to create these directories as needed. Standardize filename patterns that encode env names and realization IDs where appropriate, ensuring they are filesystem-safe and sortable (e.g., zero-padded indices). Reflect these conventions in all helper functions so that downstream tools and CI can rely on them. Update or add docstrings in reporting.py explaining the output layout for other modules and for notebook consumers.",
            "status": "done",
            "testStrategy": "Write tests that call build_reports with a small multi-env metrics_rows set and tmp_path, then assert that the resulting directory tree matches the expected layout (presence of metrics.csv, mosaics/, pdf/), that filenames follow the naming conventions, and that repeated runs with identical input do not create duplicate or conflicting artifacts.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:21:03Z"
          },
          {
            "id": 8,
            "title": "Create tests/test_reporting.py for CSV, PDFs, mosaics, and QA flags",
            "description": "Add a dedicated pytest module that exercises the reporting pipeline end-to-end using temporary directories and small synthetic metrics_rows.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Create tests/test_reporting.py that builds minimal yet representative metrics_rows covering multiple environments and QA cases, then invokes build_reports with tmp_path as output_dir. Assert that metrics.csv exists with the expected columns and schema, that mosaic PNGs and per-env PDFs are created and non-empty, and that the master PDF is present. Use PyPDF2 or ReportLab utilities to inspect PDFs for the presence of key strings like environment names and flag labels. Where practical, validate that QA flags appear in CSV and at least one PDF, and that no unhandled exceptions occur when stacked package metadata is present or absent.",
            "status": "done",
            "testStrategy": "Use pytest with tmp_path fixtures to isolate filesystem effects. Combine direct CSV inspection via pandas, filesystem checks for images and PDFs, and lightweight PDF parsing via PyPDF2 to confirm both existence and basic content expectations (e.g., text snippets and page counts).",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:21:03Z"
          },
          {
            "id": 9,
            "title": "Extend smoke tests and documentation to cover reporting pipeline usage and CI integration",
            "description": "Update smoke_test.py and project docs to demonstrate invoking build_reports and describe how reporting artifacts fit into the workflow and CI.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "details": "Modify scripts/smoke_test.py to generate a tiny synthetic metrics_rows dataset (or reuse existing stats outputs), call build_reports into a temporary or configured output directory, and log locations of the CSV and PDF artifacts as part of the smoke run. Update documentation files such as docs/WORKFLOW.md and docs/CODEX_RUNBOOK.md to describe the reporting step, including expected inputs, CLI invocation patterns (e.g., from the main pipeline script or notebook), output directory structure, and how CI jobs should collect and publish the CSV and PDF artifacts. Ensure the docs reference PALETTES and QA flag semantics so users understand how to interpret the reports.",
            "status": "done",
            "testStrategy": "Run the smoke_test script locally (or in CI) to ensure it completes without errors and produces reporting artifacts. For documentation, add a simple doc test or checklist item in the QA process to verify that the described commands match the actual behavior of the reporting pipeline and that CI successfully archives the generated CSV and PDFs.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:21:03Z"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 9,
        "expansionPrompt": "Expand Task 9 into ~9 subtasks building the reporting pipeline in src/analog_image_generator/reporting.py, where build_reports is currently a minimal stub. Subtasks should: (1) finalize the metrics_rows schema (list of dicts) and how it maps to the CSV columns defined by the PRD; (2) implement CSV export using pandas, ensuring stable column order and types across runs; (3) implement functions to generate mosaics (grayscale, facies color, legends, stacked boundary overlays) using Matplotlib or Pillow, driven by PALETTES and mask metadata; (4) implement per-environment PDF page creation with ReportLab, embedding mosaics, β/D histograms, and summary tables that include PSD_AR, compactness, connectivity, petrology tags, and QA flags; (5) implement assembly of a master PDF that summarizes global stats and appends per-env PDFs via PyPDF2; (6) integrate QA flagging logic that highlights metric deviations beyond acceptance ranges and sed/petrology compliance; (7) design the output directory structure and naming conventions, and ensure build_reports writes all artifacts deterministically; (8) create tests/test_reporting.py that use tmp paths and mocked/small metrics_rows to assert CSV content, PDF existence/non-emptiness, and presence of violation flags (via PyPDF2 or ReportLab structure inspection); (9) extend scripts/smoke_test.py and docs (WORKFLOW, CODEX_RUNBOOK) to document how to invoke the reporting pipeline and how it ties into CI.",
        "updatedAt": "2025-11-19T21:21:03Z"
      },
      {
        "id": 10,
        "title": "Ensure traceability, automation, and Task Master gating",
        "description": "Wire documentation, notebooks, and CI/smoke automation so every principle maps to code and Task Master enforces non-regression.",
        "details": "- Update `docs/GEOLOGIC_RULES.md` with table rows per new function (generators, stacked, stats, reporting) including notebook anchor IDs; sync anchors inside `notebooks/v20a_interactive_rebuild.ipynb` and `v20_complete_analysis.ipynb` (`anchor-<env>-<principle>` naming) referencing fully qualified code paths.\n- Refresh README + docs (WORKFLOW, TASKMASTER tasks) describing stacked/interactive/stats pipelines and slider sources; add meeting recap entry summarizing professor feedback.\n- Configure Task Master tag tree: parse PRD -> generate tasks -> ensure `.taskmaster/tasks/tasks.json` updated; reference new tasks (IDs) for each milestone.\n- Extend `scripts/smoke_test.py` + CI (GitHub Actions) to execute mini-run (generate 3 envs, compute metrics, build one PDF) and fail on anchor mismatch or reporting errors.\n- Document Task Master gating steps (`task-master analyze-complexity`, `expand`) and add instructions in `docs/CODEX_RUNBOOK.md` for MCP usage.\n- Capture outputs/anchors in meeting recap and ensure non-regression checklist in notebooks is fully ticked programmatically (e.g., cell that asserts all features returning True).",
        "testStrategy": "- Add doc-test automation: script verifying every GEOLOGIC_RULES row has matching notebook anchor + callable attribute; include as pytest test or CLI invoked in CI.\n- Update GitHub Actions workflow to run smoke script + doc-anchor check on pushes; ensure failing run blocks merge.\n- Manual validation: run Task Master commands locally (list/next/expand) ensuring no transport errors, capture logs for QA sign-off.",
        "priority": "medium",
        "dependencies": [
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Inventory new generator/stacked/stats/reporting functions for GEOLOGIC_RULES mapping",
            "description": "Scan the codebase to identify all newly added or changed functions in generators, stacked channel packages, statistics, and reporting that must appear in GEOLOGIC_RULES.md with code and notebook anchors.",
            "dependencies": [],
            "details": "Search under src/analog_image_generator (e.g., geologic_generators, stacked_channels, stats, reporting modules) and compile a structured list of fully qualified function names, their purpose, and associated environment/principle labels. Capture this inventory in a temporary markdown or JSON helper file that will drive updates to docs/GEOLOGIC_RULES.md and notebook anchors.",
            "status": "done",
            "testStrategy": "Manually spot-check the inventory against a few key modules (e.g., stacked_channels, reporting) to ensure no major functions are missing; optionally add a lightweight script that compares the inventory against existing GEOLOGIC_RULES entries to confirm coverage grows as expected.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 2,
            "title": "Update docs/GEOLOGIC_RULES.md with rows and code/notebook anchors for all new functions",
            "description": "Extend GEOLOGIC_RULES.md so every inventoried function has a table row with fully qualified code anchor and corresponding notebook anchor IDs following the agreed conventions.",
            "dependencies": [
              1
            ],
            "details": "Using the inventory from subtask 1, add or update rows in docs/GEOLOGIC_RULES.md for generators, stacked channel builders, stats, and reporting functions. Ensure each row includes: environment, principle, fully qualified code anchor (e.g., analog_image_generator.geologic_generators.meander_centerline), and one or more notebook anchors using the anchor-<env>-<principle> naming scheme that will be implemented in the v20 notebooks.",
            "status": "done",
            "testStrategy": "Render GEOLOGIC_RULES.md locally and visually confirm table formatting is intact, no broken markdown pipes exist, and a random sample of code anchors correspond to real functions using a quick Python REPL import test.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 3,
            "title": "Align notebook markdown anchors in v20 notebooks with GEOLOGIC_RULES conventions",
            "description": "Update markdown anchor cells in notebooks/v20a_interactive_rebuild.ipynb and v20_complete_analysis.ipynb so they follow the anchor-<env>-<principle> naming pattern and match the GEOLOGIC_RULES code anchor mapping.",
            "dependencies": [
              2
            ],
            "details": "Open notebooks/v20a_interactive_rebuild.ipynb and notebooks/v20_complete_analysis.ipynb, locate existing markdown anchors describing geologic principles, and normalize them to the anchor-<env>-<principle> format referenced from GEOLOGIC_RULES.md. Where needed, add or adjust markdown cells so each principle row in GEOLOGIC_RULES has at least one corresponding notebook anchor, and ensure any old anchor names are updated consistently across links or references in the notebooks.",
            "status": "done",
            "testStrategy": "Run both notebooks end-to-end in a local environment, then use the notebook search feature to confirm that all anchor-<env>-<principle> IDs listed in GEOLOGIC_RULES.md exist in at least one of the two v20 notebooks.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 4,
            "title": "Implement doc-anchor validation script for GEOLOGIC_RULES and notebooks",
            "description": "Create or update a validation script that parses GEOLOGIC_RULES.md, resolves each code anchor into a callable object, and verifies that each referenced notebook anchor exists in the v20 notebooks.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add a Python script (for example scripts/validate_geo_anchors.py or a tests helper) that reads docs/GEOLOGIC_RULES.md, extracts code anchor FQNs and notebook anchor IDs, imports the corresponding Python objects to ensure they are callable, and inspects notebooks/v20a_interactive_rebuild.ipynb and v20_complete_analysis.ipynb (via nbformat or similar) to confirm all listed anchor IDs are present. The script should emit a clear non-zero exit code and human-readable error messages when anchors are missing or imports fail.",
            "status": "done",
            "testStrategy": "Expose the script as a small CLI (`python scripts/validate_geo_anchors.py`) and run it locally, intentionally breaking one code anchor and one notebook anchor to verify that the script detects each problem and exits with failure; then restore the anchors and confirm a clean, passing run.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 5,
            "title": "Extend scripts/smoke_test.py with mini end-to-end run and anchor/report checks",
            "description": "Augment scripts/smoke_test.py to generate a small set of environments, compute metrics, build at least one PDF report, and assert that no anchor or reporting errors occur during the pipeline.",
            "dependencies": [
              4
            ],
            "details": "Modify scripts/smoke_test.py so it orchestrates a mini pipeline that runs the generators (including stacked mode), computes statistics, and calls the reporting functions to produce a metrics CSV and a minimal set of PDF outputs for three environments. Integrate calls to the doc-anchor validation logic (from subtask 4) within the smoke test so a failure in anchors or reporting raises an exception and causes the smoke run to fail, producing concise diagnostics for CI.",
            "status": "done",
            "testStrategy": "Execute `python scripts/smoke_test.py` locally, verify that it successfully produces three environments, metrics outputs, and at least one PDF when everything is configured correctly, and then temporarily corrupt a GEOLOGIC_RULES anchor to confirm that the smoke test fails fast with a clear error message.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 6,
            "title": "Integrate doc-anchor validation and smoke test into pytest or a dedicated CLI",
            "description": "Wire the doc-anchor check and mini smoke pipeline into the project’s automated test entry points so they can be invoked by pytest or a small CLI used in CI.",
            "dependencies": [
              4,
              5
            ],
            "details": "Either wrap the validate_geo_anchors script and smoke_test logic in pytest-compatible test functions (e.g., tests/test_doc_anchors.py and tests/test_smoke_pipeline.py) or expose them behind a dedicated CLI command such as `python -m analog_image_generator.checks.run_all`. Ensure the chosen approach provides clear test names, sensible exit codes, and configurable options (like skipping PDF generation in constrained environments if necessary).",
            "status": "done",
            "testStrategy": "Run `pytest` (or the new CLI command) locally and verify that both the doc-anchor validation and smoke pipeline checks execute, appear as separate tests or steps, and fail appropriately when underlying scripts are intentionally broken.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 7,
            "title": "Update GitHub Actions workflows to enforce unit, doc-anchor, and smoke gates",
            "description": "Modify the CI workflows (e.g., .github/workflows/ci.yml) to run unit tests, the doc-anchor validation, and the extended smoke test on pushes and pull requests, causing CI to fail on any regression.",
            "dependencies": [
              6
            ],
            "details": "Edit the existing GitHub Actions workflows or add a dedicated CI job so that each run installs dependencies, executes pytest (or the consolidated checks CLI), and specifically runs the doc-anchor and smoke tests introduced in subtasks 4–6. Ensure jobs are properly ordered, cache usage is reasonable, and any required artifacts (such as generated CSVs or PDFs) are either ignored or uploaded as build artifacts when useful for debugging.",
            "status": "done",
            "testStrategy": "Open a feature branch, push a commit that purposely breaks one code anchor in GEOLOGIC_RULES.md, and verify that the CI workflow fails on the doc-anchor or smoke step; then fix the anchor and confirm that CI returns to a green state.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 8,
            "title": "Refresh README and workflow docs for stacked/interactive/stats/reporting and Task Master usage",
            "description": "Update README.md, docs/WORKFLOW.md, docs/TASKMASTER_TASKS_EXPORT.md, and docs/CODEX_RUNBOOK.md so they accurately describe the stacked channel, interactive notebook, statistics, reporting pipelines, and how Task Master is used to drive and gate the workflow.",
            "dependencies": [
              7,
              10,
              11
            ],
            "details": "Revise the top-level README to include an overview diagram or narrative of the full pipeline from generators through stacked packages, metrics computation, and report generation, highlighting where Task Master fits. In docs/WORKFLOW.md, document the day-to-day development loop, including how to run doc-anchor checks and smoke tests. In docs/TASKMASTER_TASKS_EXPORT.md and docs/CODEX_RUNBOOK.md, explain how tasks are generated from PRDs, how tags map to milestones, how MCP is used from Cursor on WSL, and which commands must pass before merges are approved.",
            "status": "done",
            "testStrategy": "Manually review the updated docs for internal consistency, run any documented example commands (such as the smoke test and Task Master models/parse-prd invocations) in a local environment, and ask a teammate or future self to follow the README and WORKFLOW steps to confirm they can reproduce the described flows without surprises.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 9,
            "title": "Add programmatic non-regression checklist cells to v20 notebooks",
            "description": "Ensure the v20 interactive and analysis notebooks contain a final verification cell that programmatically asserts all major features and checklists are satisfied, and that this cell is used as part of the non-regression signal.",
            "dependencies": [
              3
            ],
            "details": "In notebooks/v20a_interactive_rebuild.ipynb and v20_complete_analysis.ipynb, add or update a final cell that imports a helper function (or directly performs checks) to assert that key features—such as stacked mode, sliders, metrics, and reporting hooks—return True or pass expected invariants. The cell should fail loudly when a feature is broken so that notebook runs clearly reflect non-regression status and can be referenced in GEOLOGIC_RULES and Task Master documentation.",
            "status": "done",
            "testStrategy": "Execute both notebooks from top to bottom, confirm that the new checklist cell runs successfully under a known-good state, then intentionally remove or alter one feature to verify that the checklist cell raises an error or returns False, signaling regression.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 10,
            "title": "Configure Task Master tags, tasks, and milestones for gating",
            "description": "Update .taskmaster configuration and task definitions so milestones, tags, and generated tasks reflect the new traceability and gating requirements, including references to doc-anchor checks, smoke tests, and notebook alignment work.",
            "dependencies": [
              6
            ],
            "details": "Using task-master CLI commands, adjust .taskmaster/config.json via supported commands and regenerate or update .taskmaster/tasks/tasks.json so that tasks and subtasks cover GEOLOGIC_RULES maintenance, notebook anchors, smoke/CI gates, and reporting traceability. Define or refine tags for roles and milestones (e.g., geo, gen, ux, stat, rep, qa, doc, m1–m6) and ensure that gating-related tasks explicitly call out the need for doc-anchor validation, smoke tests, and CI passing statuses before marking milestones complete.",
            "status": "done",
            "testStrategy": "Run `task-master list`, `task-master tags`, and `task-master next` to confirm the new tasks and tags appear as expected, then use `task-master analyze-complexity` and `task-master expand` on at least one gating-related task to ensure the updated configuration behaves correctly and references the right milestones.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 11,
            "title": "Verify Task Master MCP connectivity on WSL and document WSL-specific adjustments",
            "description": "Confirm that Cursor connects to the Task Master MCP server correctly on WSL with Codex as the main model, and capture any required WSL-specific configuration tweaks in the appropriate documentation file.",
            "dependencies": [
              10
            ],
            "details": "Using the prescribed wsl.exe wrapper configuration, restart Cursor, enable the task-master-ai MCP, and issue a few representative queries (listing tasks, expanding a task, and asking for next steps) to verify JSON-RPC stability and tool availability. If any adjustments to PATH, environment variables, or MCP configuration are required, update docs/TASKMASTER_WSL_SETUP.md or docs/CODEX_RUNBOOK.md so that future setup on WSL can follow a reliable, reproducible recipe.",
            "status": "done",
            "testStrategy": "From Cursor’s AI chat, run a short sequence of MCP-driven commands (e.g., list tasks, expand a specific gating task, and request help implementing it) and confirm they complete without transport errors; if errors are observed, iterate on configuration until the sequence passes and then repeat it to ensure stability.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          },
          {
            "id": 12,
            "title": "Perform final traceability alignment across GEOLOGIC_RULES, notebooks, code, CI, and Task Master",
            "description": "Execute a final holistic pass to ensure that GEOLOGIC_RULES.md, the v20 notebooks, core code modules, Task Master tasks/tags, and CI workflows all reference each other consistently and implement the intended traceability and gating behavior.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9,
              10,
              11
            ],
            "details": "Cross-check a representative subset of principles from GEOLOGIC_RULES.md against their code anchors, notebook anchors, doc descriptions, Task Master tasks, and CI steps. Verify that any change to a generator, stacked channel function, stats routine, or reporting function is discoverable through anchors and that failures in anchors, smoke tests, or notebooks will be caught by CI and reflected in Task Master gating tasks. Capture any residual gaps as follow-up tasks or documentation updates before marking the parent task complete.",
            "status": "done",
            "testStrategy": "Run the full automated test suite (including doc-anchor validation and smoke tests), confirm CI passes on a clean branch, execute the v20 notebooks, and then manually trace at least two complete examples from principle → GEOLOGIC_RULES row → notebook section → code implementation → Task Master task → CI gate to ensure there are no missing links or inconsistencies.",
            "parentId": "undefined",
            "updatedAt": "2025-11-19T21:27:29Z"
          }
        ],
        "complexity": 10,
        "recommendedSubtasks": 12,
        "expansionPrompt": "Expand Task 10 into ~12 subtasks that wire together documentation, notebooks, CI, and Task Master automation for full traceability and gating. Subtasks should: (1) systematically update docs/GEOLOGIC_RULES.md to include all newly implemented functions from generators, stacked_channels, stats, and reporting, with correct fully-qualified code anchors; (2) update notebook markdown anchors in notebooks/v20a_interactive_rebuild.ipynb and v20_complete_analysis.ipynb to match the anchor-<env>-<principle> convention and code anchors; (3) create or update a doc-test script (e.g., in scripts/ or tests/) that crawls GEOLOGIC_RULES.md, verifies that each Code Anchor resolves to a callable attribute, and that corresponding notebook anchors exist; (4) extend scripts/smoke_test.py to run a mini end-to-end pipeline (generate 3 envs, compute metrics, build one PDF) and assert no anchor/reporting errors; (5) integrate the doc-anchor check and smoke_test into pytest or a dedicated CLI invoked in CI; (6) update GitHub Actions workflows (e.g., .github/workflows/ci.yml) to run unit tests, doc-anchor validation, and smoke tests on push/PR, failing on any regression; (7) refresh README.md, docs/WORKFLOW.md, docs/TASKMASTER_TASKS_EXPORT.md, and docs/CODEX_RUNBOOK.md to describe the updated stacked/interactive/stats/reporting workflows and Task Master usage; (8) ensure notebook non-regression checklists are programmatically ticked by adding a small verification cell that asserts all major features return True; (9) configure Task Master tags and tasks via .taskmaster/ configs so that PRD parsing, analyze-complexity, and expand operations reference the new milestones and task IDs; (10) verify Task Master MCP connectivity and update docs/TASKMASTER_WSL_SETUP.md or CODEX_RUNBOOK.md with any WSL-specific adjustments; (11) document Task Master gating steps (what must be green before merge) and ensure this gating logic is reflected in CI and project docs; (12) perform a final alignment pass ensuring GEOLOGIC_RULES, notebooks, code, and Task Master task metadata all reference each other consistently.",
        "updatedAt": "2025-11-19T21:27:29Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-11-19T20:47:25.592Z",
      "taskCount": 10,
      "completedCount": 5,
      "tags": [
        "fluvial-v1"
      ],
      "created": "2025-11-19T20:50:34.011Z",
      "description": "Tasks for fluvial-v1 context",
      "updated": "2025-11-19T20:50:34.011Z"
    }
  },
  "aeolian-v1": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Aeolian parameter schema and base field scaffolding",
        "description": "Create a reusable parameter dataclass + helper utilities that normalize all aeolian slider inputs, seed RNG deterministically, and prepare base grids shared by all dune styles.",
        "details": "Define `AeolianParams` (env, theta_deg, q, H, lambda_px, f_interdune, defect_rate, seed, invert_gradation, cement_type, event_schedule) in `geologic_generators.py` or a new `aeolian_params.py`, enforcing PRD ranges and citing research PDFs in docstrings. Add helper functions for rotating coordinate grids, sampling base noise, and gating sequential steps. Wire `docs/GEOLOGIC_RULES.md` and `notebooks/aeolian.ipynb#anchor-aeolian-params` to the new class.\nPseudo-code:\n```\n@dataclass\nclass AeolianParams:\n    env: Literal[\"barchan\",\"linear\",\"transverse\"]\n    theta_deg: float = clamp(theta, 0, 180)\n    ...\n    def rng(self) -> np.random.Generator:\n        return np.random.default_rng(self.seed)\n\ndef make_base_field(params):\n    grid = gaussian_noise(params)\n    return rotate(grid, params.theta_deg)\n```\nUpdate `docs/WORKFLOW.md` to mention Aeolian slider defaults and link associated notebook anchors.",
        "testStrategy": "Add `tests/test_aeolian_params.py` with pytest cases that (a) clamp each slider to PRD ranges, (b) verify seeds yield identical grids, and (c) ensure invalid env names raise `ValueError`. Use hypothesis-style parametrization across ranges.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Draft AeolianParams schema and module layout",
            "description": "Design the AeolianParams dataclass shape and decide where it lives in the package.",
            "dependencies": [],
            "details": "Survey existing geologic generator modules under src/analog_image_generator to align naming and patterns, then draft the AeolianParams field list (env, theta_deg, q, H, lambda_px, f_interdune, defect_rate, seed, invert_gradation, cement_type, event_schedule) with clear Python types and sensible defaults. Decide whether to place the dataclass in src/analog_image_generator/geologic_generators.py or a dedicated src/analog_image_generator/aeolian_params.py module, and sketch how downstream functions (generate_aeolian, aeolian_*_ridges, etc.) will import it. Capture PRD-backed ranges and units for each parameter from research-documents/README.md and GEOLOGIC_RULES so later validation logic is straightforward.",
            "status": "pending",
            "testStrategy": "Manual design review against PRD tables and existing GEOLOGIC_RULES entries to confirm fields, types, and defaults are complete and consistent before coding."
          },
          {
            "id": 2,
            "title": "Implement AeolianParams dataclass with PRD-backed validation and normalization",
            "description": "Code the AeolianParams dataclass, enforcing PRD ranges and normalizing all aeolian slider inputs.",
            "dependencies": [
              1
            ],
            "details": "Implement the AeolianParams dataclass in the chosen module with type annotations, defaults, and a __post_init__ (or factory constructor) that clamps or validates each numeric field (theta_deg, q, H, lambda_px, f_interdune, defect_rate) to the ranges specified in the PRD and research PDFs. Enforce env as a Literal of allowed styles (barchan, linear, transverse), normalize booleans like invert_gradation, and validate cement_type and event_schedule structure. Include docstrings citing the relevant research PDFs and PRD sections, and ensure error handling (e.g., ValueError) is informative for out-of-range or unsupported combinations.",
            "status": "pending",
            "testStrategy": "Plan unit tests to instantiate AeolianParams with in-range and out-of-range values and assert that clamping, normalization, and validation errors behave as specified by the PRD."
          },
          {
            "id": 3,
            "title": "Provide deterministic RNG helper bound to AeolianParams",
            "description": "Wire deterministic RNG seeding into AeolianParams using utils.seeded_rng or an equivalent helper.",
            "dependencies": [
              2
            ],
            "details": "Add an rng() method or equivalent helper on AeolianParams that returns a numpy.random.Generator seeded deterministically from the seed field (and optionally env or other stable identifiers), reusing an existing utils.seeded_rng helper if available or implementing a small wrapper if not. Ensure the generator is reproducible across runs for the same AeolianParams instance, document the seeding strategy in the class docstring, and establish a clear pattern for downstream aeolian functions to either accept a Generator or call params.rng() for randomness.",
            "status": "pending",
            "testStrategy": "Plan tests where multiple AeolianParams instances with identical seeds generate identical random number sequences or base fields, and instances with different seeds produce distinct sequences."
          },
          {
            "id": 4,
            "title": "Implement base-field utilities for aeolian grids, rotation, and base noise",
            "description": "Create shared base-field utilities that build coordinate grids, rotate them, and sample base noise for all aeolian dune styles.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement utility functions such as make_base_field(params), make_coord_grid(shape or resolution, params), rotate_grid(grid, theta_deg), and sample_base_noise(params, rng) in the chosen aeolian module, using numpy to construct base Gaussian or other simple noise fields. Ensure these utilities accept AeolianParams (and optionally an explicit RNG from params.rng()) and apply wind azimuth theta_deg consistently when rotating grids. Keep implementations lightweight but reusable across barchan, linear, and transverse generators, and gate sequential steps (e.g., base field creation, rotation, optional post-processing) in a clear, composable way suitable for future extension.",
            "status": "pending",
            "testStrategy": "Plan to exercise these helpers in tests by generating small grids, checking that rotations respect theta_deg, and verifying that repeated calls with the same seed and parameters yield identical base fields."
          },
          {
            "id": 5,
            "title": "Wire AeolianParams and helpers into GEOLOGIC_RULES and aeolian notebook anchors",
            "description": "Update GEOLOGIC_RULES.md, notebooks/aeolian.ipynb, and docs/WORKFLOW.md to reference the new AeolianParams schema and base-field utilities.",
            "dependencies": [
              1,
              2,
              4
            ],
            "details": "Edit docs/GEOLOGIC_RULES.md to add or update entries for the Aeolian parameter schema and base-field utilities, including fully qualified code anchors such as analog_image_generator.aeolian_params.AeolianParams or the chosen module path. Update notebooks/aeolian.ipynb at anchor-aeolian-params to describe each parameter, its PRD-backed range, units, and intended slider representation, and to reference the implemented class and helper functions. Adjust docs/WORKFLOW.md to mention Aeolian slider defaults, link to the aeolian notebook anchor and GEOLOGIC_RULES entry, and ensure that anchor IDs and code anchors stay in sync with the new implementations.",
            "status": "pending",
            "testStrategy": "Manual validation by rendering the notebook and markdown docs, confirming that anchors resolve correctly, code references import without errors, and parameter descriptions match the implemented schema and PRD ranges."
          },
          {
            "id": 6,
            "title": "Add pytest coverage for AeolianParams validation, RNG, and base-field utilities",
            "description": "Create tests/test_aeolian_params.py with parametrized and property-style tests covering clamping, validation errors, and deterministic seeding.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Add tests/test_aeolian_params.py that uses pytest (and optional hypothesis-style patterns or parametrization) to verify AeolianParams behavior: numeric sliders are clamped or rejected according to PRD ranges, invalid env or cement_type values raise ValueError with clear messages, and invert_gradation/event_schedule fields behave as expected. Include tests that construct AeolianParams with fixed seeds and assert that calls into base-field utilities (e.g., make_base_field) produce identical grids for the same parameters and differing grids for different seeds. Add additional checks that grid rotation respects theta_deg and that base-noise statistics stay within reasonable bounds to catch regressions in the utilities.",
            "status": "pending",
            "testStrategy": "Implement pytest test cases and parametrized scenarios in tests/test_aeolian_params.py to cover clamping, error conditions, RNG determinism, and simple invariants on generated base fields; run the test suite to confirm all new tests pass."
          }
        ]
      },
      {
        "id": 2,
        "title": "Build Barchan dune generator with crest/slipface/interdune masks",
        "description": "Implement horned barchan synthesis driven by wind azimuth, including mask outputs for crest, slipface, stoss, and interdune plus migration metadata.",
        "details": "Add helpers (`aeolian_make_crests`, `aeolian_add_slipfaces`, `aeolian_interdune`) that construct horn seeds, advect dunes downwind, and carve interdune lows via distance transforms using params from Task 1. Compose masks dict with grayscale shading and palette keys per `docs/PALETTES.md`. Capture horn curvature vs. θ and record interdune fraction for metrics.\nPseudo-code:\n```\ndef build_barchan_field(params):\n    centerline = bezier(seed_points(params.seed))\n    horns = dilate(centerline, horn_width(theta))\n    slipface = project_downwind(horns, params.theta_deg)\n    interdune = threshold(base_field, params.f_interdune)\n    return {\n        \"gray\": blend_layers(...),\n        \"masks\": {\"crest\": crest_mask, \"slipface\": slipface, ...}\n    }\n```\nAnnotate GEOLOGIC_RULES rows for crest/slipface/interdune anchors once implemented.",
        "testStrategy": "Create `tests/test_barchan_generator.py` to assert (a) crest orientation aligns with θ within 15°, (b) ridge continuity index falls in 0.3–0.5 band, and (c) interdune connectivity stays between 0.2–0.6. Use seeded params and measure via numpy morphology helpers.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design barchan builder API and AeolianParams integration",
            "description": "Define a clear Python API for a barchan-specific field builder that consumes AeolianParams and base-field helpers from Task 1 in geologic_generators.py.",
            "dependencies": [],
            "details": "Add a function such as analog_image_generator.geologic_generators.build_barchan_field(params: AeolianParams, rng=None) that returns (gray, masks_dict, metadata). Reuse or wrap the base-field and wind-azimuth utilities from Task 1 rather than duplicating logic. Document expected params (grid size, theta_deg, horn spacing, dune height scale, interdune fraction, seed, etc.) in the function docstring and ensure names align with Task 1 schema and docs/GEOLOGIC_RULES.md aeolian rows. Decide on mask keys ('crest','slipface','stoss','interdune') and metadata structure so later stats/reporting tasks can rely on a stable contract.",
            "status": "pending",
            "testStrategy": "Add a lightweight smoke test or doctest to ensure build_barchan_field accepts an AeolianParams instance, returns the expected tuple shape, and works with the same RNG seeding conventions as Task 1."
          },
          {
            "id": 2,
            "title": "Implement aeolian_make_crests to seed horned barchan crests",
            "description": "Create aeolian_make_crests to construct horned barchan crest seeds consistent with GEOLOGIC_RULES and reference PDF.",
            "dependencies": [
              1
            ],
            "details": "In geologic_generators.py, implement aeolian_make_crests(base_field, params, rng) that lays out crescent-shaped crest seeds aligned with the incoming wind direction. Use seed points derived from grid dimensions, spacing controls, and params.seed, then trace crescent centerlines using simple Bezier or spline curves. Ensure horn orientation and aspect ratios match qualitative rules in docs/GEOLOGIC_RULES.md and research-documents/depositional_system_aeolian_barchan-dune.pdf (e.g., lee side downwind, horns pointing downwind). Output a binary or float mask for initial crests plus any intermediate centerline representation needed by downstream horn-growth logic.",
            "status": "pending",
            "testStrategy": "Create a unit test that calls aeolian_make_crests with a small synthetic base_field and fixed seed, then asserts that nonzero crest pixels exist, roughly follow a crescent geometry, and are oriented broadly perpendicular to the wind direction within a permissive band."
          },
          {
            "id": 3,
            "title": "Implement horn growth and downwind migration operations",
            "description": "Grow barchan horns from crest seeds and advect dunes downwind using geometric and morphological primitives.",
            "dependencies": [
              2
            ],
            "details": "Using the crest seeds and centerlines from aeolian_make_crests, implement horn growth as a series of dilations and directional thickening operations that extend horns outward from the central slipface. Represent centerlines with Bezier paths or parametric curves to control curvature vs. theta_deg, then rasterize these curves into masks. Apply downwind migration by translating or shearing dune bodies along the wind azimuth, updating both crest and stoss regions. Use numpy and scipy.ndimage (or equivalent) for dilation, distance transforms, and rotations while keeping operations deterministic for a given seed.",
            "status": "pending",
            "testStrategy": "Write tests that generate a small field with a known theta_deg and verify that horn centroids are displaced downwind relative to their seed positions and that horn curvature changes predictably when theta_deg is modified between two runs."
          },
          {
            "id": 4,
            "title": "Implement aeolian_add_slipfaces to project lee-side slipfaces",
            "description": "Add aeolian_add_slipfaces to construct slipface masks lee of the wind based on grown horns and crest positions.",
            "dependencies": [
              2,
              3
            ],
            "details": "Define aeolian_add_slipfaces(crest_mask, horn_mask, params) that projects slipfaces downwind from the crest line, filling in a lee-side wedge consistent with barchan geometry. Use the wind azimuth (theta_deg) to orient the slipface region, e.g., by computing a normal vector and offsetting pixels downwind with tapering thickness. Enforce that slipfaces attach smoothly to crests and horns without gaps, and keep a complementary stoss mask that occupies the upwind side of each dune. Ensure these slipface and stoss masks respect pixel-level exclusivity rules described in docs/GEOLOGIC_RULES.md for aeolian facies.",
            "status": "pending",
            "testStrategy": "Add tests that construct simple synthetic crest/horn inputs, run aeolian_add_slipfaces, and measure that slipface pixels lie predominantly downwind of crests (e.g., projected centroid angle within 15° of theta_deg) while stoss pixels lie upwind, with no overlap between slipface and stoss masks."
          },
          {
            "id": 5,
            "title": "Implement aeolian_interdune to carve interdune corridors",
            "description": "Create aeolian_interdune to carve interdune lows and corridors using distance transforms or morphology while controlling interdune fraction.",
            "dependencies": [
              3,
              4
            ],
            "details": "Implement aeolian_interdune(dune_body_mask, params) that uses distance transforms, erosion, or skeletonization to identify saddle regions between dunes and carve interdune corridors. Use params.f_interdune and any spacing controls from Task 1 to threshold the distance field, targeting a configurable global interdune area fraction. Orient corridors broadly along or across wind as dictated by GEOLOGIC_RULES for barchan fields. Return an interdune binary mask that is mostly disjoint from dune bodies and that exposes knobs to tune corridor width and connectivity for later metrics.",
            "status": "pending",
            "testStrategy": "Add tests that run aeolian_interdune on a synthetic dune_body_mask and verify that (a) interdune pixels occur mainly between dune clusters, (b) the measured interdune area fraction is within a tolerance of params.f_interdune, and (c) interdune regions form connected corridors rather than isolated noise blobs for default parameters."
          },
          {
            "id": 6,
            "title": "Compose grayscale shading and masks dict aligned with palettes",
            "description": "Combine crest, slipface, stoss, and interdune masks into grayscale shading and a masks dict with keys aligned to PALETTES and GEOLOGIC_RULES anchors.",
            "dependencies": [
              1,
              4,
              5
            ],
            "details": "Inside build_barchan_field, blend base topography, dune height fields, and mask information into a single grayscale image that encodes relative elevation or facies intensity. Construct a masks dict with keys exactly matching docs/PALETTES.md and GEOLOGIC_RULES.md for aeolian barchan entries, including at minimum 'crest', 'slipface', 'stoss', and 'interdune'. Ensure masks are mutually consistent (no unexpected overlaps unless explicitly allowed), normalized to boolean or {0,1} dtype, and that grayscale shading assigns visually distinct tonal ranges for each facies when mapped through the palette. Update GEOLOGIC_RULES anchors to reference build_barchan_field and these mask keys once behavior is stable.",
            "status": "pending",
            "testStrategy": "Create a test that calls build_barchan_field with a small grid and checks that the returned masks dict contains all required keys, that each mask has the same shape as gray, and that crest/slipface/stoss/interdune masks have nonzero coverage with minimal overlap ratios."
          },
          {
            "id": 7,
            "title": "Capture horn curvature and interdune fraction as generator metadata",
            "description": "Compute horn curvature vs wind direction and interdune fraction metrics and attach them as structured metadata.",
            "dependencies": [
              3,
              5,
              6
            ],
            "details": "Extend build_barchan_field to compute quantitative horn curvature metrics (e.g., mean curvature of horn centerlines, curvature at horn tips) and compare their orientation to the wind azimuth theta_deg. Measure interdune fraction as the ratio of interdune pixels to the total active aeolian area, and optionally include connectivity statistics such as the size of the largest interdune component. Package these values into a metadata dict (e.g., metadata['horn_curvature_vs_theta'], metadata['interdune_fraction'], metadata['interdune_connectivity']) returned alongside gray and masks. Ensure field names and units are documented for later use in stats.py and reporting tasks.",
            "status": "pending",
            "testStrategy": "Add tests that run build_barchan_field for a fixed seed and verify that metadata fields exist, are finite numeric values, and that the interdune_fraction closely matches the measured area fraction from the interdune mask while horn curvature statistics vary in sensible ways when horn geometry parameters are changed."
          },
          {
            "id": 8,
            "title": "Integrate barchan builder with generate_aeolian entry points",
            "description": "Wire the new barchan generator into generate_aeolian or a dedicated entry function so the environment can request barchan fields by mode.",
            "dependencies": [
              1,
              6,
              7
            ],
            "details": "Update geologic_generators.py (and any higher-level factory such as generate_aeolian) to call build_barchan_field when params.env=='aeolian' and params.dune_style or similar indicates 'barchan'. Ensure the integration path respects existing interfaces by returning (gray, masks, metadata) in the expected shape and by passing through AeolianParams from Task 1 without re-validation boilerplate. If needed, add a barchan-specific wrapper like generate_barchan_aeolian that is used by interactive notebooks and scripts, and ensure GEOLOGIC_RULES and notebook anchors reference this function for the barchan environment rows.",
            "status": "pending",
            "testStrategy": "Add an integration test that calls generate_aeolian (or the new wrapper) with a configuration selecting barchan dunes and asserts that the underlying build_barchan_field is invoked (via monkeypatch or metadata flag), and that the returned masks and metadata include barchan-specific keys like horn curvature and interdune_fraction."
          },
          {
            "id": 9,
            "title": "Add tests/test_barchan_generator.py for orientation and connectivity",
            "description": "Create a dedicated pytest module that validates barchan generator orientation, ridge continuity bands, and interdune connectivity using seeded runs.",
            "dependencies": [
              2,
              3,
              5,
              6,
              7,
              8
            ],
            "details": "Add tests/test_barchan_generator.py that constructs one or more AeolianParams fixtures with fixed seeds and theta_deg values, runs the full barchan generation path, and computes geometric diagnostics using numpy/scipy morphology helpers. Implement checks that (a) crest orientation aligns with theta_deg within about 15°, e.g., via FFT-based orientation or structure tensor analysis, (b) a ridge continuity index (largest crest component area divided by total crest area) falls within the 0.3–0.5 band for default barchan parameters, and (c) interdune connectivity, measured as the largest interdune component area divided by total interdune area or 1−χ/A, lies between 0.2 and 0.6. Keep tests deterministic by seeding RNG and keep grids small enough for fast execution.",
            "status": "pending",
            "testStrategy": "Use pytest with seeded AeolianParams fixtures to run build_barchan_field end-to-end and compute orientation, continuity, and connectivity metrics; mark tests as fast and ensure they run without optional GUI or notebook dependencies."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Linear/Seif dune generator with ridge continuity controls",
        "description": "Generate linear dune fields that honor mean wind direction, secondary wind kinks, and target ridge continuity metrics.",
        "details": "Add `aeolian_linear_ridges` to extrude sinusoidal ridges along θ_wind, apply secondary wind perturbations, and cut interdune corridors. Compute ridge continuity metric (largest crest component / total crest pixels) and expose it for stats. Respect defect rate ρ by randomly removing segments. Update masks to include ridge/stoss/slipface states, storing continuity metadata.\nPseudo-code:\n```\ndef build_linear_field(params):\n    spines = seed_parallel_lines(theta=params.theta_deg, spacing=params.lambda_px)\n    spines = add_secondary_kinks(spines, secondary_theta=params.theta_deg+30*q)\n    crest_mask = enforce_continuity(spines, rho=params.defect_rate)\n    metrics = {\"ridge_continuity\": largest_component(crest_mask)}\n    return crest_mask, metrics\n```\nDocument anchor `aeolian_ridge_continuity` and note default ranges from linear-dune PDF.",
        "testStrategy": "Unit tests should (a) compute ridge continuity ≥0.7 for default q, (b) drop into 0.5–0.6 when ρ≈0.25, and (c) confirm PSD orientation difference ≤10°. Use mocked params to compare measured orientation via FFT vs. θ.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define aeolian_linear_ridges builder API and scaffolding",
            "description": "Design and stub an aeolian_linear_ridges (or equivalent) builder in geologic_generators.py that accepts AeolianParams and shared base fields and fits into the existing generate_aeolian dispatch pattern.",
            "dependencies": [],
            "details": "Create a new aeolian_linear_ridges (or build_linear_field) function in src/analog_image_generator/geologic_generators.py that consumes AeolianParams plus any precomputed base grids from Task 1, and returns a grayscale image plus a masks_dict and metrics dict. Ensure the signature and return types are consistent with the emerging aeolian API (e.g., barchan generator), including keys for crest, stoss, slipface, and interdune masks, and a slot for aeolian_ridge_continuity. Wire the function into generate_aeolian or the appropriate registry so that the linear style can be selected by env or style flag without breaking existing stubs.",
            "status": "pending",
            "testStrategy": "Add a minimal unit test that imports aeolian_linear_ridges, calls it with a simple mocked AeolianParams and base grid, and asserts that it returns a grayscale array, a masks_dict with expected keys, and a metrics dict containing an aeolian_ridge_continuity entry (even if temporarily set to NaN)."
          },
          {
            "id": 2,
            "title": "Implement parallel spine seeding aligned with wind direction and spacing",
            "description": "Create a spine seeding routine that generates approximately parallel ridge centerlines aligned with params.theta_deg and spaced using params.lambda_px and q.",
            "dependencies": [
              1
            ],
            "details": "Implement a helper such as seed_parallel_lines that uses Aeolian coordinate rotation utilities to generate a set of line centerlines (spines) across the domain at orientation theta_deg. Use params.lambda_px as the nominal spacing between spines, modulated if needed by transport parameter q, and ensure full coverage of the simulation domain including edges. Represent spines in a convenient form (e.g., arrays of coordinates or a binary mask) that will later be rasterized into crest_mask and can be perturbed by secondary wind effects.",
            "status": "pending",
            "testStrategy": "Write tests that construct a small grid with known theta_deg and lambda_px, call the spine seeding helper, and verify that the mean spacing between spine centrelines matches lambda_px within a tolerance and that the dominant orientation, estimated via a simple FFT or gradient-based method, is within a few degrees of theta_deg."
          },
          {
            "id": 3,
            "title": "Add secondary wind kink perturbations based on research PDF",
            "description": "Superimpose secondary wind-induced kinks onto the linear spines using a secondary direction function informed by the linear-dune research PDF and q.",
            "dependencies": [
              2
            ],
            "details": "Extend the spine generation pipeline with an add_secondary_kinks step that perturbs the base parallel spines according to a secondary wind direction (e.g., theta_deg plus an offset that scales with q or a separate parameter). Implement sinusoidal or piecewise-linear kinks along each spine with controllable wavelength and amplitude, referencing guidance from research-documents/depositional_system_aeolian_linear-dune.pdf in docstrings. Ensure the perturbations preserve overall ridge continuity and do not self-intersect excessively, producing geologically plausible linear or seif dune shapes.",
            "status": "pending",
            "testStrategy": "Create tests that compare kinked vs. unkinked spines: verify that kink amplitude and frequency respond monotonically to q (or the chosen secondary parameter) and that 2D power spectra or orientation histograms exhibit a secondary lobe corresponding to the secondary wind direction while maintaining the primary orientation close to theta_deg."
          },
          {
            "id": 4,
            "title": "Construct crest mask and aeolian_ridge_continuity metric via components",
            "description": "Rasterize perturbed spines into a crest_mask and implement a ridge continuity metric based on connected-component analysis of crest pixels.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement a rasterization step that converts spine geometries into a binary crest_mask at image resolution, ensuring consistent crest thickness and connectivity. Using scipy.ndimage or skimage, perform connected-component labeling on crest_mask to compute the size of each crest component, then define aeolian_ridge_continuity as the ratio of the largest component area to the total number of crest pixels. Store this metric in the metrics dict alongside any auxiliary values (e.g., number of components) while keeping the definition stable for downstream stats.compute_metrics.",
            "status": "pending",
            "testStrategy": "Add unit tests that build synthetic crest_mask patterns (e.g., a single continuous ridge, multiple short segments) and feed them into the continuity routine, asserting that the resulting aeolian_ridge_continuity is between 0 and 1 and behaves as expected (near 1.0 for one continuous ridge, significantly lower when crest pixels are split into many small segments)."
          },
          {
            "id": 5,
            "title": "Implement interdune corridor carving and stoss/slipface mask differentiation",
            "description": "Derive interdune corridors from the crest_mask and generate separate stoss and slipface masks consistent with wind direction and dune morphology.",
            "dependencies": [
              4
            ],
            "details": "Using the crest_mask and wind direction, implement operations that carve interdune corridors by applying distance transforms, thresholding, and morphological opening or erosion to separate interdune lows from ridge bodies. Use wind-aligned gradients or rotated coordinates to classify pixels on the upwind side of crests as stoss and the downwind side as slipface, producing mutually exclusive crest, stoss, slipface, and interdune masks. Ensure the resulting masks respect AeolianParams (e.g., f_interdune controls mean interdune area fraction) and are suitable for later gray shading and facies mapping.",
            "status": "pending",
            "testStrategy": "Create tests that run the full mask construction on small grids with controlled parameters, then verify that crest, stoss, slipface, and interdune masks are disjoint, that their union covers the active aeolian area, and that the measured interdune fraction is within an acceptable tolerance of f_interdune. Also check that slipface pixels lie on the lee side of crests relative to theta_deg using simple geometric checks."
          },
          {
            "id": 6,
            "title": "Apply defect_rate-driven crest breaking while preserving interpretable continuity",
            "description": "Introduce defect_rate-controlled breaking of crest segments and ensure the aeolian_ridge_continuity metric degrades smoothly and remains interpretable for stats.",
            "dependencies": [
              4,
              5
            ],
            "details": "Implement a routine that takes an initial crest_mask and probabilistically removes or gaps crest segments according to params.defect_rate, for example by sampling segments or along-crest positions to delete while avoiding total ridge destruction. Recompute aeolian_ridge_continuity on the modified crest_mask and verify that continuity decreases monotonically with increasing defect_rate but remains stable enough for comparison across runs. Expose any additional metadata needed (e.g., number of breaks, mean segment length) in the metrics dict without changing the core continuity definition.",
            "status": "pending",
            "testStrategy": "Design tests that generate linear dune fields at fixed theta_deg and q while varying defect_rate (e.g., 0.0, 0.1, 0.25), and assert that aeolian_ridge_continuity for the default case is high (around the specified ≥0.7 target) and decreases into the expected 0.5–0.6 band or lower as defect_rate approaches 0.25, with low variance across multiple seeds."
          },
          {
            "id": 7,
            "title": "Finalize aeolian_linear_ridges outputs compatible with stats.compute_metrics",
            "description": "Ensure the linear dune generator returns grayscale, masks_dict, and metrics in a schema consistent with the stats engine and other aeolian generators.",
            "dependencies": [
              1,
              5,
              6
            ],
            "details": "Refine the aeolian_linear_ridges (or build_linear_field) return structure so that it includes a primary grayscale image, a masks_dict with standardized keys (e.g., crest, stoss, slipface, interdune, and any ridge index masks), and a metrics dict containing aeolian_ridge_continuity and orientation statistics derived from PSD analysis. Align metric names, units, and types with the expectations of stats.compute_metrics and any shared Aeolian helpers so that Task 8 can consume the continuity metric without special casing. Update generate_aeolian wiring if necessary to route aeolian style selection and outputs consistently.",
            "status": "pending",
            "testStrategy": "Add a small integration-style test that calls aeolian_linear_ridges with realistic AeolianParams, passes its outputs into a lightweight stub or existing stats.compute_metrics interface, and asserts that all required metrics keys are present, correctly typed, and within reasonable ranges, and that masks_dict keys match the palette and stats conventions used elsewhere in the project."
          },
          {
            "id": 8,
            "title": "Document and wire aeolian_ridge_continuity anchors in GEOLOGIC_RULES and notebook",
            "description": "Add the aeolian_ridge_continuity geologic rule and code anchor to docs/GEOLOGIC_RULES.md and notebooks/aeolian.ipynb with proper anchor IDs and references.",
            "dependencies": [
              4,
              6,
              7
            ],
            "details": "Update docs/GEOLOGIC_RULES.md to include a new row describing the aeolian_ridge_continuity principle, referencing the code anchor analog_image_generator.geologic_generators.aeolian_linear_ridges (or the specific continuity helper) and documenting default parameter ranges based on the linear-dune research PDF. In notebooks/aeolian.ipynb, add or update a markdown cell with an anchor ID following the anchor-aeolian-aeolian_ridge_continuity convention that explains how the continuity metric is computed and interpreted. Ensure the rule and notebook anchors stay in sync and follow the existing geologic rules discipline.",
            "status": "pending",
            "testStrategy": "Use a documentation-focused check (manual or automated) that confirms the new aeolian_ridge_continuity entry appears in docs/GEOLOGIC_RULES.md with the correct code anchor, and that notebooks/aeolian.ipynb contains the matching markdown anchor and description, updating any existing tests or CI checks that scan for rule and anchor consistency if present."
          },
          {
            "id": 9,
            "title": "Create linear dune generator tests for continuity, orientation, and defect effects",
            "description": "Add a dedicated test module (or extend the barchan tests) to validate PSD-based orientation alignment, ridge continuity ranges, and degradation with increasing defect_rate for the linear generator.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Create tests/test_linear_generator.py or extend tests/test_barchan_generator.py with a suite of pytest cases that instantiate AeolianParams for linear dunes, run the aeolian_linear_ridges generator, and compute diagnostic metrics from the outputs. Implement tests that verify the dominant PSD-based orientation of the crest field matches theta_deg within about 10 degrees, that aeolian_ridge_continuity is ≥0.7 for default q and low defect_rate, and that continuity systematically drops into the 0.5–0.6 range when defect_rate is around 0.25. Include checks across multiple random seeds to ensure robustness and adjust tolerances to avoid flakiness while still catching regressions.",
            "status": "pending",
            "testStrategy": "Run the new test module under pytest with several seeded AeolianParams configurations, asserting PSD orientation alignment with θ, continuity values in the specified bands for default and high defect_rate cases, and stable behavior across seeds; integrate these tests into the existing aeolian test suite so they run as part of the normal CI workflow."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement Transverse dune generator with λ=f(H) spacing",
        "description": "Produce transverse dune fields whose crest spacing scales linearly with dune height and remain orthogonal to wind direction.",
        "details": "Create `aeolian_transverse_ridges` that builds crest-parallel ridges via repeated morphological bands aligned perpendicular to θ. Derive spacing λ = k*H (fit from PRD citation) with guard rails for the slider range. Include defect rate adaptation for crest bifurcations and ensure mask labeling for crest/stoss/slipface/interdune. Track crest orientation error vs. θ and spacing residual for stats.\nPseudo-code:\n```\ndef build_transverse_field(params):\n    spacing = params.lambda_px or alpha*params.H\n    crest_mask = draw_parallel_ridges(spacing, orientation=theta+90)\n    slipface = downwind_projection(crest_mask)\n    return crest_mask, slipface, build_gray(...)\n```\nUpdate GEOLOGIC_RULES anchors referencing `aeolian_make_crests` usage in transverse context.",
        "testStrategy": "Add pytest cases verifying (a) |θ_PSD − θ_wind| ≤ 15° median across seeds, (b) measured spacing falls within ±10% of λ input, and (c) ridge continuity sits between 0.4–0.6 as specified.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define build_transverse_field / aeolian_transverse_ridges API and stubs",
            "description": "Design and add the public-facing transverse dune generator entry points wired to AeolianParams.",
            "dependencies": [],
            "details": "Specify the function signatures for `build_transverse_field(params: AeolianParams)` and any thin wrapper like `aeolian_transverse_ridges(params)` in `src/analog_image_generator/geologic_generators.py`, matching existing aeolian generator patterns. Ensure they accept the full AeolianParams object, return `(gray, masks_dict, metrics_dict)` or whatever convention Task 1/2 established, and register the new style with any aeolian dispatch utilities. Add TODO comments or basic docstrings indicating that spacing, masks, metrics, and defect logic will be implemented in later subtasks.",
            "status": "pending",
            "testStrategy": "Create a minimal smoke test that instantiates AeolianParams with transverse configuration and confirms the new functions import and return correctly shaped arrays/dicts (even if placeholders) without raising exceptions."
          },
          {
            "id": 2,
            "title": "Implement λ = k * H spacing logic with slider guards and overrides",
            "description": "Encode the transverse crest spacing rule λ = k * H with guard rails and lambda_px overrides.",
            "dependencies": [
              1
            ],
            "details": "Implement a helper (e.g., `compute_transverse_spacing(params)`) inside `geologic_generators.py` that computes crest spacing in pixels using `lambda_px = params.lambda_px if params.lambda_px is not None else k * params.H`. Fit or hard-code the initial k value based on PRD and `research-documents/depositional_system_aeolian_transverse-dune.pdf`, and enforce PRD-constrained min/max bounds on both H and λ. Ensure the helper clamps spacing to safe pixel ranges for the current grid size, logs or records any clamping, and returns both target λ and the raw k-derived value for metrics. Wire this into `build_transverse_field` so spacing is always computed via this function.",
            "status": "pending",
            "testStrategy": "Unit test the helper with a range of H and lambda_px inputs to confirm it respects PRD bounds, correctly prefers explicit lambda_px when set, and clamps spacing within safe numeric limits for representative grid sizes."
          },
          {
            "id": 3,
            "title": "Generate crest-parallel ridges orthogonal to wind direction",
            "description": "Create crest masks as repeated ridges aligned perpendicular to params.theta_deg using bands or distance patterns.",
            "dependencies": [
              1,
              2
            ],
            "details": "Within `build_transverse_field`, implement logic (or reuse `aeolian_make_crests` patterns) to synthesize a binary crest mask composed of approximately parallel ridges. Use the wind direction `params.theta_deg` and generate crests orthogonal to wind by orienting bands at `theta_deg + 90°`, accounting for angle normalization. Implement ridge spacing using the output of `compute_transverse_spacing`, via repeated morphological bands or distance transform stripes, ensuring coverage across the domain and avoiding aliasing at edges. Return a stable `crest_mask` array that is compatible with downstream slipface/stoss/interdune derivations.",
            "status": "pending",
            "testStrategy": "Write targeted tests that generate crest masks for several theta_deg values (e.g., 0°, 45°, 90°) and use orientation analysis (e.g., FFT/PSD or structure tensor approximations) to verify the dominant crest orientation is within ~10–15° of the expected orthogonal direction and that average ridge spacing roughly matches the requested λ."
          },
          {
            "id": 4,
            "title": "Derive slipface, stoss, and interdune masks from crest geometry and wind",
            "description": "Compute slipface, stoss, and interdune masks from crest positions and wind direction, integrating them into masks_dict.",
            "dependencies": [
              3
            ],
            "details": "Using the `crest_mask` and `params.theta_deg`, implement morphological operations that partition space into crest, stoss, slipface, and interdune regions. Follow conventions from barchan/linear helpers where possible (e.g., projecting downwind from crests for slipfaces, upwind for stoss, and classifying remaining areas as interdune with optional thickness limits). Ensure output masks share consistent shapes and data types, respect domain boundaries, and can be combined into a coherent `masks_dict` whose keys and semantics align with Aeolian entries in `docs/GEOLOGIC_RULES.md` and `docs/PALETTES.md`. Return both the masks and an updated grayscale field that visually encodes these states.",
            "status": "pending",
            "testStrategy": "Add tests that verify mutual exclusivity and completeness of crest/stoss/slipface/interdune masks (no overlaps, union covers the grid), and that downwind vs upwind masks respond correctly when theta_deg is rotated (e.g., flipping wind by 180° swaps expected stoss/slipface directions)."
          },
          {
            "id": 5,
            "title": "Incorporate defect_rate to generate crest bifurcations and continuity metrics",
            "description": "Use defect_rate to introduce crest bifurcations, gaps, and realistic continuity while keeping transverse character intact.",
            "dependencies": [
              3,
              4
            ],
            "details": "Extend the crest generation stage to use `params.defect_rate` to stochastically break, bifurcate, or locally offset ridge segments, producing realistic defect structures without destroying the overall transverse pattern. Implement algorithms such as randomly deleting segments along ridges, locally splitting crests into branches, or jittering ridge trajectories, with probabilities governed by defect_rate. Compute a crest continuity metric (e.g., size of largest connected crest component divided by total crest pixels) for transverse fields, target a nominal 0.4–0.6 range, and expose this metric alongside masks for later stats and tests.",
            "status": "pending",
            "testStrategy": "Create tests that run the generator over multiple seeds and a few defect_rate values (e.g., 0.0, 0.2, 0.5), then measure crest continuity to ensure it stays within defined bounds and monotonically decreases as defect_rate increases, while still preserving recognizable transverse band structure."
          },
          {
            "id": 6,
            "title": "Compute crest orientation error and spacing residual metrics",
            "description": "Measure crest orientation error vs wind direction and spacing residuals for later statistical pipelines.",
            "dependencies": [
              2,
              3,
              5
            ],
            "details": "Add metric computation within `build_transverse_field` (or a dedicated metrics helper) that calculates (a) crest orientation error as the absolute difference between the dominant crest orientation inferred from the crest mask PSD and the ideal `theta_deg + 90°`, and (b) spacing residuals as fractional deviation between measured crest spacing and the target λ from `compute_transverse_spacing`. Aggregate these metrics across the domain (e.g., median orientation error across seeds, mean/median spacing residual) and store them in the metrics_dict returned from the generator so higher-level stats pipelines can consume them.",
            "status": "pending",
            "testStrategy": "Implement tests that generate transverse fields for reasonable parameter sets and verify that median orientation error is typically ≤15° and spacing residuals fall within ±10% for default configurations, using a small ensemble of seeds to avoid flukes."
          },
          {
            "id": 7,
            "title": "Align masks, palettes, and GEOLOGIC_RULES / notebook anchors for transverse dunes",
            "description": "Update documentation anchors and palette mappings so transverse outputs match Aeolian mask conventions and geologic principles.",
            "dependencies": [
              1,
              4,
              6
            ],
            "details": "Review `docs/GEOLOGIC_RULES.md` and `docs/PALETTES.md` to identify existing Aeolian mask keys and color assignments, then ensure the transverse generator uses exactly the same key names and semantics for crest, stoss, slipface, and interdune. Add or update GEOLOGIC_RULES entries to reference `analog_image_generator.geologic_generators.build_transverse_field` (or equivalent) with clear description of the λ = k * H rule and defect_rate behavior. Update the appropriate notebook markdown cell in `notebooks/aeolian.ipynb` with a new anchor ID like `anchor-aeolian-transverse` referencing the code-level anchor, keeping anchor formats consistent with project conventions.",
            "status": "pending",
            "testStrategy": "Use lightweight documentation checks (manual or scripted) that confirm the transverse generator function name appears both in the code anchor column and the notebook anchor column, and that mask keys in docs match those produced by the generator in a small smoke run."
          },
          {
            "id": 8,
            "title": "Add tests/test_transverse_generator.py for PSD, spacing, and continuity validation",
            "description": "Create a dedicated pytest module that validates orientation error, spacing residuals, and continuity ranges across seeded runs.",
            "dependencies": [
              2,
              3,
              5,
              6,
              7
            ],
            "details": "Introduce `tests/test_transverse_generator.py` that constructs AeolianParams for transverse dunes with representative values of H, lambda_px (set and unset), theta_deg, and defect_rate, then calls `build_transverse_field`. Implement tests that (a) estimate PSD-based crest orientation and assert median |θ_PSD − (theta_deg + 90°)| ≤ 15°, (b) measure crest spacing from the crest mask and confirm average spacing lies within ±10% of the target λ, and (c) compute crest continuity metrics to ensure they fall within the desired 0.4–0.6 band for default parameters. Use deterministic seeds and small grids for performance, borrowing helper utilities from existing aeolian tests where available.",
            "status": "pending",
            "testStrategy": "Run the new test module under pytest and, where possible, reuse shared fixtures and analysis helpers from barchan and linear generator tests to ensure consistent PSD, spacing, and continuity measurement methods across dune styles."
          }
        ]
      },
      {
        "id": 5,
        "title": "Add aeolian sedimentary feature overlays and property trends",
        "description": "Implement overlays for cross-bedding, reactivation surfaces, inverse grading, grain rounding, surface textures, erosional events, impacts, and cementation metadata.",
        "details": "Create helpers listed in `docs/GEOLOGIC_RULES.md` rows 57–65 (`aeolian_cross_bedding_masks`, `aeolian_inverse_grading_profile`, `aeolian_grain_rounding`, `aeolian_surface_impacts`, `aeolian_cementation`). Integrate them with mask outputs from Tasks 2–4, ensuring overlays add channels for dip azimuth histograms, gradation direction, roundedness index, surface roughness, event counts, and cement type tags for reporting. Link to palette entries to tint cemented regions red-orange.\nPseudo-code:\n```\ndef apply_sedimentary_overlays(masks, params):\n    cross_sets = synthesize_cross_beds(masks[\"slipface\"], dip=params.theta_deg)\n    rounding = compute_rounding(params.q, transport_distance(masks))\n    impacts = sprinkle_impacts(masks[\"crest\"], intensity=params.defect_rate)\n    cement = aeolian_cementation(masks, params.cement_type)\n    return {**masks, **cross_sets, \"rounding\": rounding, ...}\n```\nUpdate notebook anchors and PRD references to reflect implemented controls.",
        "testStrategy": "Expand pytest suite to verify (a) at least one cross-bed set >20° dip exists per realization, (b) roundedness index stays within 0.6–0.9 for default params, (c) gradation metadata flips sign when invert toggle changes, and (d) event counts increase when simulating wind shifts.",
        "priority": "medium",
        "dependencies": [
          2,
          3,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design aeolian sedimentary overlay schema and key conventions",
            "description": "Define the masks, numeric arrays, and metadata dict structures used to represent aeolian sedimentary overlays that attach cleanly to existing dune masks.",
            "dependencies": [],
            "details": "Survey existing aeolian mask outputs from Tasks 2–4 in src/analog_image_generator/geologic_generators.py and design a consistent overlay schema (e.g., extra boolean masks, float fields, and small metadata dicts) that can be merged into the main masks_dict. Specify key names for cross-beds, reactivation surfaces, inverse grading, grain rounding, surface roughness, impact/event overlays, and cementation tags, ensuring shapes align with core height/width grids and that metadata fields can support dip azimuth histograms, grading direction flags, rounding indices, roughness metrics, event counts, and cement type labels.",
            "status": "pending",
            "testStrategy": "Draft a small design doc or code comment block, then prototype a tiny dummy masks_dict and ensure the merged overlays conform to the agreed key and shape conventions without collisions."
          },
          {
            "id": 2,
            "title": "Implement aeolian_cross_bedding_masks helper with reactivation surfaces",
            "description": "Create aeolian_cross_bedding_masks in geologic_generators.py to synthesize cross-bed sets and optional reactivation surfaces tied to slipface and crest masks.",
            "dependencies": [
              1
            ],
            "details": "In src/analog_image_generator/geologic_generators.py, implement aeolian_cross_bedding_masks(masks, params) that uses slipface and crest masks from Tasks 2–4 to generate cross-bedding and reactivation-surface overlays. Use AeolianParams (e.g., theta_deg and any cross-bedding parameters) to control dip magnitude and azimuth, producing one or more boolean or integer masks encoding cross-bed set membership and reactivation planes. Compute and store dip azimuth histograms and representative dip angles in a metadata dict consistent with the overlay schema from subtask 1.",
            "status": "pending",
            "testStrategy": "Create small synthetic slipface and crest masks and verify that the helper returns masks of matching shape, at least one cross-bed set with dip >20° for default params, and a non-empty dip azimuth histogram in the metadata."
          },
          {
            "id": 3,
            "title": "Implement aeolian_inverse_grading_profile helper and grading metadata",
            "description": "Add aeolian_inverse_grading_profile to compute vertical or downslope grading trends and track grading direction, including invert_gradation toggle handling.",
            "dependencies": [
              1
            ],
            "details": "Implement aeolian_inverse_grading_profile(masks, params) in src/analog_image_generator/geologic_generators.py to produce one or more float arrays describing grain size or intensity as a function of position (e.g., along slipface normals or vertical coordinate) for aeolian dunes. Introduce metadata fields such as grading_direction (e.g., +1 for normal, -1 for inverse) and any summary statistics needed for reporting. Ensure that an invert_gradation or similar toggle in AeolianParams flips both the underlying gradient sign and the corresponding metadata flag, aligned with the overlay schema defined earlier.",
            "status": "pending",
            "testStrategy": "Using controlled synthetic masks and parameter settings, assert that flipping the invert_gradation flag reverses the sign of the computed grading trend while leaving magnitudes stable and that grading_direction metadata changes accordingly."
          },
          {
            "id": 4,
            "title": "Implement aeolian_grain_rounding helper and surface texture indices",
            "description": "Create aeolian_grain_rounding to model grain rounding and surface texture trends from transport distance and q, returning stable roundedness indices.",
            "dependencies": [
              1
            ],
            "details": "In src/analog_image_generator/geologic_generators.py, implement aeolian_grain_rounding(masks, params) that estimates transport distance (e.g., via hop counts, distance transforms, or along-wind integration) and uses q and other AeolianParams to produce a roundedness index field over the dune or selected facies. Add complementary surface texture or roughness metrics if required by GEOLOGIC_RULES rows 57–65. Ensure outputs are numerically stable and bounded (e.g., roundedness index in a 0–1 or 0.6–0.9 band for default params) and store summary statistics in metadata for later stats and reporting.",
            "status": "pending",
            "testStrategy": "Run the helper on small test masks with varying q and transport distances, checking that roundedness indices increase monotonically with distance and that default parameter settings yield indices within the target band documented in the parent task’s test strategy."
          },
          {
            "id": 5,
            "title": "Implement aeolian_surface_impacts helper for impact and erosional events",
            "description": "Implement aeolian_surface_impacts to overlay impact and erosional event markers that modify or annotate existing masks and track event counts.",
            "dependencies": [
              1
            ],
            "details": "Add aeolian_surface_impacts(masks, params) in src/analog_image_generator/geologic_generators.py that sprinkles impacts or erosional events preferentially along crest, stoss, or slipface regions using masks from Tasks 2–4 and parameters such as defect_rate or wind shift frequency. Represent impacts as additional boolean or integer masks or as sparse event maps, and compute per-region event counts and optional roughness deltas. Store event_count fields and any roughness metrics in the overlay metadata, ensuring the structure matches the schema defined in subtask 1.",
            "status": "pending",
            "testStrategy": "Generate simple crest and slipface masks and vary an impact intensity parameter, asserting that event counts in the metadata increase with higher intensity and that impacts fall predominantly on the targeted geomorphic elements."
          },
          {
            "id": 6,
            "title": "Implement aeolian_cementation helper and palette-based tinting",
            "description": "Create aeolian_cementation to tag cemented regions and link them to docs/PALETTES.md entries for red–orange tinting and cement type metadata.",
            "dependencies": [
              1
            ],
            "details": "Implement aeolian_cementation(masks, params) in src/analog_image_generator/geologic_generators.py that takes cement_type and other AeolianParams to derive a cementation mask (or graded field) describing which portions of the dune are cemented. Consult docs/PALETTES.md to map cement_type values to palette keys and define a red–orange tint channel or RGB indices for cemented areas. Attach a cementation metadata dict including cement_type, coverage fraction, and any palette keys used so that downstream rendering and reporting can consistently tint cemented regions.",
            "status": "pending",
            "testStrategy": "Using a small masks_dict and several cement_type parameter values, verify that the helper produces cementation masks with plausible coverage, emits the expected palette keys for cemented regions, and changes tint metadata when cement_type is varied."
          },
          {
            "id": 7,
            "title": "Compose apply_sedimentary_overlays entry point to call all helpers",
            "description": "Define apply_sedimentary_overlays in geologic_generators.py that consumes dune masks and AeolianParams, calls all aeolian overlay helpers, and returns an enriched masks_dict.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Implement apply_sedimentary_overlays(masks, params) in src/analog_image_generator/geologic_generators.py following the provided pseudo-code, invoking aeolian_cross_bedding_masks, aeolian_inverse_grading_profile, aeolian_grain_rounding, aeolian_surface_impacts, and aeolian_cementation. Ensure that cross-bedding and reactivation surfaces are correctly anchored to slipface and crest masks, that dip and azimuth controls come from AeolianParams, and that all overlay outputs are merged into the masks_dict using the schema defined in subtask 1. Return both updated masks and any aggregated metadata structures needed for later stats and reporting without breaking the existing generator API.",
            "status": "pending",
            "testStrategy": "Write a small integration script or notebook cell that builds a minimal dune masks_dict (mocking Tasks 2–4 outputs), calls apply_sedimentary_overlays with default AeolianParams, and checks that all expected overlay keys and metadata fields are present and correctly shaped."
          },
          {
            "id": 8,
            "title": "Wire overlay metadata fields into masks_dict contract for downstream use",
            "description": "Ensure that dip histograms, grading direction, rounding indices, roughness, event counts, and cement tags are consistently exposed in the masks and metadata contract.",
            "dependencies": [
              7
            ],
            "details": "Audit the outputs of apply_sedimentary_overlays to confirm that every required property—dip azimuth histograms, representative dip angles, grading_direction flags, roundedness indices, surface roughness metrics, impact and erosional event counts, and cement type tags—is stored under clear, documented keys either in masks_dict or in a paired metadata dict. Update or add lightweight data classes or typed dicts if appropriate so Task 6 (pipeline), Task 8 (metrics), and Task 9 (reporting) can consume these fields without ad hoc parsing.",
            "status": "pending",
            "testStrategy": "Add a simple assertion-based check (e.g., in a debug helper or temporary test) that iterates through overlay outputs from apply_sedimentary_overlays and validates the presence, type, and shape of all mandated metadata fields, adjusting names or nesting until the contract is stable."
          },
          {
            "id": 9,
            "title": "Update GEOLOGIC_RULES and aeolian notebook anchors for new overlays",
            "description": "Bring docs/GEOLOGIC_RULES.md rows 57–65 and aeolian notebook anchors in sync with the implemented overlay helpers and entry point.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "details": "Edit docs/GEOLOGIC_RULES.md rows 57–65 so that each aeolian helper (aeolian_cross_bedding_masks, aeolian_inverse_grading_profile, aeolian_grain_rounding, aeolian_surface_impacts, aeolian_cementation, and apply_sedimentary_overlays) has accurate code anchor entries using fully qualified names like analog_image_generator.geologic_generators.aeolian_cross_bedding_masks. Update the corresponding notebook markdown anchors in the appropriate aeolian notebook under notebooks/ (e.g., notebooks/aeolian.ipynb) following the anchor-<env>-<principle> convention and ensure PRD references in .taskmaster/docs/prd.txt or related PRDs reflect the new controls and parameters.",
            "status": "pending",
            "testStrategy": "Open the aeolian notebook and GEOLOGIC_RULES.md side by side to verify that every implemented helper appears in both the code anchor and notebook anchor columns and that links or IDs resolve correctly when rendered."
          },
          {
            "id": 10,
            "title": "Add tests/test_aeolian_overlays.py to validate overlay behaviors",
            "description": "Create pytest coverage for aeolian sedimentary overlays, including cross-bed dips, grading flips, rounding bands, impact sensitivity, and cementation tags.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "details": "Add a new test module tests/test_aeolian_overlays.py that constructs deterministic AeolianParams and minimal barchan/transverse masks (or uses existing generators) to exercise the overlay helpers and apply_sedimentary_overlays. Include tests that (a) ensure at least one cross-bed set has dip >20° for standard params, (b) verify grading metadata and profiles flip sign when the invert_gradation toggle is changed, (c) check that grain rounding indices fall within the expected band for default q and increase with transport distance, (d) confirm impact and erosional event counts grow with higher defect_rate or similar controls, and (e) assert that cementation masks and metadata carry the correct cement type tags and palette-based tint information.",
            "status": "pending",
            "testStrategy": "Implement parametrized pytest cases in tests/test_aeolian_overlays.py using fixed random seeds and small grids, and run the suite locally to confirm stable, reproducible checks across cross-bedding, grading, rounding, impacts, and cementation behaviors."
          }
        ]
      },
      {
        "id": 6,
        "title": "Compose aeolian generator pipeline and masks export",
        "description": "Wire `generate_aeolian` to orchestrate morphology-specific builders, sedimentary overlays, palette application, and mask packaging for downstream stats/reporting.",
        "details": "Implement dispatcher that selects Barchan/Linear/Transverse builders, applies Task 5 overlays, blends grayscale shading, and returns `(gray, masks_dict)` conforming to existing API. Ensure sequential layering order (base → ridges → slipfaces → interdunes → final gray → facies RGB) is emitted for preview and batch export. Write palette legend metadata aligned with `docs/PALETTES.md` and ensure mask keys align with reporting schema. Update docstrings with citations and keep GEOLOGIC_RULES compose row in sync.\nPseudo-code:\n```\ndef generate_aeolian(params_dict):\n    params = AeolianParams.from_dict(params_dict)\n    base = make_base_field(params)\n    builder = BUILDERS[params.env]\n    gray, masks = builder(base, params)\n    masks = apply_sedimentary_overlays(masks, params)\n    legend = build_palette_metadata(masks)\n    return gray, {**masks, \"legend\": legend}\n```\nAdd CLI hook or utility to drive new generator for smoke tests.",
        "testStrategy": "Create `tests/test_generate_aeolian.py` to run each env with fixed seed, asserting mask keys, grayscale ranges, sequential snapshot counts, and validating that overlays exist. Include regression fixtures (npz) for CI tolerance within ±1e-6.",
        "priority": "medium",
        "dependencies": [
          2,
          3,
          4,
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design generate_aeolian API and parameter ingestion",
            "description": "Specify the public signature and behavior of generate_aeolian, including how it constructs AeolianParams from a generic params_dict and how callers are expected to pass env-specific settings.",
            "dependencies": [],
            "details": "Define the generate_aeolian(params_dict: dict) function in `src/analog_image_generator/geologic_generators.py` as the main aeolian entrypoint used by interactive UI, stats, and reporting. Sketch how AeolianParams.from_dict will validate env, clamp ranges according to the existing AeolianParams schema from Task 1, and normalize any missing defaults. Document accepted keys, env values (\"barchan\", \"linear\", \"transverse\"), and error semantics (e.g., raising ValueError on invalid env). Ensure the function docstring clearly states that it returns `(gray, masks_dict)` and that masks_dict is JSON-serializable for downstream pipelines.",
            "status": "pending",
            "testStrategy": "Add or update a small doc-based test (or type-check style test) that imports generate_aeolian, passes a minimal but valid params_dict for one env, and asserts that AeolianParams.from_dict is called and that a ValueError is raised for an invalid env."
          },
          {
            "id": 2,
            "title": "Implement Aeolian builder registry and env dispatch",
            "description": "Create a registry mapping env strings to morphology-specific builder functions and wire generate_aeolian to dispatch through this registry.",
            "dependencies": [
              1
            ],
            "details": "Introduce a BUILDERS or AEOLIAN_BUILDERS dict in `geologic_generators.py` (or a nearby aeolian module) that maps canonical env names (\"barchan\", \"linear\", \"transverse\") to the generator functions implemented in Tasks 2–4, e.g., `build_barchan_field`, `build_linear_dune_field`, `build_transverse_dune_field`. Ensure the mapping is centralized so later code can introspect supported envs. In generate_aeolian, after constructing AeolianParams, look up the builder via this registry, handle unknown envs with a clear error, and call `gray, masks = builder(base, params)` using the base-field prepared from Task 1.",
            "status": "pending",
            "testStrategy": "Write unit tests that exercise the registry directly: assert that all expected env keys exist, that each maps to a callable, and that generate_aeolian raises a meaningful exception when params_dict[\"env\"] is not in the registry."
          },
          {
            "id": 3,
            "title": "Integrate base-field creation, builders, and overlays into pipeline",
            "description": "Wire together base-field generation, env-specific builders, and sedimentary overlays so generate_aeolian produces final gray and masks outputs conforming to the existing API.",
            "dependencies": [
              1,
              2
            ],
            "details": "Call the base-field scaffolding from Task 1 (e.g., `make_base_field(params)` or equivalent) at the top of generate_aeolian, then feed its output plus AeolianParams into the selected builder from the registry. After the builder returns initial gray and masks, invoke the Task 5 overlay helpers via an `apply_sedimentary_overlays(masks, params)` or similar function that enriches masks with cross-bedding, grading, rounding, impacts, and cementation metadata. Ensure the final return value is `(gray_final, masks_dict)` where `gray_final` reflects any shading adjustments due to overlays and masks_dict contains all base morphology and overlay channels. Also design and implement a thin CLI hook or utility (e.g., in `scripts/` or an existing CLI module) that calls generate_aeolian with a small params set for smoke tests.",
            "status": "pending",
            "testStrategy": "Add tests that call generate_aeolian for each env with a fixed seed and minimal params, asserting that (a) gray has the expected 2D shape and dtype, (b) masks is a dict containing at least the base morphology keys plus additional overlay keys, and (c) the CLI smoke hook runs without error and produces outputs in a temporary directory."
          },
          {
            "id": 4,
            "title": "Define sequential stage ordering and storage schema",
            "description": "Specify and implement the ordered sequence of aeolian stages and how they are stored so interactive previews and batch exporters can reuse them.",
            "dependencies": [
              3
            ],
            "details": "Within generate_aeolian (or a helper), formalize the stage list `['base', 'ridges', 'slipfaces', 'interdunes', 'gray_final', 'facies_rgb']` and ensure each stage is captured either as dedicated masks entries or as a structured sub-dict (e.g., `masks['stages']`). Decide on consistent keys and data types for each stage, making sure they are serializable to NPZ/JSON for batch workflows. Implement logic to populate this structure in the correct order as the pipeline runs: snapshot base field, then ridge masks, slipface masks, interdune maps, final gray, and facies RGB. Expose this ordering via a small helper (e.g., `get_aeolian_stage_order()`) so `interactive.py` and exporters can use the same canonical sequence.",
            "status": "pending",
            "testStrategy": "Extend tests to assert that, for each env, the masks dict includes all expected stage keys in the correct order and that each stage array has consistent shape with the base gray image. Optionally, serialize a small run to disk and re-load it to confirm stage ordering survives round-trip storage."
          },
          {
            "id": 5,
            "title": "Align masks, facies keys, and palette metadata with documentation",
            "description": "Review mask naming, facies labels, and palette associations in generate_aeolian outputs and ensure they match docs/PALETTES.md and docs/TASKMASTER_TASKS_EXPORT.md.",
            "dependencies": [
              3,
              4
            ],
            "details": "Cross-check the keys in the masks dict (e.g., crest, slipface, interdune, overlay channels, facies indices) against the schemas and naming conventions documented in `docs/PALETTES.md` and `docs/TASKMASTER_TASKS_EXPORT.md`. Update key names, facies IDs, and any palette lookups so that the aeolian outputs integrate cleanly with existing stats and reporting code. Ensure that any new overlay channels introduced by Task 5 use consistent prefixes or suffixes and are documented in a central place (e.g., a small AEOLIAN_MASK_SCHEMA mapping). Confirm that facies RGB representations tie back to palette entries and that no undocumented keys are emitted.",
            "status": "pending",
            "testStrategy": "Add assertion-heavy tests that inspect the returned masks_dict keys and compare them to an expected set derived from the docs, verifying both presence and absence where appropriate. Also test that palette-related metadata fields exist and have values that line up with the documented facies IDs or color names."
          },
          {
            "id": 6,
            "title": "Implement legend and palette metadata helpers for reporting",
            "description": "Create legend metadata structures or helper functions that transform aeolian masks and palette info into a form that downstream reporting can render into PDFs and other artifacts.",
            "dependencies": [
              5
            ],
            "details": "Design a `build_palette_metadata(masks, params)` helper (or similar) that collects the facies and overlay channels used by generate_aeolian, looks up their human-readable names and colors from `docs/PALETTES.md`, and returns a structured legend object, e.g., a list of entries with keys like `id`, `label`, `color`, and `category`. Integrate this helper into generate_aeolian so that the returned masks_dict includes a `legend` entry or nested structure suitable for CSV/PDF exporters. Ensure that the legend format is compatible with existing reporting functions (e.g., `reporting.build_reports`) and that it captures any aeolian-specific nuances such as cementation tints or cross-bedding indicators.",
            "status": "pending",
            "testStrategy": "Write unit tests for the legend helper that feed in a synthetic masks dict covering several aeolian facies and overlays, then assert that the resulting legend contains one entry per visible facies/overlay, with colors and labels matching the palette docs. Also test that generate_aeolian includes this legend entry in masks_dict for each env."
          },
          {
            "id": 7,
            "title": "Update GEOLOGIC_RULES compose_aeolian row and documentation anchors",
            "description": "Bring GEOLOGIC_RULES, docstrings, and notebook anchors up to date with the new generate_aeolian orchestration for full traceability.",
            "dependencies": [
              1,
              3,
              5,
              6
            ],
            "details": "Edit `docs/GEOLOGIC_RULES.md` to update or add the compose_aeolian row so that it references the fully qualified `analog_image_generator.geologic_generators.generate_aeolian` function and describes its role in orchestrating aeolian morphology, overlays, and palettes. Ensure the corresponding notebook markdown anchors (e.g., `notebooks/aeolian.ipynb#anchor-aeolian-compose` or similar) reference the same function and principles, following the `anchor-<env>-<principle>` convention. Refresh relevant docstrings in `geologic_generators.py` and any CLI utilities to include citations, parameter descriptions, and notes about stage ordering and legend metadata, keeping everything consistent with the Notebook Anchors Discipline.",
            "status": "pending",
            "testStrategy": "Perform a documentation-focused check by running any existing doc or anchor validation scripts if available, or otherwise manually verifying that each changed function appears in both the code-anchors and notebook-anchors columns in GEOLOGIC_RULES. Optionally add a lightweight test that parses GEOLOGIC_RULES and asserts that generate_aeolian is listed with the expected anchor ID."
          },
          {
            "id": 8,
            "title": "Add tests and regression harness for generate_aeolian across envs",
            "description": "Create tests/test_generate_aeolian.py with seed-stable tests for each env, covering output schema, value ranges, overlay presence, and regression stability, plus a simple smoke path used by CLI.",
            "dependencies": [
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Implement a new `tests/test_generate_aeolian.py` module that parametrizes over env values (\"barchan\", \"linear\", \"transverse\") and uses fixed seeds in AeolianParams. For each env, call generate_aeolian and assert that gray is within expected numeric ranges (e.g., 0–1 or 0–255, matching the rest of the project), masks_dict contains required base and overlay keys, and that overlay-specific invariants hold (e.g., at least some nonzero cross-bedding or cementation channels when enabled). Add regression fixtures (e.g., small NPZ files) capturing reference gray and key masks for one or more envs and compare current outputs with tolerances such as ±1e-6. Include a test that exercises the CLI smoke utility, verifying that it runs without error and that its outputs conform to the expected directory and file schema.",
            "status": "pending",
            "testStrategy": "Run the new pytest module and ensure all parametrized env tests pass. Verify that regression tests read and compare NPZ fixtures correctly and that the CLI smoke test is fast enough for CI while still providing coverage of the end-to-end pipeline."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement aeolian sliders, previews, and batch exporter",
        "description": "Extend `interactive.py` to expose aeolian slider metadata, render sequential previews, and run batch parameter sweeps for N realizations.",
        "details": "Populate `build_sliders(\"aeolian\")` returning slider definitions using PRD ranges/defaults and cite sources in docstrings. For `preview_sequence`, call `generate_aeolian` for each intermediate stage (base/ridges/slipfaces/interdunes/gray/facies), capturing matplotlib frames or ipywidgets outputs. Add a batch-export helper to iterate seeds/param combos, storing metrics snapshots in `outputs/aeolian/<timestamp>/`. Ensure UI reacts immediately when sliders change (ipywidgets observe) and integrate into notebooks.\nPseudo-code:\n```\ndef build_sliders(env):\n    if env == \"aeolian\":\n        return {\n            \"theta_deg\": {\"min\":0,\"max\":180,\"step\":5,\"default\":60},\n            ...\n        }\n\ndef preview_sequence(env, params, seeds):\n    for seed in seeds:\n        frames = compute_stages(generate_aeolian({...}))\n        show(frames)\n```\nDocument sequential preview order per PRD and ensure `docs/WORKFLOW.md` references new batch exporter toggle.",
        "testStrategy": "Write widget/unit tests using `ipywidgets` test utilities verifying slider defaults match PRD table, preview call order matches stage list, and batch exporter writes expected number of realizations with distinct seeds. Smoke-test via `scripts/smoke_test.py` extension for aeolian env.",
        "priority": "medium",
        "dependencies": [
          1,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Review PRD/AGENTS and define Aeolian slider schema",
            "description": "Review PRD, AGENTS.md, and AeolianParams to list all aeolian-controllable parameters and their ranges/defaults, then design the slider metadata structure for env == \"aeolian\".",
            "dependencies": [],
            "details": "Open PRD and AGENTS.md plus the AeolianParams definition to enumerate all aeolian parameters that should be exposed as sliders (e.g., theta_deg, dune_height_px, defect_rate, package_count, erosional_relief_px). For each, record min/max/step/default values from the requirements and confirm units. Compare the existing slider metadata pattern for other environments (e.g., fluvial) in interactive.py or related modules, and define a consistent schema for aeolian slider entries, including keys like min, max, step, default, description, and possibly group/category metadata. Capture any citations or rationale needed for docstrings so later implementation can reference the correct sources.",
            "status": "pending",
            "testStrategy": "Manually verify that the collected parameter list matches AeolianParams and PRD tables, and that the proposed schema fields line up with existing slider structures for other environments (e.g., no missing min/max/step/default). Optionally create a small scratch test that instantiates the schema and confirms it is JSON-serializable and compatible with existing consumers."
          },
          {
            "id": 2,
            "title": "Implement build_sliders(\"aeolian\") based on AeolianParams and PRD ranges",
            "description": "Extend src/analog_image_generator/interactive.py so build_sliders(\"aeolian\") returns a complete slider metadata dict using PRD/AGENTS ranges and AeolianParams defaults, including source citations in docstrings.",
            "dependencies": [
              1
            ],
            "details": "In interactive.py, implement the aeolian branch of build_sliders(env) so that when env == \"aeolian\" it constructs and returns an ordered dict mapping each AeolianParams field to a slider config. Each config should include at least min, max, step, default, and a helpful label/description, pulling numeric ranges and defaults directly from PRD and AGENTS.md where defined. Ensure that defaults stay in sync with AeolianParams dataclass defaults to avoid divergence. Add or update docstrings to describe aeolian slider semantics and include short citations or anchor IDs to PRD/GEOLOGIC_RULES as required by project conventions. Keep the structure compatible with existing or planned slider consumers so downstream UI code can treat aeolian exactly like other envs.",
            "status": "pending",
            "testStrategy": "Add or extend unit tests (later consolidated in tests/test_interactive_aeolian.py) that call build_sliders(\"aeolian\") and assert: (a) all expected AeolianParams fields are present as keys, (b) each slider entry has min, max, step, and default fields with the correct types, and (c) numeric defaults match AeolianParams and PRD tables within a small tolerance. Optionally use parametrized tests over a list of expected sliders."
          },
          {
            "id": 3,
            "title": "Implement aeolian preview_sequence with staged outputs",
            "description": "Implement preview_sequence for env == \"aeolian\" to call generate_aeolian and capture sequential stages (base, ridges, slipfaces, interdunes, gray, facies) as matplotlib or ipywidgets frames for each seed.",
            "dependencies": [
              1,
              2
            ],
            "details": "In interactive.py, extend preview_sequence(env, params, seeds) so that when env == \"aeolian\" it orchestrates multi-stage previews. For each seed, call generate_aeolian (from the aeolian pipeline composed in Task 6) with the appropriate AeolianParams constructed from current slider values, and obtain either intermediate snapshots or reconstruct them from returned outputs. Ensure that the stages are produced and recorded in the exact order base → ridges → slipfaces → interdunes → gray → facies as specified. Wrap each stage in a consistent frame representation (e.g., matplotlib Figure objects, or ipywidgets Image/Output widgets) and return a structure that the UI and notebooks can iterate over. Handle multiple seeds by either concatenating or clearly grouping frames per seed to support both single-seed preview and small ensembles.",
            "status": "pending",
            "testStrategy": "Write tests that mock or stub generate_aeolian to return recognizable stage markers, then call preview_sequence(\"aeolian\", ...) and assert that: (a) the function invokes generate_aeolian with each provided seed, (b) the returned sequence contains all six stages in the correct order, and (c) the number of frames matches len(seeds) * 6. For widget-heavy paths, use lightweight assertions on object types or metadata rather than full rendering."
          },
          {
            "id": 4,
            "title": "Add aeolian batch-export helper for seeds and parameter sweeps",
            "description": "Create a batch-export helper in interactive.py that iterates over seeds and aeolian parameter combinations, calls generate_aeolian and stats.compute_metrics, and writes outputs under outputs/aeolian/<timestamp>/.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement a function (e.g., run_aeolian_batch or export_aeolian_batch) that accepts a base AeolianParams configuration, a list or grid of parameter overrides, and a list of seeds. For each combination of seed and parameter set, call generate_aeolian to obtain gray images and masks_dict, then call stats.compute_metrics to compute aeolian metrics snapshots. Save outputs into a timestamped directory like outputs/aeolian/<ISO-timestamp>/, organizing per run using informative filenames that encode seed and parameter index. Persist metrics in a CSV (or multiple CSVs) and, if appropriate, small preview images or npz files for regression. Ensure that the helper integrates cleanly with existing project conventions and does not hard-code notebook-specific behavior, but instead can be triggered from both CLI/notebooks and ipywidgets buttons.",
            "status": "pending",
            "testStrategy": "Implement unit/integration tests that patch generate_aeolian and stats.compute_metrics to lightweight fakes, then invoke the batch-export helper with a small grid (e.g., 2 seeds × 3 param combos). Assert that: (a) an outputs/aeolian/<timestamp>/ directory gets created, (b) the expected number of metrics entries and files are written (2 × 3 runs), and (c) filenames or CSV rows correctly encode seed and parameter indices. Clean up temporary output directories in the test teardown to keep the test suite tidy."
          },
          {
            "id": 5,
            "title": "Design reusable hooks for notebooks to consume aeolian previews and batches",
            "description": "Refactor or add helper functions so notebooks can embed aeolian preview sequences and batch runs via simple calls without duplicating interactive logic.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Identify the core pure operations behind aeolian previews and batch exports—such as building AeolianParams from slider or dict-like input, computing preview frames, and running batch sweeps—and extract them into reusable functions within interactive.py or a small companion module. Provide thin, well-documented hooks like get_aeolian_preview_frames(params, seeds) and run_aeolian_batch_jobs(params_grid, seeds) that encapsulate the orchestration but remain UI-agnostic. Ensure that ipywidgets-facing functions call into these hooks rather than reimplementing logic, so Jupyter notebooks can import and reuse the same functionality directly. This design should minimize duplication and make it easy to test preview and batch behavior headlessly.",
            "status": "pending",
            "testStrategy": "Create tests that call the new notebook-oriented hooks directly with simple AeolianParams or dict configurations and mocked generate_aeolian/stats functions, verifying that they return frame lists or batch result summaries in the expected structure. Confirm that the UI entry points (e.g., preview_sequence and the batch-export helper) internally delegate to these hooks, possibly by checking call counts with a spy or by comparing outputs for identical inputs."
          },
          {
            "id": 6,
            "title": "Wire ipywidgets-based aeolian UI with responsive observers",
            "description": "Implement or extend ipywidgets widgets and observers so aeolian sliders trigger efficient recomputation of previews and batch-export toggles, ensuring the UI feels responsive in notebooks.",
            "dependencies": [
              2,
              3,
              5
            ],
            "details": "Using ipywidgets (and any existing fluvial UI patterns), construct slider widgets for each aeolian parameter based on build_sliders(\"aeolian\") metadata. Register observe or traitlets callbacks so that when a slider value changes, the underlying AeolianParams are updated and preview_sequence(\"aeolian\", ...) is recomputed in a throttled or debounced manner to avoid excessive recomputation. Wire a batch export button or toggle that invokes the aeolian batch-export helper using the current slider state and seed configuration, giving the user clear feedback about progress and output locations. Ensure the implementation keeps expensive work off the main thread where possible (or at least avoids redundant reruns) and that UI state (selected env, seeds, modes) stays coherent as sliders change.",
            "status": "pending",
            "testStrategy": "Write high-level widget tests that instantiate the aeolian UI, programmatically change one or more slider values, and assert that the preview callback fires (e.g., by inspecting a mock preview function or an incremented counter). Similarly, simulate clicking the batch export button with a stubbed batch helper and assert that it is invoked exactly once with parameters derived from current slider values. Manual smoke tests in a notebook can complement automated tests to confirm responsiveness and lack of excessive recomputation."
          },
          {
            "id": 7,
            "title": "Document aeolian sliders, preview order, and batch exporter in docs and notebooks",
            "description": "Update docs/WORKFLOW.md and the aeolian notebook to describe aeolian slider semantics, the sequential preview order, and how to use the new batch exporter, aligning with GEOLOGIC_RULES anchors.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Edit docs/WORKFLOW.md to add or extend sections covering the aeolian environment: describe each slider, including parameter meaning, units, valid ranges, and defaults, and reference the corresponding GEOLOGIC_RULES and notebook anchors as required by the anchoring discipline. Explicitly document the sequential preview order (base → ridges → slipfaces → interdunes → gray → facies) and link it to the aeolian generator pipeline in generate_aeolian. Add or update the aeolian-focused Jupyter notebook to show usage examples for building sliders, running previews, and triggering batch exports, ensuring that anchor IDs follow the anchor-<env>-<principle> pattern and reference the fully qualified code symbols used for aeolian interactive functions. Verify that documentation stays in sync with the implemented APIs and that any UI screenshots or descriptions reflect the current behavior.",
            "status": "pending",
            "testStrategy": "Perform documentation checks by rendering docs/WORKFLOW.md (and, if applicable, Sphinx or markdown previews) and opening the aeolian notebook to ensure all references and anchors resolve. Optionally add a lightweight test that scans docs/WORKFLOW.md and the notebook JSON for required anchor IDs and code references to confirm they are present, and manually confirm that the described preview order and slider semantics match the actual implementation in interactive.py."
          },
          {
            "id": 8,
            "title": "Add tests for aeolian interactive sliders, previews, and batch export",
            "description": "Create tests (e.g., tests/test_interactive_aeolian.py) to verify aeolian slider defaults, preview call ordering, and batch-export output counts, plus basic widget behavior where feasible.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Introduce a dedicated test module, such as tests/test_interactive_aeolian.py, focusing on the interactive layer for the aeolian environment. Add tests for build_sliders(\"aeolian\") that validate coverage and correctness of min/max/step/default values relative to AeolianParams and PRD. Implement tests for preview_sequence(\"aeolian\") that use mocks to ensure generate_aeolian is called for each seed and that the returned stages follow the exact required order. Add tests for the batch-export helper that run against a temporary directory and verify that the expected number of files and metrics rows are created for a small sweep. Where practical, include basic ipywidgets tests to confirm that changing slider values or pressing a batch-export button triggers the right internal calls. Ensure tests run under the existing CI configuration and extend any smoke-test scripts (e.g., scripts/smoke_test.py) with an aeolian interactive check if appropriate.",
            "status": "pending",
            "testStrategy": "Run the new test_interactive_aeolian.py module with pytest and ensure all tests pass locally. Confirm that tests are robust to headless CI environments by avoiding hard dependencies on graphical backends and by using non-interactive matplotlib backends or pure data checks. Optionally integrate a small smoke-test entry that constructs aeolian sliders and runs a minimal preview and batch run to guard against regressions across future refactors."
          }
        ]
      },
      {
        "id": 8,
        "title": "Extend stats engine with aeolian Phase1/Phase2 metrics",
        "description": "Implement Phase 1/2 metrics plus aeolian-specific metrics (orientation rose vs θ, ridge continuity, interdune connectivity, PSD residuals, property trend logging).",
        "details": "In `stats.py`, expand `compute_metrics` to detect env == \"aeolian\" and compute β_iso, β_dir_{0,45,90,135}, anisotropy ratio, two-segment fits, entropy, D/SFI, PSD anisotropy/aspect/θ, topology counts per mask using numpy/scipy. Incorporate Task 5 metadata for cross-bedding histograms, roundedness, texture variance, inversions, event counts, and crater frequencies. Return dict ready for CSV/export. Provide helper functions for PSD orientation residual vs θ_wind.\nPseudo-code:\n```\ndef compute_metrics(gray, masks, env):\n    if env == \"aeolian\":\n        beta = calc_beta(gray)\n        psd = fft2(gray)\n        ridge = masks[\"ridge\"]\n        metrics = {\n            \"beta_iso\": beta.iso,\n            \"ridge_continuity\": ridge_continuity(ridge),\n            ...\n        }\n        return metrics\n    ...\n```\nUpdate docstrings referencing research PDFs and acceptance thresholds.",
        "testStrategy": "Add `tests/test_aeolian_metrics.py` generating small synthetic masks to verify (a) PSD orientation residual within ±10° for aligned inputs, (b) ridge continuity matches Task 3 output, (c) interdune connectivity calculation equals 1−χ/A, and (d) cross-bedding hist hist counts include >0 entries. Use fixtures to assert metrics schema stability.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design env-aware compute_metrics API and aeolian dispatch",
            "description": "Design the overall structure of compute_metrics in stats.py to support env-specific branches and add a clear API contract for the aeolian case.",
            "dependencies": [],
            "details": "Define the function signature, expected input types and shapes for gray and masks coming from generate_aeolian, and how env will be passed. Sketch an internal dispatch pattern (e.g., if/elif or small registry) that cleanly isolates aeolian metrics from other environments while sharing common helpers. Capture required metric groups (beta, entropy, PSD, topology, metadata) and expected output dict keys to guide subsequent implementation.",
            "status": "pending",
            "testStrategy": "Add a lightweight design-only test or assertion-based check that calling compute_metrics with env='aeolian' raises NotImplementedError or returns a placeholder structure before full implementation, ensuring the dispatch path exists."
          },
          {
            "id": 2,
            "title": "Implement beta and anisotropy helpers for aeolian gray fields",
            "description": "Create helper functions to compute β_iso, directional β at 0/45/90/135 degrees, and anisotropy ratios from gray-scale dune fields.",
            "dependencies": [
              1
            ],
            "details": "Using numpy and scipy, implement reusable functions such as compute_beta_iso, compute_beta_directional, and compute_anisotropy_ratio that operate on the gray intensity field. Decide on windowing or global statistics, handle NaNs or masked areas, and ensure outputs are scalar metrics suitable for CSV export. Implement two-segment fits for directional trends (e.g., piecewise linear fits to angle–intensity relationships) with robust handling of edge cases and small images.",
            "status": "pending",
            "testStrategy": "Write unit tests that construct synthetic gray arrays with known oriented gradients or sinusoidal patterns and verify that β_iso and directional β values follow expected ordering and that anisotropy ratios exceed 1 for strongly oriented patterns."
          },
          {
            "id": 3,
            "title": "Implement entropy and D/SFI-style fractal metrics",
            "description": "Add entropy-based and D/SFI-like fractal or roughness metrics for aeolian gray fields following depositional_environments_generation_rules.pdf guidance.",
            "dependencies": [
              1
            ],
            "details": "Implement helper functions to compute intensity histogram entropy, possible multi-scale entropy, and a D or SFI-like scalar that characterizes spatial roughness or fractal behavior of the dune field. Use numpy histogramming and optionally scipy-based scaling analysis, adhering to thresholds and interpretations described in depositional_environments_generation_rules.pdf. Ensure these functions accept gray plus optional masks and return stable scalar metrics for inclusion in the aeolian metrics dict.",
            "status": "pending",
            "testStrategy": "Create tests using simple synthetic patterns (uniform, checkerboard, noisy gradient) and assert that entropy and D/SFI metrics follow intuitive ordering (e.g., uniform < checkerboard < noise) and remain numerically stable across resolutions."
          },
          {
            "id": 4,
            "title": "Implement PSD-based anisotropy and orientation metrics via FFT",
            "description": "Compute PSD anisotropy, aspect ratio, dominant orientation θ, and residual vs θ_wind using FFT-based analysis of aeolian gray fields.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement FFT-based helpers that compute the 2D power spectrum of the gray field, convert to polar coordinates, and derive metrics such as anisotropy (max/min radial power), aspect ratio of the dominant lobe, and dominant orientation angle. Add a function that takes θ_wind and returns an orientation residual metric based on the difference between PSD peak orientation and wind direction. Ensure efficient use of numpy.fft, apply windowing to reduce edge effects, and expose all PSD-derived metrics in a structured result for the aeolian branch.",
            "status": "pending",
            "testStrategy": "In tests, generate synthetic stripe patterns aligned at known angles (e.g., 0, 45, 90 degrees) and assert that the dominant θ is within a small angular tolerance and that the residual vs θ_wind falls within ±10° for aligned cases while showing larger residuals for misaligned patterns."
          },
          {
            "id": 5,
            "title": "Implement topology and mask-based metrics for ridges and interdunes",
            "description": "Add mask-driven topology metrics including ridge continuity, interdune connectivity, and cross-bedding set counts using labeled connectivity analysis.",
            "dependencies": [
              1
            ],
            "details": "Using masks from generate_aeolian (e.g., ridge, interdune, cross_bedding), implement functions that compute ridge continuity (largest crest component area divided by total ridge area), an interdune connectivity metric compatible with 1−χ/A or similar, and counts or size distributions of cross-bedding sets. Use scipy.ndimage or similar labeling operations to detect connected components and derive topology metrics. Ensure masks are validated, handle missing keys gracefully, and return scalar metrics per mask suitable for CSV export.",
            "status": "pending",
            "testStrategy": "Write tests that build small binary masks with known connected components and gaps, then verify that ridge continuity matches expected ratios, interdune connectivity equals the specified formula on simple shapes, and cross-bedding counts match the number of synthetic sets seeded in the mask."
          },
          {
            "id": 6,
            "title": "Integrate aeolian overlays and metadata from Task 5 into metrics",
            "description": "Wire in Aeolian overlay metadata such as rounding indices, gradation inversions, texture variance, event counts, and crater frequencies into the metrics pipeline.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Extend compute_metrics and associated helpers to accept or retrieve metadata structures produced in Task 5 (e.g., dictionaries describing rounding indices, gradation inversion flags, texture variance fields, event logs, and crater statistics). Normalize these into scalar or small-vector metrics with clear naming, encode flags as booleans or 0/1 where appropriate, and merge them into the aeolian metrics dict alongside numeric field-based metrics. Ensure that missing metadata is handled robustly with defaults or explicit null-like values.",
            "status": "pending",
            "testStrategy": "Create a fake metadata object matching the Task 5 schema and test that compute_metrics(env='aeolian') incorporates all expected fields into its output dict with correct values, default behaviors when keys are missing, and no unexpected KeyErrors."
          },
          {
            "id": 7,
            "title": "Define stable aeolian metrics schema and naming for exports",
            "description": "Design and codify a stable schema for aeolian metrics, including names, units, and descriptions compatible with reporting and docs.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Compile a comprehensive list of aeolian metrics (beta, entropy, D/SFI, PSD metrics, topology metrics, metadata-derived metrics) and define canonical field names, units, and brief descriptions. Implement a small schema registry or mapping in stats.py or a nearby module that compute_metrics uses when constructing the output dict, ensuring column order stability where needed. Cross-check the schema against reporting.build_reports expectations and update docs/TASKMASTER_TASKS_EXPORT.md to reflect the new aeolian metrics.",
            "status": "pending",
            "testStrategy": "Add tests that call compute_metrics with env='aeolian' on a simple input and assert that the returned dict keys exactly match the schema definition (names and count), failing if new metrics are added without updating the schema, thereby guarding schema stability over time."
          },
          {
            "id": 8,
            "title": "Integrate aeolian metrics with reporting and CSV/export pipeline",
            "description": "Ensure aeolian metrics flow cleanly into reporting.build_reports and CSV generation, matching the agreed schema and formats.",
            "dependencies": [
              7,
              6
            ],
            "details": "Update reporting-related code to recognize env=='aeolian' and include the full set of aeolian metrics in CSV outputs, per-environment PDFs, and the master PDF. Confirm that metric names, units, and groupings match the schema from the previous subtask and that legends or labels remain consistent with PALETTES and documentation. Verify that batch runs and Task Master exports can consume the new metrics without additional configuration changes.",
            "status": "pending",
            "testStrategy": "Create integration-style tests that run a minimal end-to-end flow: generate a synthetic aeolian realization, call compute_metrics, and then invoke the reporting layer to produce CSV rows or in-memory report structures, asserting that all expected aeolian columns are present and correctly populated."
          },
          {
            "id": 9,
            "title": "Optimize aeolian metrics computation for batch and CI performance",
            "description": "Profile and optimize the aeolian metrics implementation to run efficiently in batch sweeps and CI pipelines without timeouts.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              8
            ],
            "details": "Review all aeolian metric helpers for unnecessary recomputation, ensure FFTs and label operations are reused or cached when possible, and prefer vectorized numpy operations over Python loops. Add sensible limits or fallbacks for extremely large grids and document expected runtime characteristics. Where appropriate, gate expensive research-only metrics behind flags while keeping the default CI path fast. Confirm memory usage is acceptable for multi-realization batch runs.",
            "status": "pending",
            "testStrategy": "Add timing-focused tests or benchmarks that run compute_metrics(env='aeolian') on representative grid sizes and assert that runtime falls below agreed thresholds, at least in debug mode. Optionally, include a simple regression guard that fails if runtime increases dramatically compared to a stored baseline."
          },
          {
            "id": 10,
            "title": "Add synthetic-pattern tests for aeolian metrics and schema stability",
            "description": "Create tests/test_aeolian_metrics.py with synthetic patterns and masks to validate metric correctness, angular tolerances, topology, histograms, and schema stability.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9
            ],
            "details": "Implement a dedicated test module tests/test_aeolian_metrics.py that constructs synthetic gray fields (oriented stripes, isotropic noise, gradients) and masks (simple ridge and interdune patterns, cross-bedding patches) to exercise all aeolian metrics. Validate PSD orientation residuals against θ_wind within ±10°, confirm ridge continuity and interdune connectivity formulas, check that cross-bedding and overlay histograms produce nonzero, reasonable counts, and assert that the output metrics dict adheres to the defined schema. Use fixed random seeds to keep tests deterministic.",
            "status": "pending",
            "testStrategy": "Develop a suite of pytest tests in tests/test_aeolian_metrics.py covering PSD orientation, entropy ordering, D/SFI behavior, ridge continuity, interdune connectivity, overlay metadata integration, and strict checks on the metrics dict keys and basic value ranges to guard against regressions."
          }
        ]
      },
      {
        "id": 9,
        "title": "Enhance reporting for aeolian CSV + PDF deliverables",
        "description": "Upgrade `reporting.py` to emit aeolian-aware CSV columns, per-style PDFs, and a master PDF containing thumbnails, legends, and property summaries.",
        "details": "Modify `build_reports` to accept env metadata, add Aeolian metrics columns (ridge continuity, PSD residual, rounding index, gradation sign, cement type, event counts). Use ReportLab to create templates for barchan/linear/transverse PDFs with slider summaries, sequential preview frames, and acceptance-band annotations. Generate a master PDF combining all runs plus legends from docs/PALETTES.md, ensuring cement tags appear in legends. Save CSV schema reference in `docs/TASKMASTER_TASKS_EXPORT.md`.\nPseudo-code:\n```\ndef build_reports(metrics_rows, output_dir):\n    csv_path = write_csv(metrics_rows, aeolian_schema)\n    for style in {\"barchan\",\"linear\",\"transverse\"}:\n        style_rows = filter_rows(metrics_rows, style)\n        pdf = build_style_pdf(style_rows)\n    build_master_pdf(metrics_rows)\n```\nEnsure outputs integrate with Task Master artifacts per acceptance criteria.",
        "testStrategy": "Add `tests/test_reporting_aeolian.py` to (a) write sample metrics and assert CSV headers match schema, (b) verify per-style PDFs exist and contain expected text via PyPDF2, and (c) confirm master PDF includes thumbnails count equal to realizations. Include golden-file checksum checks for CI.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Review existing reporting, metrics, and artifact expectations",
            "description": "Inspect current reporting stub, metrics contracts, and artifact expectations for Aeolian runs.",
            "dependencies": [],
            "details": "Open `src/analog_image_generator/reporting.py`, `scripts/smoke_test.py`, Task 8 metrics specs, and any existing CSV/PDF outputs to understand required inputs, current naming conventions, and how Task Master ingests artifacts for Aeolian environments.",
            "status": "pending",
            "testStrategy": "No direct tests; validate by capturing notes on current file structure, expected output locations, and any implicit contracts used by `scripts/smoke_test.py` or Task Master."
          },
          {
            "id": 2,
            "title": "Design Aeolian CSV schema and update TASKMASTER_TASKS_EXPORT documentation",
            "description": "Define full Aeolian-aware CSV column schema and document it clearly.",
            "dependencies": [
              1
            ],
            "details": "Specify a structured CSV schema including base columns plus Aeolian metrics such as ridge continuity, PSD residual, rounding index, gradation sign, cement type, and event counts. Ensure column order, naming, dtypes, and units are clearly defined and then update or extend `docs/TASKMASTER_TASKS_EXPORT.md` so the Aeolian schema is versioned and traceable for downstream consumers.",
            "status": "pending",
            "testStrategy": "Manually review the updated schema against Task 8 metrics definitions to ensure every Aeolian metric has a documented column and that the document reflects the actual planned header order and naming."
          },
          {
            "id": 3,
            "title": "Implement CSV writing helpers to emit Aeolian metrics with new schema",
            "description": "Create or refactor helpers that write Aeolian rows to CSV using the designed schema.",
            "dependencies": [
              2
            ],
            "details": "Extend `src/analog_image_generator/reporting.py` with dedicated CSV writer utilities that accept env metadata and Aeolian metrics rows, map them to the new schema, and either create or append to the CSV file. Ensure missing values are handled consistently, headers are stable, and row ordering supports later PDF aggregation while remaining compatible with existing non-Aeolian reporting (if any).",
            "status": "pending",
            "testStrategy": "Add local helper-focused checks in `tests/test_reporting_aeolian.py` that call the CSV writer with a few synthetic Aeolian metrics rows and assert that the generated file exists and its header exactly matches the schema defined in `docs/TASKMASTER_TASKS_EXPORT.md`."
          },
          {
            "id": 4,
            "title": "Extend build_reports signature and orchestration for Aeolian env metadata",
            "description": "Update build_reports to accept env metadata and route Aeolian metrics into CSV/PDF generation.",
            "dependencies": [
              3
            ],
            "details": "Modify `build_reports` in `reporting.py` to accept explicit env metadata (e.g., env name, style, run identifiers, parameter summaries) alongside Aeolian metrics rows. Have it call the new CSV writer helpers, group metrics by dune style (barchan, linear, transverse), and prepare the per-style slices and thumbnails needed by downstream PDF builders without yet finalizing layout details.",
            "status": "pending",
            "testStrategy": "Create a thin test in `tests/test_reporting_aeolian.py` that calls `build_reports` with a temporary directory, mock env metadata, and a few Aeolian metrics rows, then asserts that the function completes without raising and that at least one CSV file is produced in the expected directory."
          },
          {
            "id": 5,
            "title": "Implement per-style Aeolian PDF templates with ReportLab",
            "description": "Use ReportLab to build barchan, linear, and transverse PDF layouts with thumbnails and metrics.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Add style-specific PDF builder functions (e.g., `build_style_pdf(style_name, style_rows, env_metadata, thumbnails, output_dir)`) that use ReportLab to render per-style documents containing run thumbnails, slider summaries, key Aeolian metrics, and acceptance-band annotations. Ensure the layout is reusable across styles with minimal duplication while allowing style-specific labels or sections when needed.",
            "status": "pending",
            "testStrategy": "In `tests/test_reporting_aeolian.py`, generate a tiny sample PDF per style using a few dummy metrics rows and placeholder images, then use PyPDF2 to confirm each file is readable, has at least one page, and contains key strings such as the style name and one or two metric labels in the extracted text."
          },
          {
            "id": 6,
            "title": "Integrate palette legends and cementation color keys from PALETTES.md",
            "description": "Load legend definitions and cementation colors from docs/PALETTES.md for use in PDFs.",
            "dependencies": [
              1,
              5
            ],
            "details": "Implement a small parser or mapping loader that reads `docs/PALETTES.md` (or an auxiliary structured file if available) to obtain palette labels, color swatches, and cementation tags. Expose a helper that renders consistent legend blocks and cementation color keys in ReportLab canvases, and then incorporate these blocks into each per-style PDF so legends stay synchronized with the palette documentation.",
            "status": "pending",
            "testStrategy": "Write a test that exercises the legend-loading helper on `docs/PALETTES.md`, validating that expected palette names and at least one cementation tag are discovered, and then generate a tiny PDF to assert that those legend labels appear in its extracted text via PyPDF2."
          },
          {
            "id": 7,
            "title": "Implement master Aeolian PDF assembly with legends and property summaries",
            "description": "Compose a master PDF that aggregates all Aeolian runs, style PDFs, legends, and summary tables.",
            "dependencies": [
              5,
              6
            ],
            "details": "Add a `build_master_pdf` function that collates metrics across all Aeolian runs, constructs property summary tables, embeds or references per-style sections, and appends palette legends and cementation keys. Ensure the master layout includes thumbnails for each realization, clearly labeled sections per style, and stable ordering so downstream checks can count thumbnails and pages reliably.",
            "status": "pending",
            "testStrategy": "Extend `tests/test_reporting_aeolian.py` to generate a master PDF from a small mixed-style metrics set, then use PyPDF2 to verify the file is readable, contains text fragments for all three styles, and that the number of thumbnail placeholders or run labels matches the number of input realizations."
          },
          {
            "id": 8,
            "title": "Align file naming and output directories with smoke tests and Task Master artifacts",
            "description": "Standardize CSV and PDF filenames and directories to match smoke tests and artifact contracts.",
            "dependencies": [
              3,
              4,
              5,
              7
            ],
            "details": "Define a clear output layout under an `outputs` or project-specific reporting directory such that CSVs, per-style PDFs, and the master PDF follow predictable naming patterns keyed by env, tag, and timestamp. Update `build_reports` to honor this convention and cross-check against `scripts/smoke_test.py` and Task Master documentation so the Aeolian outputs are automatically discovered and archived without breaking existing workflows.",
            "status": "pending",
            "testStrategy": "Augment `tests/test_reporting_aeolian.py` with assertions about path patterns and filenames after running `build_reports`, and, if practical, invoke the relevant part of `scripts/smoke_test.py` in a temporary directory to ensure it locates the Aeolian artifacts successfully."
          },
          {
            "id": 9,
            "title": "Add end-to-end Aeolian reporting tests and basic checksums",
            "description": "Create focused tests to validate CSV headers, PDF content, and simple structural checksums for Aeolian reporting.",
            "dependencies": [
              3,
              5,
              7,
              8
            ],
            "details": "Introduce `tests/test_reporting_aeolian.py` to drive a minimal end-to-end Aeolian reporting run that writes CSV, per-style PDFs, and a master PDF from a small synthetic metrics dataset. Assert that CSV headers match the documented schema, PDFs are readable and contain expected labels, and optionally compute lightweight checksums or size thresholds so CI can detect accidental schema or layout regressions without relying on heavy golden files.",
            "status": "pending",
            "testStrategy": "Implement Pytest cases that call `build_reports` into a temporary directory, then (a) read the CSV to verify headers and a few cell values, (b) inspect generated PDFs with PyPDF2 to confirm they contain key text and the expected number of pages, and (c) compute a simple checksum or file-size band to guard against major unintentional changes in output structure."
          }
        ]
      },
      {
        "id": 10,
        "title": "Traceability, documentation, and QA automation for aeolian release",
        "description": "Update documentation/notebooks for new functions, ensure GEOLOGIC_RULES anchors align, and add QA smoke routines covering interactive + stats + reporting pipeline.",
        "details": "Refresh `docs/GEOLOGIC_RULES.md`, `docs/WORKFLOW.md`, PRD appendix, and `notebooks/aeolian.ipynb` anchors for every function added in Tasks 1–9. Create QA checklist in notebooks (per AGENTS instructions) confirming acceptance metrics hits. Extend `scripts/smoke_test.py` to run `generate_aeolian` for each env, compute metrics, and write reports, flagging threshold violations. Capture recap in `docs/MEETING_RECAP_2025-11-12.md` and add Task Master updates. Ensure `README` highlights Aeolian support and prerequisites.",
        "testStrategy": "Add `tests/test_smoke_aeolian.py` invoking the smoke script in dry-run mode, asserting it produces metrics + PDF placeholders and that checklists tick through. Validate docs via markdown lint (pre-commit) and ensure notebooks store anchors referencing updated code names.",
        "priority": "medium",
        "dependencies": [
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Align GEOLOGIC_RULES anchors with Aeolian functions from Tasks 1–9",
            "description": "Inventory all Aeolian-related functions implemented in Tasks 1–9 and update `docs/GEOLOGIC_RULES.md` so that every Aeolian generator, overlay, stats, and reporting entry has a correct code anchor and notebook anchor reference.",
            "dependencies": [],
            "details": "Scan the `analog_image_generator` package for Aeolian-specific functions (e.g., barchan, linear, transverse, overlays, metrics, reporting helpers) introduced in Tasks 1–9 and list them alongside their fully qualified names. Update the Aeolian rows in `docs/GEOLOGIC_RULES.md` so each principle includes a `code anchor` column using the fully qualified Python name (e.g., `analog_image_generator.geologic_generators.aeolian_barchan.build_barchan_field`) and a `notebook anchor` column pointing to the expected `notebooks/aeolian.ipynb` markdown anchor IDs that follow the `anchor-aeolian-<principle>` convention. Ensure there are no stale or mismatched anchor entries and that every implemented Aeolian function is represented exactly once in the rules table.",
            "status": "pending",
            "testStrategy": "Run markdown lint (or existing docs checks) on `docs/GEOLOGIC_RULES.md` and manually cross-check a sample of anchors by opening the notebook and code to ensure the fully qualified names and anchor IDs resolve as expected."
          },
          {
            "id": 2,
            "title": "Document Aeolian workflow, sliders, and acceptance criteria in WORKFLOW, README, and PRD",
            "description": "Refresh `docs/WORKFLOW.md`, `README.md`, and relevant PRD appendices to describe Aeolian support, exposed slider ranges, and acceptance criteria consistent with AGENTS and GEOLOGIC_RULES.",
            "dependencies": [
              1
            ],
            "details": "Edit `docs/WORKFLOW.md` to add an Aeolian-focused section that explains how to run Aeolian generators, how Task Master tasks map to Aeolian milestones, and how notebook anchors and GEOLOGIC_RULES entries stay in sync. Update `README.md` to highlight Aeolian support, prerequisites (e.g., required Python packages, Jupyter widgets), and how to invoke Aeolian runs and smoke tests from the CLI. In the PRD appendix (e.g., `.taskmaster/docs/prd.txt` or Aeolian-specific PRD files), add or refine requirements for Aeolian slider ranges (e.g., wind direction, dune height, defect rate, spacing parameters), target metrics bands, and QA acceptance criteria, ensuring they match the principles and function list captured in `docs/GEOLOGIC_RULES.md`. Verify that terminology and parameter names are consistent across all documents.",
            "status": "pending",
            "testStrategy": "Render or preview the updated markdown files, run any existing markdown lint or link-check tasks, and confirm that Aeolian sections reference the same function names, slider parameters, and metric bands as `docs/GEOLOGIC_RULES.md` and the implemented code."
          },
          {
            "id": 3,
            "title": "Update `notebooks/aeolian.ipynb` anchors and add QA checklist per AGENTS",
            "description": "Revise `notebooks/aeolian.ipynb` to include markdown anchor cells for each Aeolian principle and to add a top-level QA checklist cell that reflects the AGENTS roles and acceptance criteria for the Aeolian release.",
            "dependencies": [
              1,
              2
            ],
            "details": "Open `notebooks/aeolian.ipynb` and add or update dedicated markdown cells that define anchors with IDs following the required convention `anchor-aeolian-<principle>` for every Aeolian rule represented in `docs/GEOLOGIC_RULES.md`. Each anchor cell should clearly reference the corresponding fully qualified code function and briefly summarize the geologic principle being demonstrated. At the top of the notebook, insert a QA checklist markdown cell that enumerates GEO, GEN, UX, STAT, REP, QA, and DOC acceptance items specific to Aeolian workflows (e.g., masks correctness, slider responsiveness, metric bands, report generation). Ensure the checklist items are phrased so they can be manually checked off during review and that they align with the notebook content and smoke test behavior.",
            "status": "pending",
            "testStrategy": "Run the Aeolian notebook end-to-end in Jupyter, verify that all anchor markdown cells are present and correctly named, and confirm that the QA checklist appears at the top and can be visually checked off when conditions are met. Optionally run any existing notebook execution tests if present."
          },
          {
            "id": 4,
            "title": "Extend `scripts/smoke_test.py` to run Aeolian pipeline in dry-run mode",
            "description": "Modify `scripts/smoke_test.py` to add an Aeolian-focused smoke routine that runs `generate_aeolian` for each configured environment, computes metrics via `stats.compute_metrics`, and generates reports via `reporting.build_reports` into a temporary or dry-run output location.",
            "dependencies": [
              1,
              3
            ],
            "details": "Extend the existing `scripts/smoke_test.py` script by adding a clear entry point or flag (e.g., `--aeolian` or `--env-type aeolian`) that triggers Aeolian-only smoke tests. Within this flow, invoke `generate_aeolian` for each Aeolian environment defined in configuration, passing deterministic seeds and minimal domain sizes suitable for CI. Pipe the generated outputs into `stats.compute_metrics` and then call `reporting.build_reports` with a temporary directory or dry-run mode so that PDF/CSV outputs and any auxiliary artifacts are produced without polluting the main reports directory. Ensure logging clearly indicates which envs ran and where outputs are written, and that failures bubble up with non-zero exit codes appropriate for CI usage.",
            "status": "pending",
            "testStrategy": "Run `python scripts/smoke_test.py --aeolian --dry-run` (or equivalent) locally to verify that it completes successfully, creates metrics and report artifacts in the temporary directory, and prints clear logs. Confirm it returns a non-zero exit code when an intentional failure is injected (e.g., by forcing an exception) to ensure CI will correctly detect regressions."
          },
          {
            "id": 5,
            "title": "Implement Aeolian metric and report threshold checks in smoke test",
            "description": "Add simple yet robust threshold checks into the Aeolian branch of `scripts/smoke_test.py` to flag obvious metric or reporting regressions and fail fast when outputs fall outside expected bands or are missing.",
            "dependencies": [
              4
            ],
            "details": "Within the Aeolian path of `scripts/smoke_test.py`, after calling `stats.compute_metrics` and `reporting.build_reports`, implement checks that validate critical Aeolian metrics and artifacts. For example, assert that key metric values (e.g., dune orientation errors, continuity indices, coverage fractions) lie within sane ranges derived from Tasks 2–5 test expectations, and that all expected report files (CSV, PDF, or placeholder artifacts) are present in the dry-run output directory. On any violation or missing artifact, emit a clear error message describing which environment and metric failed and exit with a non-zero status. Keep thresholds conservative to avoid flakiness but strict enough to catch regressions, and centralize them in a small config structure or function so they can be tuned as Aeolian implementations evolve.",
            "status": "pending",
            "testStrategy": "Execute the Aeolian smoke test multiple times with default parameters to ensure stable pass behavior, then temporarily tweak parameters or thresholds to force failures and confirm that the script correctly detects out-of-range metrics, missing reports, and propagates meaningful error messages and non-zero exit codes suitable for CI."
          },
          {
            "id": 6,
            "title": "Integrate Aeolian docs and smoke checks into pre-commit and CI workflows",
            "description": "Update pre-commit hooks and CI configurations so that Aeolian documentation validation and the Aeolian smoke test run automatically where appropriate, without significantly impacting developer feedback time.",
            "dependencies": [
              2,
              3,
              5
            ],
            "details": "Review existing pre-commit and CI configuration files (e.g., `.pre-commit-config.yaml`, GitHub Actions workflows, or other CI scripts) and add or extend steps to cover Aeolian-related checks. For pre-commit, ensure that relevant markdown linting, notebook metadata checks, and fast sanity checks for `docs/GEOLOGIC_RULES.md`, `docs/WORKFLOW.md`, `README.md`, and `notebooks/aeolian.ipynb` run locally, ideally avoiding heavy compute. In CI, add a job or stage that runs `scripts/smoke_test.py` in Aeolian dry-run mode along with the new `tests/test_smoke_aeolian.py`, enforcing the thresholds and artifact checks implemented earlier. Confirm that these additions fit into the existing release loop and that documentation and QA automation steps are clearly reflected in the pipeline logs.",
            "status": "pending",
            "testStrategy": "Trigger the CI pipeline (or run the equivalent CI scripts locally) to verify that the Aeolian-specific pre-commit and CI steps execute successfully, fail when the smoke test or docs are intentionally broken, and complete within acceptable time limits. Confirm that any status badges or release gates account for the new checks."
          },
          {
            "id": 7,
            "title": "Write Aeolian release recap and Task Master usage in MEETING_RECAP",
            "description": "Capture a concise but complete recap in `docs/MEETING_RECAP_2025-11-12.md` summarizing the Aeolian implementation status, QA posture, and how Task Master tags and tasks were used to track this work.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Edit `docs/MEETING_RECAP_2025-11-12.md` to add a dedicated Aeolian section describing which generators, overlays, stats, and reporting pieces are now implemented, how GEOLOGIC_RULES anchors and notebook anchors map to the code, and what the new QA automation (smoke tests, CI hooks, and tests) covers. Include a brief narrative of Task Master usage, noting relevant tags (e.g., `aeolian-v1` or milestone tags), key tasks (including Task 10 and its subtasks), and any decisions or tradeoffs made around thresholds, performance, or documentation scope. Ensure that the recap links to or references updated docs and notebooks so future reviewers can trace requirements through to implementation and QA artifacts.",
            "status": "pending",
            "testStrategy": "Manually review the recap document to ensure it accurately reflects the implemented Aeolian features, anchors, and QA pipeline, and cross-check that linked files and Task Master tags exist and match current project state."
          },
          {
            "id": 8,
            "title": "Add `tests/test_smoke_aeolian.py` to validate Aeolian smoke pipeline",
            "description": "Create `tests/test_smoke_aeolian.py` that exercises the Aeolian branch of `scripts/smoke_test.py` in a controlled mode, asserting that metrics and report placeholders are produced and that notebook QA checklist expectations are satisfied.",
            "dependencies": [
              4,
              5,
              6,
              3
            ],
            "details": "Implement a new test module `tests/test_smoke_aeolian.py` that either imports a callable API from `scripts/smoke_test.py` or invokes the script via a subprocess in a fast dry-run mode confined to a temporary directory. The test should run the Aeolian smoke pipeline for at least one representative environment, then assert that metrics outputs (e.g., JSON or CSV files) and report placeholders (e.g., generated PDFs or stub files) exist and are non-empty. Where feasible, also validate that the notebook QA checklist is referenced or surfaced by the smoke routine (for example via a summary log or checklist status), or at minimum confirm that the anchors and checklist cell can be located by a lightweight notebook inspection helper. Ensure the test is deterministic, skips gracefully if Aeolian features are not configured, and integrates cleanly into the existing pytest suite.",
            "status": "pending",
            "testStrategy": "Run `pytest tests/test_smoke_aeolian.py` locally to confirm the test passes on a clean tree, then introduce controlled breakages (e.g., delete an expected output file or loosen a threshold) to verify that the test fails with clear diagnostics that point to missing metrics, reports, or checklist violations."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-11-16T17:13:58.795Z",
      "updated": "2025-11-16T17:13:58.795Z",
      "description": "Tasks for aeolian-v1 context"
    }
  },
  "estuarine-v1": {
    "tasks": [
      {
        "id": 1,
        "title": "Define estuarine parameter schema & slider metadata",
        "description": "Introduce a strongly typed estuarine parameter schema plus slider metadata so downstream generators and UI can consume a single source of truth for ranges, defaults, and documentation links from the PRD.",
        "details": "- Create `EstuarineParams` (e.g., dataclass or TypedDict) under `analog_image_generator.geologic_generators` capturing tidal prism, wave energy index, channel/bar wavelengths (px), mud fraction, delta-front angle φ, dominance δ, interlayer type enum, and RNG seed.\n- Keep values normalized to slider ranges from `.taskmaster/docs/prd_estuarine.txt`; compute derived values such as km using the 5 m/px scale only inside helper properties.\n- Expose metadata (min, max, step, default, tooltip text citing the correct PDF section) via `analog_image_generator.interactive.build_sliders` so UI consumers read from `PARAM_METADATA[\"estuarine\"]` instead of hardcoding.\n- Provide helper `clamp_estuarine_params(raw: Mapping[str, Any]) -> EstuarineParams` that enforces ranges and converts enums/angles.\n- Pseudocode:\n```\n@dataclass\nclass EstuarineParams:\n    tidal_prism: float = 0.5\n    ...\n\nPARAM_METADATA = {\n  \"estuarine\": {\n      \"tidal_prism\": {\"min\": 0.1, \"max\": 1.0, \"step\": 0.05, \"default\": 0.5,\n                        \"source\": \"depositional_system_estuarian_tide-dominated.pdf §3\"},\n      ...\n  }\n}\n```\n- Update `docs/GEOLOGIC_RULES.md` later to point to the new helper once implemented (tracked in a later task).",
        "testStrategy": "- Add unit tests that instantiate `EstuarineParams` with defaults and boundary values, asserting clamping honors PRD limits and enum normalization (pytest target `tests/test_params.py`).\n- Validate `build_sliders(\"estuarine\")` returns metadata for every slider with expected min/max/default and citation text.\n- Include a regression test that feeds invalid values (e.g., δ>1) through `clamp_estuarine_params` and asserts the corrected value stays within range.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Extract estuarine parameters and ranges from PRD",
            "description": "Review `.taskmaster/docs/prd_estuarine.txt` and identify all required estuarine parameters, their slider ranges, defaults, and any linked documentation sections.",
            "dependencies": [],
            "details": "Open `.taskmaster/docs/prd_estuarine.txt` and systematically list tidal prism, wave energy index, channel and bar wavelengths (px), mud fraction, delta-front angle φ, dominance δ, interlayer type enum values, RNG seed behavior, and any additional PRD-defined knobs. Record for each parameter the min, max, step, default, and references to PDF sections so they can be encoded in both the strongly typed schema and slider metadata without guesswork.",
            "status": "pending",
            "testStrategy": "Manually cross-check the extracted parameter list, ranges, and defaults against the PRD and referenced PDFs to ensure no parameter is missing or mis-specified."
          },
          {
            "id": 2,
            "title": "Define strongly typed `EstuarineParams` schema",
            "description": "Implement a typed parameter container `EstuarineParams` under `analog_image_generator.geologic_generators` using the PRD-derived fields and defaults.",
            "dependencies": [
              1
            ],
            "details": "Create `EstuarineParams` (e.g., as a `@dataclass` or `TypedDict`) in the appropriate estuarine-related module of `analog_image_generator.geologic_generators`. Include fields for tidal prism, wave energy index, channel and bar wavelengths (in normalized slider units), mud fraction, delta-front angle φ, dominance δ, interlayer type enum, and RNG seed. Ensure defaults match PRD slider defaults and keep all values in the normalized slider units, providing convenience properties for any 5 m/px km conversions without changing stored units.",
            "status": "pending",
            "testStrategy": "Add or extend unit tests that instantiate `EstuarineParams` with default values and representative overrides, asserting that types, defaults, and any derived properties (e.g., km conversions) behave as expected."
          },
          {
            "id": 3,
            "title": "Implement `PARAM_METADATA[\"estuarine\"]` slider metadata",
            "description": "Create a metadata structure describing slider configuration for estuarine parameters so UI components can consume a single source of truth.",
            "dependencies": [
              1
            ],
            "details": "Add or extend a shared metadata module to define `PARAM_METADATA[\"estuarine\"]`, mapping each estuarine parameter name to an object containing min, max, step, default, display label, and tooltip text. Ensure tooltip strings cite the correct PDF sections (e.g., `depositional_system_estuarian_tide-dominated.pdf §3`) and that numeric values exactly match ranges and defaults extracted from the PRD. Keep the structure compatible with `analog_image_generator.interactive.build_sliders` expectations.",
            "status": "pending",
            "testStrategy": "Write tests that iterate over `PARAM_METADATA[\"estuarine\"]` and verify that each parameter from `EstuarineParams` appears with the correct min, max, step, and default values, and that no unexpected extra keys are present."
          },
          {
            "id": 4,
            "title": "Add `clamp_estuarine_params` normalization and validation helper",
            "description": "Introduce `clamp_estuarine_params(raw: Mapping[str, Any]) -> EstuarineParams` that enforces PRD-backed ranges and normalizes enums and angles.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement `clamp_estuarine_params` to accept loosely-typed input mappings (e.g., from UI or config files), coerce numeric values into the valid slider ranges using `PARAM_METADATA[\"estuarine\"]`, and normalize categorical values such as interlayer type enums and angles like φ and δ (e.g., degrees vs radians). Use the `EstuarineParams` schema for the return type, ensuring that any out-of-range or malformed values are clamped or converted consistently and that defaults are applied when keys are missing.",
            "status": "pending",
            "testStrategy": "Create unit tests that feed `clamp_estuarine_params` with values below and above allowed ranges, missing fields, and variant enum/angle representations, asserting that the returned `EstuarineParams` instances are clamped to PRD limits and enums are normalized."
          },
          {
            "id": 5,
            "title": "Integrate estuarine schema and metadata with `build_sliders`",
            "description": "Wire `EstuarineParams` and `PARAM_METADATA[\"estuarine\"]` into `analog_image_generator.interactive.build_sliders` so UI sliders are generated from the shared configuration.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Update `analog_image_generator.interactive.build_sliders` (or related interactive modules) to read from `PARAM_METADATA[\"estuarine\"]` when requested mode is estuarine, generating slider controls whose ranges, defaults, and tooltips reflect the shared metadata and map cleanly onto `EstuarineParams`. Ensure any existing hardcoded estuarine sliders are removed or migrated, and that the function returns structures compatible with downstream consumers of estuarine parameters.",
            "status": "pending",
            "testStrategy": "Extend or add tests that call `build_sliders(\"estuarine\")` and verify that the returned slider definitions cover every field in `EstuarineParams`, use the expected min/max/default values from metadata, and can round-trip through `clamp_estuarine_params` without losing information."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement tide-dominated estuarine primitives & masks",
        "description": "Add tide-dominated generation helpers that synthesize ebb/flood channels, elongate tidal bars, and mudflat masks with the geometric constraints from the PRD.",
        "details": "- Under `analog_image_generator.geologic_generators`, add functions like `tide_channel_network(params: EstuarineParams, shape: tuple[int, int]) -> NDArray` that create two forked centerlines rotated ±θ relative to shoreline to achieve bimodal peaks separated by ≥25°; use filtered noise + cubic splines and enforce channel sinuosity S within [1.0, 1.8].\n- Implement `tide_bars(channel_mask, params) -> NDArray` that dilates along-flow segments to create elongate tidal bars with length:width ratios 3–15× and λ_b spacing between 20–150 px; modulate amplitude using tidal prism.\n- Add `mudflat_mask(channel_mask, params)` that thresholds distance transforms + mud fraction m to carve intertidal flats.\n- Ensure each helper returns boolean masks plus summary stats (length/width per bar) for later QC metrics.\n- Pseudocode sketch:\n```\ndef tide_channel_network(params, shape):\n    dirs = [+φ_shift, -φ_shift]\n    channels = []\n    for theta in dirs:\n        curve = spline_seed(seed=params.seed, sinuosity=params.channel_sinuosity)\n        curve = rotate(curve, theta)\n        channels.append(draw_tube(curve, width=base_width(params)))\n    chan_mask = union(channels)\n    return chan_mask, measure_orientations(channels)\n```\n- Use numpy/scipy interpolation and `scipy.ndimage.morphology` to dilate/distance operations; keep RNG seeded.",
        "testStrategy": "- Write targeted tests (e.g., `tests/test_estuarine_tide.py`) that call the helper with fixed seeds and assert: (a) measured orientation histogram has two peaks with |Δθ| ≥ 25°, (b) computed sinuosity falls within the slider range, (c) bar length/width ratios stay within 3–15×, (d) mudflat mask area fraction matches mud fraction ±5%.\n- Add quick metric assertions that orientation stats captured in helper metadata feed into later QC calculations (serialized dict fields).",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement `tide_channel_network` with bimodal orientations and controlled sinuosity",
            "description": "Create the core tide-dominated channel network helper that generates two forked ebb/flood centerlines with required orientation separation and sinuosity constraints, returning a boolean mask plus orientation statistics.",
            "dependencies": [],
            "details": "Add `tide_channel_network(params: EstuarineParams, shape: tuple[int, int]) -> tuple[NDArray, dict]` under `analog_image_generator.geologic_generators`. Use a seeded RNG to generate noisy control points, smooth them with cubic splines, and construct two centerlines rotated by ±φ_shift relative to the shoreline so that the measured orientation histogram shows two peaks separated by at least 25°. Enforce channel sinuosity S within [1.0, 1.8] by iterating on control point amplitude or step length until the path-length/straight-line ratio falls in range. Rasterize the splines into tubes using a width function `base_width(params)` and union them into a boolean `chan_mask`. Compute and return summary stats (e.g., per-branch mean orientation, sinuosity values, total channel length) alongside the mask, using NumPy and SciPy interpolation utilities and `scipy.ndimage` where needed.",
            "status": "pending",
            "testStrategy": "Add unit tests in `tests/test_estuarine_tide.py` that call `tide_channel_network` with fixed seeds and assert: (a) the returned mask has the requested shape and is boolean, (b) measured orientation histogram exhibits two prominent peaks with angular separation ≥25°, and (c) computed sinuosity for each branch lies within [1.0, 1.8]. Include edge-case tests for different image sizes and shoreline orientations to ensure robustness."
          },
          {
            "id": 2,
            "title": "Implement `tide_bars` for elongate tidal bar masks and length/width statistics",
            "description": "Build the tidal bar helper that dilates along-flow channel segments into elongate bars with required aspect ratios and spacing, returning a bar mask and per-bar summary metrics.",
            "dependencies": [
              1
            ],
            "details": "Implement `tide_bars(channel_mask: NDArray, params: EstuarineParams) -> tuple[NDArray, dict]` that operates on the tide channel network output. Use `scipy.ndimage` morphology and distance transforms along the local flow direction to dilate channel segments into elongate tidal bars with length:width ratios between 3× and 15× and bar spacing λ_b between 20 and 150 pixels, modulating bar thickness and continuity using a tidal prism–related parameter from `params`. Label individual bars (e.g., via connected-component analysis) and compute per-bar statistics such as length, width, aspect ratio, spacing, and orientation, returning a boolean `bar_mask` plus a structured stats dictionary suitable for later QC metrics and CSV export.",
            "status": "pending",
            "testStrategy": "Extend `tests/test_estuarine_tide.py` with tests that construct a simple synthetic `channel_mask` (or reuse `tide_channel_network` output with a fixed seed) and call `tide_bars`. Assert that the mask is boolean, at least one bar is detected, and that computed per-bar length:width ratios fall within [3, 15]. Measure inter-bar spacing along the main flow direction and confirm typical values lie within 20–150 pixels. Add a parametrized test to verify that changing the tidal prism parameter affects bar thickness or continuity in a monotonic, documented way."
          },
          {
            "id": 3,
            "title": "Implement `mudflat_mask` and integrate tide helpers into a cohesive tide primitives API",
            "description": "Create the mudflat mask helper using distance transforms and mud fraction thresholds, and standardize the return signatures and metadata for all tide-dominated primitives so they can be consumed cleanly by `generate_estuarine` and downstream metrics.",
            "dependencies": [
              1,
              2
            ],
            "details": "Add `mudflat_mask(channel_mask: NDArray, params: EstuarineParams) -> tuple[NDArray, dict]` that uses `scipy.ndimage` distance transforms and a mud fraction parameter `m` to derive intertidal mudflat regions between channels and surrounding areas. Threshold the distance field and optionally combine with any shoreline or elevation cues from `params` to produce a boolean `mud_mask`, and compute basic stats such as total mudflat area fraction and typical width. Then, ensure all three tide helpers (`tide_channel_network`, `tide_bars`, `mudflat_mask`) share consistent signatures (boolean mask plus stats dict), live under `analog_image_generator.geologic_generators`, and are documented for later use by `generate_estuarine` and `stats.compute_metrics`. Update or add minimal module-level documentation strings describing the tide-dominated estuarine primitives API.",
            "status": "pending",
            "testStrategy": "Add tests for `mudflat_mask` that feed in known `channel_mask` patterns and mud fraction values, asserting that the resulting mask is boolean, non-empty, and that mudflat area scales sensibly with `m` (e.g., higher `m` yields larger mudflat coverage). Include a small integration-style test that calls all three helpers in sequence with fixed seeds, verifying that their outputs share the expected shapes, dtypes, and keys in their stats dictionaries so they can be passed into `generate_estuarine` and `stats.compute_metrics` without additional adaptation."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement wave-dominated estuarine primitives & shoreline extraction",
        "description": "Create wave-dominated helpers for shoreface-parallel bars, distributary mouth bars, and shoreline curvature derived from delta-front angle φ.",
        "details": "- Add `wave_shoreface_bars(params, shape)` that generates parallel bars aligned with shoreline azimuth derived from φ, using band-pass filtered noise in the alongshore direction to keep wavelengths λ_b=20–150 px and mouth-bar aspect ratios 2.0–4.0.\n- Build `mouth_bar_field(channel_entries, params)` that seeds fans at channel termini with Gaussian envelopes whose major/minor axes satisfy the PRD ranges; mix in wave energy index to control taper.\n- Implement `extract_shoreline(gray_base) -> (polyline, curvature_stats)` using marching squares on shoreline mask plus smoothing; compute curvature std dev and ensure δ≈1 cases fall below 0.25 rad/pixel by adjusting smoothing strength.\n- Return masks for bars, shoreline, and metadata (curvature std, aspect ratios) for QC.\n- Pseudocode snippet:\n```\ndef wave_shoreline(params, shape):\n    theta = np.deg2rad(params.delta_front_angle)\n    shoreline = build_base_curve(theta)\n    bars = stack_parallel_offset(shoreline, spacing=params.bar_wavelength)\n    return bars_mask, shoreline_poly\n```\n- Keep RNG deterministic via `params.seed` offsets so mixed-energy blending remains reproducible.",
        "testStrategy": "- Unit tests verifying (a) mouth-bar aspect ratio distributions fall within 2.0–4.0 when wave energy index >0.7, (b) shoreline curvature std dev <0.25 for mixed-energy presets, (c) δ→1 scenarios produce PSD aspect ratios >2.0 when evaluated later (record metadata for Task 7).\n- Compare extracted shoreline polylines against analytic arcs to ensure curvature computation is accurate within ±0.02 rad/pixel.\n- Validate mask coverage percentages sum to expected area budgets (channels + bars + flats + shoreline edge).",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement `wave_shoreface_bars(params, shape)` with shoreline-aligned bar masks",
            "description": "Create the core wave_shoreface_bars helper that builds shoreface-parallel bar masks aligned to the shoreline azimuth derived from the delta-front angle φ, enforcing wavelength and aspect ratio constraints.",
            "dependencies": [
              1
            ],
            "details": "Implement `wave_shoreface_bars(params, shape)` under the estuarine generators module, computing shoreline azimuth from `params.delta_front_angle` and constructing a base shoreline curve (e.g., via splines) from which to offset multiple parallel bars. Use deterministic band-pass filtered noise in the alongshore direction (seeded via `params.seed` plus local offsets) to keep bar wavelengths λ_b strictly within 20–150 px. Ensure the generated bar geometries produce mouth-bar-relevant aspect ratios in the 2.0–4.0 range where appropriate, and return a boolean mask (or labeled mask) for wave-generated bars plus any intermediate fields required by downstream helpers (e.g., shoreline polyline or normal vectors).",
            "status": "pending",
            "testStrategy": "Add unit tests (e.g., in tests/test_estuarine_wave.py) that call `wave_shoreface_bars` with fixed `EstuarineParams` seeds and validate: (a) the dominant bar orientation is within a small angular tolerance of the shoreline azimuth; (b) the alongshore power spectrum of the bar mask shows most energy between 20–150 px wavelengths; (c) repeated calls with the same params and seed produce identical masks to confirm deterministic RNG behavior."
          },
          {
            "id": 2,
            "title": "Implement `mouth_bar_field(channel_entries, params)` with Gaussian fans and QC metadata",
            "description": "Build the mouth_bar_field helper that seeds distributary mouth-bar fans at channel termini using Gaussian envelopes constrained by PRD aspect ratio ranges and controlled by a wave energy index.",
            "dependencies": [
              1
            ],
            "details": "Implement `mouth_bar_field(channel_entries, params)` to iterate over channel entry points (termini) and place 2D Gaussian envelopes or similar kernels oriented seaward, with major and minor axes sampled or clamped to satisfy PRD-configured aspect ratio ranges (targeting 2.0–4.0 for wave-dominated mouth bars). Modulate fan taper and downstream extent using a wave energy index from `params` so higher wave energy yields more laterally smeared, tapered bars. Keep RNG deterministic via `params.seed` offsets per channel entry. Return a composite mouth-bar mask and a metadata structure capturing per-bar aspect ratios, positions, and any wave-energy-related parameters for later QC and reporting.",
            "status": "pending",
            "testStrategy": "Create unit tests that feed synthetic `channel_entries` into `mouth_bar_field` with fixed `EstuarineParams`, then estimate effective major/minor axis ratios of the resulting bars (e.g., via second-moment analysis or ellipse fitting) and assert that most ratios fall within 2.0–4.0 when `wave_energy_index > 0.7`. Verify that increasing the wave energy index increases lateral spreading/taper metrics in a monotonic way, and that repeated runs with the same inputs and seed produce identical masks and metadata."
          },
          {
            "id": 3,
            "title": "Implement `extract_shoreline(gray_base)` and integrate wave shoreline pipeline with curvature QC",
            "description": "Implement shoreline extraction and curvature statistics from gray-scale estuarine outputs using marching squares and smoothing, and wire everything into a wave_shoreline-style helper that returns masks and QC metadata.",
            "dependencies": [
              1
            ],
            "details": "Implement `extract_shoreline(gray_base) -> (polyline, curvature_stats)` that thresholds or segments `gray_base` to form a shoreline mask, runs marching squares to extract a polyline, and applies controlled smoothing (e.g., Gaussian or spline smoothing) while computing per-vertex curvature and curvature standard deviation. Adjust smoothing strength as a function of dominance δ (mixed-energy, δ≈1) so such presets produce curvature std dev < 0.25 rad/pixel, while preserving realistic roughness at other δ values. Then implement or finalize a `wave_shoreline(params, shape)` orchestration helper that uses the shoreline geometry together with `wave_shoreface_bars` and `mouth_bar_field` to produce coherent wave-dominated estuarine outputs, returning bar masks, shoreline masks/polylines, and metadata (curvature std, aspect ratios, and relevant parameters) for QC and downstream stats tasks.",
            "status": "pending",
            "testStrategy": "Write unit and integration tests that: (a) generate synthetic noisy shoreline masks with known curvature scales, run `extract_shoreline`, and confirm that curvature standard deviation decreases with stronger smoothing; (b) construct estuarine presets with δ≈1 and verify curvature_std < 0.25 rad/pixel as required; (c) verify that repeated calls with the same `gray_base` and RNG seed are deterministic; and (d) for the integrated `wave_shoreline` helper, assert that returned masks have expected shapes, that metadata includes curvature and aspect ratio summaries, and that basic invariants (e.g., shoreline lies near the bar field) are satisfied."
          }
        ]
      },
      {
        "id": 4,
        "title": "Blend regimes inside `generate_estuarine` and emit sequential masks",
        "description": "Implement the public `generate_estuarine` orchestrator that mixes tide/wave primitives via the dominance slider δ, produces grayscale + mask dict, and records sequential preview stages.",
        "details": "- Inside `analog_image_generator.geologic_generators.generate_estuarine`, call the tide and wave helpers with shared params, then linearly or sigmoid-blend masks using δ (0 = fully tide, 1 = fully wave) while ensuring mud fraction scaling and shoreline curvature targets.\n- Compose intermediate rasters: base topography, combined channels, bars, flats, shoreline, grayscale, and facies RGB (using `docs/PALETTES.md`), storing them in an ordered list for previews.\n- Normalize outputs to `Array` alias (later NDArray) and ensure `masks_dict` exposes keys `channel`, `bar`, `mudflat`, `shoreline`, `dominance_tide`, `dominance_wave`, plus stats bundles for downstream metrics.\n- Include quick metrics (length/width histograms, orientation peaks) in a sidecar metadata object returned with masks to avoid recomputation.\n- Pseudocode skeleton:\n```\ndef generate_estuarine(params_dict):\n    params = clamp_estuarine_params(params_dict)\n    tide_masks = tide_channel_network(...)\n    wave_masks = wave_shoreface_bars(...)\n    blended = blend_masks(tide_masks, wave_masks, params.dominance)\n    sequential = [base_gray, blended['channel'], ..., facies_rgb]\n    return final_gray, {**blended, \"sequence\": sequential}\n```\n- Update `__all__` / module exports if needed so notebooks can import the new helpers.",
        "testStrategy": "- Integration test creating canonical parameter sets (δ=0, 0.5, 1) and asserting returned tuple contains a grayscale array with expected shape plus mask keys; verify sequential list contains the prescribed stages in order.\n- Snapshot-test histograms/metrics stored in metadata to detect regressions (pytest + numpy testing utilities).\n- Run smoke test via `scripts/smoke_test.py` to ensure generator executes within budgeted time (<1s for 512×512) and respects RNG reproducibility by comparing checksums for identical seeds.",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement tidal–wave mask blending logic in `generate_estuarine`",
            "description": "Wire `generate_estuarine` to call the tide and wave helpers with shared clamped parameters and blend their mask stacks using the dominance slider δ so that the regime mix transitions smoothly from fully tide-dominated (δ=0) to fully wave-dominated (δ=1).",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Inside `analog_image_generator.geologic_generators.generate_estuarine`, first obtain a validated `EstuarineParams` instance via the existing clamping routine, then call the tide and wave generator helpers to produce comparable mask dictionaries (e.g., `channel`, `bar`, `mudflat`, `shoreline`). Implement a blending pathway (linear or sigmoid curve) that combines each corresponding mask using δ and optionally a non-linear response curve for shoreline curvature targeting, while preserving total mud fraction and avoiding artifacts such as double-counting channels. Ensure the blended outputs are normalized to the shared `Array` alias type and add derived masks `dominance_tide` and `dominance_wave` that explicitly encode the local or global contribution of each regime for downstream metrics.",
            "status": "pending",
            "testStrategy": "Add focused unit or integration tests that call `generate_estuarine` with canonical parameter sets (δ=0, 0.5, 1) and assert that the blended masks smoothly transition between pure tide and pure wave outputs. Check that each expected key (`channel`, `bar`, `mudflat`, `shoreline`, `dominance_tide`, `dominance_wave`) is present, that array shapes and dtypes match between tide, wave, and blended masks, and that mud fraction statistics remain within acceptable bounds for all δ values."
          },
          {
            "id": 2,
            "title": "Compose ordered sequential preview rasters from blended components",
            "description": "Construct an explicit, ordered sequence of intermediate rasters from the blended estuarine components for use in UI previews and debugging, ensuring each stage is consistently shaped and normalized.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "After blending the tide and wave masks, assemble an ordered Python list (e.g., `sequence`) that captures the key intermediate rasters in the estuarine workflow: base topography, combined channel mask visualization, bar masks, mudflat/flat regions, shoreline raster, final grayscale analog, and optional facies RGB image using palettes from `docs/PALETTES.md`. Normalize all sequence elements to the `Array` alias, enforce consistent spatial dimensions and value ranges, and adopt a stable ordering contract that downstream consumers (UI sliders, notebooks) can rely on. Store this list under a dedicated key such as `\"sequence\"` within the returned mask dictionary so it can be serialized or inspected without additional recomputation.",
            "status": "pending",
            "testStrategy": "Extend integration tests for `generate_estuarine` to assert that the returned mask dictionary includes a `\"sequence\"` key containing a non-empty list of `Array`-compatible rasters with consistent shapes. Verify that the sequence length and ordering match a documented specification (e.g., via a simple index-to-stage mapping) and consider a lightweight snapshot test over sequence metadata (stage names, shapes, min/max ranges) to guard against accidental reordering or omission of stages."
          },
          {
            "id": 3,
            "title": "Finalize `generate_estuarine` return signature, metadata sidecar, and exports",
            "description": "Polish the `generate_estuarine` API so it returns grayscale output plus a rich mask dictionary with quick metrics metadata and is correctly exported for notebook and stats integration.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Ensure `generate_estuarine` returns a tuple `(final_gray: Array, masks_dict: dict)` where `masks_dict` includes the blended structural masks (`channel`, `bar`, `mudflat`, `shoreline`, `dominance_tide`, `dominance_wave`) and the sequential preview list under `\"sequence\"`. Attach a lightweight metadata sidecar (either embedded under a `\"metadata\"` key in `masks_dict` or as a closely associated object) that contains quick metrics such as basic length/width histograms and orientation peaks to support later calls into `stats.compute_metrics` without redundant work. Update module-level exports (`__all__` or equivalent) so that `analog_image_generator.geologic_generators.generate_estuarine` is importable from notebooks and other packages, and align any docstrings or type hints with the estuarine schema and Task Master task expectations.",
            "status": "pending",
            "testStrategy": "Write or update high-level integration tests that import `generate_estuarine` via the public package namespace, call it with representative estuarine parameter sets, and assert the returned tuple structure, key coverage in `masks_dict`, and presence of the quick-metrics metadata block. Include checks that metadata fields such as histogram bin counts and orientation summaries are populated and stable enough to support downstream regression testing, and verify that public imports succeed from expected entry points (e.g., `from analog_image_generator.geologic_generators import generate_estuarine`)."
          }
        ]
      },
      {
        "id": 5,
        "title": "Wire interactive sliders & preview pipeline for estuarine v20a",
        "description": "Implement the interactive layer so estuarine sliders use the shared metadata and preview sequences update in real time with quick metrics.",
        "details": "- Extend `analog_image_generator.interactive.build_sliders` to return slider descriptors for `env=\"estuarine\"`, building UI metadata (label, range, default, tooltip) directly from `PARAM_METADATA` to prevent drift; include interlayer type dropdown and RNG seed control.\n- Implement `preview_sequence(env, params, seeds)` to repeatedly call `generate_estuarine`, capturing the sequential frames plus quick metrics (β_iso, anisotropy ratio) and wiring them into whichever widget stack (ipywidgets or CLI) consumes them.\n- Provide hooks for quick metrics panel updates by computing a lightweight subset of `stats.compute_metrics` (Phase 1 fields) synchronously and caching them with the preview.\n- Ensure slider callbacks throttle updates (e.g., via debouncing) so adjusting φ or δ doesn’t block UI.\n- Pseudocode snippet:\n```\ndef preview_sequence(\"estuarine\", params, seeds):\n    seq = []\n    for seed in seeds:\n        gray, masks = generate_estuarine({**params, \"seed\": seed})\n        quick = compute_metrics(gray, masks, env=\"estuarine\", fields=[\"beta_iso\", ...])\n        seq.append({\"seed\": seed, \"frames\": masks[\"sequence\"], \"metrics\": quick})\n    return seq\n```\n- Document slider order and stage descriptions for notebooks/demo scripts.",
        "testStrategy": "- Widget-level unit tests (can use ipywidgets `interaction` stubs) verifying slider config structure and that callbacks feed sanitized params into the generator.\n- Functional test running `preview_sequence(\"estuarine\", default_params, seeds=[42])` asserts the sequential frames list matches the expected stage order and quick metrics dictionary contains beta values.\n- Manual smoke test in notebook (recorded in QA checklist) verifying slider adjustments propagate to preview within <150 ms and quick metrics refresh accordingly.",
        "priority": "medium",
        "dependencies": [
          1,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend estuarine slider metadata in `build_sliders`",
            "description": "Add support in the interactive layer so `analog_image_generator.interactive.build_sliders` returns a complete set of slider and control descriptors when `env=\"estuarine\"` is requested.",
            "dependencies": [],
            "details": "Introduce an `env == \"estuarine\"` branch (or equivalent registry entry) inside `analog_image_generator.interactive.build_sliders` that constructs slider descriptor objects directly from `PARAM_METADATA` for all estuarine parameters, including label, range, default value, and tooltip text to prevent drift from the central metadata. Ensure this descriptor schema matches the existing pattern used for other environments so downstream widget stacks (ipywidgets and CLI) can consume them without changes. Add controls for an interlayer type dropdown (e.g., tide-, wave-, or mixed-dominant presets) and an RNG seed control, wiring their metadata from the same source. Validate that the resulting list/dict of descriptors is deterministic and ordered in a way that will later be documented for notebooks and demo scripts.",
            "status": "pending",
            "testStrategy": "Add unit tests that call `build_sliders(env=\"estuarine\")` and assert that: (a) all expected estuarine parameters appear as descriptor entries, (b) labels, ranges, defaults, and tooltips match the corresponding values in `PARAM_METADATA`, and (c) the interlayer type dropdown and seed control are present with the correct option sets and value types."
          },
          {
            "id": 2,
            "title": "Implement `preview_sequence` for estuarine with quick metrics",
            "description": "Create the `preview_sequence(env, params, seeds)` function that drives `generate_estuarine`, captures sequential frames, and computes lightweight quick metrics for use in interactive previews.",
            "dependencies": [],
            "details": "Implement `preview_sequence(env, params, seeds)` (or extend an existing variant) so that when `env == \"estuarine\"` it iterates over the provided `seeds`, merging the base `params` with each `seed` and calling `generate_estuarine` to obtain `(gray, masks_dict)`. From `masks_dict`, extract the sequential preview stack (e.g., `masks_dict[\"sequence\"]`) and build a list of per-seed records shaped like `{ \"seed\": seed, \"frames\": frames_sequence, \"metrics\": quick_metrics }`. For each seed, compute a lightweight subset of `stats.compute_metrics` (Phase 1 fields) such as β_iso and anisotropy ratio by calling `compute_metrics(gray, masks_dict, env=\"estuarine\", fields=[...])`, and store only those quick metrics with the sequence to keep the function responsive. Design the return structure so that both ipywidgets-based notebooks and CLI preview commands can consume it without additional adaptation.",
            "status": "pending",
            "testStrategy": "Write functional tests that call `preview_sequence(\"estuarine\", default_params, seeds=[42, 43])` using a small image size, then assert that: (a) the return value is a list with one entry per seed in order, (b) each entry contains `seed`, `frames`, and `metrics` keys, (c) `frames` is a non-empty sequence matching the stages emitted by `generate_estuarine` (e.g., same length as `masks_dict[\"sequence\"]`), and (d) `metrics` includes at least β_iso and anisotropy ratio fields populated with finite numeric values. Use stubs or fixtures for `generate_estuarine` and `compute_metrics` if needed to avoid heavy computations in tests."
          },
          {
            "id": 3,
            "title": "Wire estuarine sliders, debounced callbacks, and quick metrics preview",
            "description": "Connect estuarine slider changes to `preview_sequence` with debounced callbacks and expose the results to both the image preview and the quick metrics panel, ensuring UI responsiveness.",
            "dependencies": [],
            "details": "In the interactive layer, define the callback wiring that binds estuarine slider and control changes (including δ, φ, interlayer type, and RNG seed) to `preview_sequence(\"estuarine\", params, seeds)` for both ipywidgets notebooks and any CLI-based preview commands. Implement a throttling or debouncing mechanism so rapid slider movement (especially for δ and φ) batches updates and avoids blocking the UI; for ipywidgets this may involve timers or `observe` handlers that only trigger after a short idle interval. Ensure that each callback assembles sanitized parameter dictionaries from slider values, calls `preview_sequence`, and then updates the displayed preview frames and a small quick-metrics panel showing β_iso, anisotropy ratio, and any other Phase 1 metrics included in the sequence payload. Finally, document the slider order, control grouping, and stage descriptions in a place that notebooks and demo scripts can reference for consistent layout.",
            "status": "pending",
            "testStrategy": "Add widget-level tests (or thin integration tests) that simulate slider value changes using ipywidgets stubs or a minimal mock of the interactive stack, asserting that: (a) callbacks construct parameter dictionaries with the updated values and pass them into `preview_sequence(\"estuarine\", ...)`, (b) debouncing prevents multiple redundant calls when values are changed rapidly, and (c) the preview and quick metrics update functions receive data in the expected shape. For CLI mode, add a test that invokes the preview command with mocked sliders/params and verifies the quick metrics summary and frame count in the rendered output or returned structure."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Phase 1 metrics within `stats.compute_metrics`",
        "description": "Extend the statistics module to compute the baseline Phase 1 metrics (β_iso, β_dir*, anisotropy ratio, two-segment parameters) for estuarine outputs so quick metrics and reporting have consistent values.",
        "details": "- In `analog_image_generator.stats.compute_metrics`, add FFT/variogram utilities that take grayscale/masks and compute isotropic variograms `β_iso`, directional variograms at 0/45/90/135°, anisotropy ratio, and two-line fit parameters `(β₁, β₂, h₀)`.\n- Accept an `env` argument and branch for `\"estuarine\"` to include channel/bar masks in weighting.\n- Cache metrics inside the returned dict along with provenance (window size, lag step) for reproducibility.\n- Provide helper functions (e.g., `_variogram(field, theta_deg)`) to keep compute_metrics readable.\n- Pseudocode outline:\n```\ndef compute_metrics(gray, masks, env):\n    metrics = {}\n    beta_iso = variogram(gray)\n    beta_dirs = {theta: variogram(gray, theta) for theta in [0,45,90,135]}\n    metrics.update({...})\n    metrics.update(two_segment_fit(beta_iso))\n    return metrics\n```\n- Ensure metrics can run on downsampled frames for speed when called from quick metrics mode (Task 5).",
        "testStrategy": "- Unit tests using analytic fields (e.g., linear gradients, sinusoidal textures) where expected variogram/anisotropy values are known, verifying results within tolerance.\n- Regression tests to ensure compute time stays within limits (pytest mark for performance) and that repeated calls with identical inputs yield identical metrics.\n- Validate quick-metric subset agrees with the full compute by comparing truncated and full runs in tests.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement core variogram and anisotropy utilities for Phase 1 metrics",
            "description": "Create reusable FFT/variogram helpers to compute isotropic and directional variograms plus anisotropy ratios from grayscale estuarine frames.",
            "dependencies": [],
            "details": "Add internal helpers in the stats module (e.g., `_variogram(field, theta_deg=None, mask=None, max_lag_px=None, lag_step_px=1, use_fft=True)` and `_compute_anisotropy_ratio(beta_dirs)`) that take grayscale images and optional channel/bar masks and return lag distances with corresponding semivariance curves. Ensure support for isotropic variograms (theta_deg=None) and directional variograms at specified angles, correct handling of NaNs and masked pixels, and stable numerical behavior for small and large images. Document arguments and return types so `compute_metrics` can easily consume these utilities for Phase 1 metrics.",
            "status": "pending",
            "testStrategy": "Write unit tests using synthetic 2D fields (isotropic noise, linear gradients, and oriented sine waves) to verify that isotropic variograms are direction-invariant, directional variograms peak along the imposed orientation, and computed anisotropy ratios behave as expected within numeric tolerances."
          },
          {
            "id": 2,
            "title": "Extend `stats.compute_metrics` to compute Phase 1 variogram-based metrics for `env=\"estuarine\"`",
            "description": "Wire the new variogram utilities into `stats.compute_metrics` so β_iso, β_dir* and anisotropy ratio are computed for estuarine outputs using grayscale and masks.",
            "dependencies": [],
            "details": "Modify `analog_image_generator.stats.compute_metrics` to accept an `env` argument (if not already present) and add a dedicated branch for `env == \"estuarine\"` that calls the new variogram helpers on the grayscale frame, producing β_iso and directional variograms at 0, 45, 90, and 135 degrees. Combine channel and bar masks from the `masks` dict to weight variogram computations appropriately, then compute anisotropy ratios and collate all Phase 1 scalar metrics into the returned metrics dictionary using stable, well-documented key names. Preserve existing behavior for non-estuarine environments and keep the main function readable by delegating computation details to helpers.",
            "status": "pending",
            "testStrategy": "Add unit tests around `compute_metrics` that pass estuarine-like grayscale arrays and masks, asserting that the returned metrics dict includes β_iso, directional variograms, and anisotropy ratio keys with expected shapes and that β_iso[0] is approximately zero. Include tests to confirm that calling `compute_metrics` for other environments still works and that estuarine metrics remain deterministic for a fixed random seed."
          },
          {
            "id": 3,
            "title": "Add two-segment fit, provenance caching, and quick-metrics downsampling in `compute_metrics`",
            "description": "Fit two-segment parameters to β_iso, cache Phase 1 metric provenance in the metrics dict, and support an optional downsampled quick-metrics path.",
            "dependencies": [],
            "details": "Implement a helper such as `_two_segment_fit(beta_iso, lags)` that returns `(beta1, beta2, h0)` for the isotropic variogram, and call it from the estuarine branch of `compute_metrics`, storing the fitted parameters alongside the core Phase 1 metrics. Extend the metrics dictionary to cache provenance fields like window size, lag step, downsample factor (if applicable), and any key mask names used so results are reproducible. Add an optional argument or mode flag (e.g., `quick=True` or `downsample_factor`) that runs the Phase 1 computations on a downsampled version of the grayscale and masks for speed, while clearly recording this in the provenance fields so downstream quick-metrics and reporting can interpret the results consistently.",
            "status": "pending",
            "testStrategy": "Create tests that call `compute_metrics` with and without the quick/ downsample option on moderately sized synthetic estuarine frames, checking that the metrics dict always contains two-segment fit parameters and provenance fields (window size, lag step, and any downsample metadata) and that the structure is identical across modes. Optionally add a simple timing-based regression check to verify that the quick-metrics path is faster on larger arrays while still returning numerically reasonable metrics."
          }
        ]
      },
      {
        "id": 7,
        "title": "Add Phase 2 + estuarine-specific QC metrics and acceptance flags",
        "description": "Augment `compute_metrics` (or helper routines) with entropy, D/SFI, PSD anisotropy, topology per facies, and the estuarine-specific acceptance checks defined in the PRD.",
        "details": "- Implement PSD-based anisotropy (aspect ratio and orientation θ) via 2D FFT of the bar/channel masks; ensure δ→0 cases yield aspect 1.3–1.8 and δ→1 exceed 2.0, recording deviations as QC flags.\n- Compute Shannon entropy H, D/SFI, and per-facies topology stats (connected-component counts, Euler number) leveraging `scipy.ndimage` labelers.\n- Add specialized metrics: ebb/flood orientation separation (difference between tide helper peaks), mouth-bar aspect ratio distribution (should fall within target ranges), shoreline curvature std dev, interlayer thickness histograms compared with required proportions, and LU_T / LU_F thickness + porosity proxies (derive using mask thickness fields + parameterized thickness distributions respecting Jiwei et al. 2025 values).\n- Emit QC booleans and messages (e.g., `qc_psd_aspect_pass`, `qc_mouth_bar_ratio_pass`, `qc_interlayer_distribution_pass`) into the metrics dict so reporting can highlight rerun requirements.\n- Pseudocode:\n```\nmetrics[\"ebb_flood_delta_theta\"] = abs(theta_ebb - theta_flood)\nmetrics[\"qc_ebb_flood\"] = metrics[\"ebb_flood_delta_theta\"] >= 25\n...\n```\n- Ensure calculations reuse metadata captured in Tasks 2–4 to avoid recomputation.",
        "testStrategy": "- Synthetic tests constructing known masks (e.g., ellipses with specified aspect ratios) verifying QC flags toggle at the documented thresholds.\n- Add randomized property-based tests to ensure mouth-bar histograms remain within expected bounds for δ extremes.\n- Write regression tests comparing computed PSD aspect vs dominance slider for δ in {0, 0.5, 1} to guarantee monotonic behavior.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Phase 2 estuarine metric calculations in compute_metrics",
            "description": "Extend the stats pipeline to compute Phase 2 metrics (entropy, D/SFI, PSD anisotropy, and per-facies topology) for estuarine environments within compute_metrics or dedicated helpers.",
            "dependencies": [],
            "details": "Augment analog_image_generator.stats.compute_metrics (or a dedicated helper module) to compute Shannon entropy H, D/SFI, PSD-based anisotropy (aspect ratio and orientation θ) via 2D FFT on bar/channel masks, and per-facies topology statistics such as connected-component counts and Euler number using scipy.ndimage labelers. Ensure the implementation accepts an estuarine env branch, reuses Phase 1 FFT/variogram utilities and any metadata recorded by Tasks 2–4 to avoid recomputation, and returns all new scalar metrics in the metrics dict with clear, stable keys suitable for downstream reporting.",
            "status": "pending",
            "testStrategy": "Add unit tests that feed synthetic masks with known structure (e.g., ellipses of controlled aspect ratio, simple checkerboards, and uniform vs highly heterogeneous fields) into compute_metrics and assert that PSD anisotropy aspect ratios match expected ranges for δ→0 and δ→1 cases, entropy and D/SFI behave monotonically across increasing disorder, and topology stats (component counts, Euler number) match analytically derived expectations."
          },
          {
            "id": 2,
            "title": "Add estuarine-specific QC metrics and acceptance flags",
            "description": "Implement the estuarine-specific QC and acceptance checks, wiring their thresholds to the PRD and exposing them as boolean flags and messages in the metrics dict.",
            "dependencies": [],
            "details": "Using the Phase 2 metrics and existing estuarine metadata, implement estuarine-specific QC checks including ebb/flood orientation separation (Δθ between tide helper peaks), mouth-bar aspect ratio distribution, shoreline curvature standard deviation, interlayer thickness histograms, and LU_T / LU_F thickness and porosity proxy metrics derived from mask thickness fields and Jiwei et al. 2025 thickness distributions. For each check, encode PRD-specified thresholds and write boolean flags (e.g., qc_psd_aspect_pass, qc_mouth_bar_ratio_pass, qc_interlayer_distribution_pass, qc_ebb_flood) plus short human-readable messages into the metrics dict so downstream reporting can highlight rerun requirements.",
            "status": "pending",
            "testStrategy": "Create focused unit tests that construct synthetic estuarine scenarios where ebb/flood channel orientations differ by controlled angles, mouth-bar aspect ratios fall inside or outside the target ranges, shoreline curvature is artificially smoothed or roughened, and interlayer thickness distributions are tuned to match or violate PRD proportions, then assert that each qc_* flag flips state at the documented thresholds and that the associated metrics (e.g., ebb_flood_delta_theta) are numerically correct."
          },
          {
            "id": 3,
            "title": "Integrate Phase 2 metrics, QC outputs, and metadata reuse into estuarine workflow",
            "description": "Wire the new Phase 2 metrics and QC flags into the estuarine stats workflow, ensuring consistent dict schema, metadata reuse, and minimal recomputation.",
            "dependencies": [],
            "details": "Refactor compute_metrics and any estuarine stats helpers so Phase 2 metrics and QC flags are computed only once per run, consuming metadata and intermediate arrays produced by Tasks 2–4 (e.g., orientation histograms, shoreline traces, thickness fields) instead of recomputing masks or FFTs. Normalize metric and flag naming, update the metrics dict schema to include both raw metrics and qc_* booleans plus explanatory messages, and ensure the estuarine branch remains compatible with Phase 1 outputs so reporting and Task 9 documentation can treat Phase 1 and Phase 2 metrics uniformly.",
            "status": "pending",
            "testStrategy": "Implement integration-style tests that call compute_metrics on representative estuarine outputs with precomputed metadata stubs or fixtures, verifying that the returned metrics dict contains all Phase 2 values and qc_* flags with the expected keys, that repeated calls reuse cached metadata rather than recomputing heavy operations (validated via lightweight timing or mock counters), and that the extended schema remains backward compatible with existing Phase 1 tests and consumers."
          }
        ]
      },
      {
        "id": 8,
        "title": "Extend reporting (CSV + PDFs + master summary) for estuarine metrics",
        "description": "Upgrade `analog_image_generator.reporting.build_reports` to emit CSV rows, per-style PDFs, and the master PDF/legend bundle that include the new estuarine-specific metrics and QC flags.",
        "details": "- Expand CSV schema to cover Phase 1/2 metrics plus estuarine-specific values (orientation split, mouth-bar ratios, shoreline curvature std, PSD aspect, interlayer stats) with stable column ordering; document schema in README/docs.\n- Generate per-style PDF reports (tide, wave, mixed) using ReportLab: include slider inputs, representative thumbnails from sequential frames, QC flag badges, and legends referencing `docs/PALETTES.md`.\n- Assemble a master PDF that summarises all runs per environment with tables and aggregated charts (e.g., histogram overlays) plus a cover page referencing sources.\n- Ensure PDF generation respects WSL pathing and writes to `dist/reports/<timestamp>`; support CLI invocation by returning output paths.\n- Update `scripts/smoke_test.py` to call `build_reports` with sample metrics_rows produced by generator/stats to verify the pipeline end-to-end.\n- Pseudocode:\n```\ndef build_reports(metrics_rows, output_dir):\n    csv_path = output_dir / \"estuarine_metrics.csv\"\n    pd.DataFrame(metrics_rows).to_csv(...)\n    for style, rows in groupby_tag(metrics_rows):\n        pdf = ReportLabCanvas(...)\n        ... draw legend, thumbnails, QC table ...\n    merge_pdfs(per_style, master_path)\n```\n",
        "testStrategy": "- Unit tests verifying CSV headers match the documented schema and QC flags serialize as booleans/strings.\n- Regression tests that generate sample PDFs then inspect metadata (page count, presence of legend text) using PyPDF2; compare file hashes to golden fixtures when feasible.\n- Smoke test ensuring `build_reports` handles empty metrics list gracefully and raises informative errors when thumbnails missing.",
        "priority": "medium",
        "dependencies": [
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend CSV schema and row construction for estuarine metrics",
            "description": "Design and implement an explicit CSV schema in `analog_image_generator.reporting.build_reports` that includes Phase 1/2 metrics plus all estuarine-specific fields, then ensure `metrics_rows` are serialized into stable, well-typed columns and documented in the reporting docs.",
            "dependencies": [],
            "details": "Inspect the structure of `metrics_rows` produced for the estuarine environment (including Phase 1/2 metrics and estuarine-specific values such as orientation split, mouth-bar ratios, shoreline curvature statistics, PSD aspect, interlayer stats, and QC flags). Define an ordered list of CSV columns and any required type coercions so that `build_reports` can construct a `pandas.DataFrame` with a stable column order and clear semantics. Implement CSV writing to `dist/reports/<timestamp>/estuarine_metrics.csv`, ensuring paths are WSL-safe and directories are created as needed. Update README/docs to describe the schema, including column names, units (where applicable), and how QC flags are represented, keeping the schema stable for downstream consumers.",
            "status": "pending",
            "testStrategy": "Add unit tests that call `build_reports` with a small synthetic estuarine `metrics_rows` list, then load the emitted CSV and assert: (1) headers exactly match the documented schema and ordering, (2) estuarine-specific fields and QC flags appear with the expected dtypes/encodings, and (3) no extra or missing columns are present."
          },
          {
            "id": 2,
            "title": "Implement per-style estuarine PDF report generation (tide, wave, mixed)",
            "description": "Group estuarine `metrics_rows` by style and generate per-style PDF reports that include slider inputs, representative thumbnails, QC flag badges, and legends consistent with `docs/PALETTES.md`.",
            "dependencies": [],
            "details": "Within `analog_image_generator.reporting.build_reports`, group estuarine `metrics_rows` by style tag (e.g., tide, wave, mixed) and for each group create a ReportLab canvas to build a per-style PDF report. Implement layout sections for: high-level run metadata, slider input summaries, tables of key metrics (including estuarine-specific ones), and QC flag badges or icons derived from the metrics. Wire in representative thumbnails from sequential preview frames (paths provided by the generation pipeline), arranging them in a simple grid or strip. Add a legend section that pulls color and label information aligned with `docs/PALETTES.md`, so facies and masks are consistently described. Ensure PDFs are saved under `dist/reports/<timestamp>/style-<name>.pdf` with WSL-safe paths and collected for later merging.",
            "status": "pending",
            "testStrategy": "Create a small in-memory estuarine `metrics_rows` sample covering tide, wave, and mixed styles, then run `build_reports` to generate PDFs into a temporary output directory. Use a PDF inspection library (e.g., PyPDF2) in tests to assert that: (1) each expected per-style PDF exists, (2) each has at least one page, and (3) key legend or header text (such as the style name) is present in the document text layer."
          },
          {
            "id": 3,
            "title": "Assemble master estuarine summary PDF and integrate CLI/smoke test behavior",
            "description": "Create a master estuarine summary PDF that aggregates per-style reports into a single document with a cover page, tables, and charts, ensure `build_reports` returns output paths for CLI use, and update `scripts/smoke_test.py` to exercise the full reporting pipeline on WSL.",
            "dependencies": [],
            "details": "Implement logic in `analog_image_generator.reporting.build_reports` to collect all per-style estuarine PDFs and merge them into a master summary PDF, using either ReportLab or a PDF-merging utility. Add a cover page that names the estuarine environment, briefly describes metric sources, and references palettes and geologic rules. Include summary tables (e.g., one row per run with key metrics and QC flags) and, where feasible, simple aggregated charts such as histogram overlays or small multiples of key metrics. Ensure all report files are written under `dist/reports/<timestamp>` with deterministic filenames and that `build_reports` returns a structured object (e.g., dict) of CSV and PDF paths suitable for CLI or notebook invocation. Finally, update `scripts/smoke_test.py` to construct a representative estuarine `metrics_rows` sample, call `build_reports`, and assert that the CSV, per-style PDFs, and master PDF are created without error on WSL.",
            "status": "pending",
            "testStrategy": "Add an end-to-end smoke-style test that invokes `build_reports` with a small synthetic estuarine `metrics_rows` payload and a temporary output directory, then asserts that: (1) the returned path structure includes the CSV, each per-style PDF, and the master summary PDF, (2) all files exist on disk and are non-empty, and (3) the updated `scripts/smoke_test.py` runs successfully within CI. Optionally, perform lightweight PDF inspections (page counts, presence of the cover page title) to guard against regressions in the merged master document."
          }
        ]
      },
      {
        "id": 9,
        "title": "Document estuarine rules, anchors, and workflow updates",
        "description": "Update documentation and notebook anchors so every new estuarine principle/function is traceable per AGENTS.md guidelines.",
        "details": "- Extend `docs/GEOLOGIC_RULES.md` with entries for the new helper functions (`tide_channel_network`, `wave_shoreface_bars`, `generate_estuarine`, etc.) linking to notebook anchors such as `notebooks/estuarine.ipynb#anchor-estuarine-tide-channels`.\n- Create or update `notebooks/estuarine.ipynb` markdown cells to include anchors describing each implemented principle and slider, referencing Jiwei et al. 2025 figures.\n- Update `.taskmaster/docs/prd_estuarine.txt` and README/WORKFLOW sections if parameter naming or acceptance logic changed during implementation, keeping the tables in sync.\n- Document the new CSV schema and PDF outputs in `docs/WORKFLOW.md` or a dedicated reporting section to help QA/REP roles.\n- Summarize changes in `docs/MEETING_RECAP_*.md` or changelog entries referencing the acceptance criteria being satisfied.",
        "testStrategy": "- Manual doc review checklist ensuring every code anchor appears both in GEOLOGIC_RULES and the notebook anchor column (per AGENTS instructions).\n- Automated Markdown tests (e.g., custom pytest that parses GEOLOGIC_RULES) verifying anchors reference valid notebook IDs and function names exist in code.\n- Spell-check/markdown-lint passes via pre-commit to catch link or formatting regressions.",
        "priority": "medium",
        "dependencies": [
          4,
          7,
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend GEOLOGIC_RULES for estuarine helper functions and anchors",
            "description": "Update docs/GEOLOGIC_RULES.md so all new estuarine helper functions and principles are documented and mapped to code and notebook anchors.",
            "dependencies": [
              4,
              7,
              8
            ],
            "details": "Identify all estuarine-related functions (e.g., analog_image_generator.geologic_generators.tide_channel_network, wave_shoreface_bars, generate_estuarine, and related helpers) and add rows for each in docs/GEOLOGIC_RULES.md. For every row, fill in the geologic principle, key parameters, acceptance notes, and a code anchor using the fully qualified function name plus a notebook anchor pointing into notebooks/estuarine.ipynb (e.g., #anchor-estuarine-tide-channels). Ensure naming follows the anchor-<env>-<principle> convention and that existing fluvial/other entries remain unchanged while the estuarine block is clearly separated and complete.",
            "status": "pending",
            "testStrategy": "Run a manual checklist verifying each estuarine function in the codebase appears in docs/GEOLOGIC_RULES.md with both a code anchor and a notebook anchor column entry, and spot-check a few anchors by opening notebooks/estuarine.ipynb to confirm the IDs match exactly."
          },
          {
            "id": 2,
            "title": "Create and align estuarine notebook anchors for principles and sliders",
            "description": "Author or revise markdown cells in notebooks/estuarine.ipynb so every estuarine principle, generator, and slider is documented and anchored.",
            "dependencies": [
              4,
              7,
              8
            ],
            "details": "Open notebooks/estuarine.ipynb and add or update markdown cells that describe each implemented estuarine principle (tide channels, wave shoreface bars, mixed-energy behavior, stacked packages if present) and each user-facing slider. For every principle, create an anchor ID like anchor-estuarine-tide-channels that matches the entries planned in docs/GEOLOGIC_RULES.md, and reference the appropriate Jiwei et al. 2025 figures or other literature where applicable. Ensure that each slider description points back to the corresponding parameter name and that anchors are unique, consistently formatted, and easy to scan during reviews.",
            "status": "pending",
            "testStrategy": "Manually render the notebook and verify that each anchor ID listed in docs/GEOLOGIC_RULES.md is present as a heading or markdown anchor in notebooks/estuarine.ipynb, confirm links like notebooks/estuarine.ipynb#anchor-estuarine-tide-channels navigate correctly, and quickly review text for completeness and consistency with parameter names and Jiwei et al. 2025 references."
          },
          {
            "id": 3,
            "title": "Update estuarine PRD, workflow, and reporting docs for new schema and outputs",
            "description": "Synchronize PRD, workflow, and reporting documentation for estuarine parameters, CSV schema, and PDF outputs with the latest implementation.",
            "dependencies": [
              4,
              7,
              8
            ],
            "details": "Review the current estuarine implementation (parameters, acceptance logic, CSV fields, and PDF figures) and update .taskmaster/docs/prd_estuarine.txt so parameter names, ranges, defaults, and acceptance criteria match the code and slider metadata. Edit docs/WORKFLOW.md (or the dedicated reporting section) to describe the estuarine CSV schema, key columns, PDF layouts, and any QC flags or stacked-package metadata that QA/REP rely on. Finally, add a concise summary of these estuarine documentation and workflow updates to the appropriate docs/MEETING_RECAP_*.md or changelog entry, explicitly calling out which acceptance criteria from AGENTS.md are now satisfied.",
            "status": "pending",
            "testStrategy": "Compare parameter tables and acceptance logic in .taskmaster/docs/prd_estuarine.txt and docs/WORKFLOW.md against the current code and CSV/PDF outputs for a representative estuarine run, verifying that field names, ranges, and semantics match; then have a second-pass manual doc review focused on QA/REP checklists to ensure all required schema and workflow notes are present."
          }
        ]
      },
      {
        "id": 10,
        "title": "Add automated tests & smoke coverage for estuarine pipeline",
        "description": "Create comprehensive pytest suites and smoke scripts that cover generator outputs, slider plumbing, metrics, and reporting to prevent regressions before CI/Task Master handoff.",
        "details": "- Add new test modules (`tests/test_estuarine_generator.py`, `tests/test_estuarine_metrics.py`, `tests/test_reporting_estuarine.py`) exercising default, tide-heavy, and wave-heavy scenarios; assert RNG reproducibility, mask coverage, QC flags, and reporting artifacts.\n- Expand `scripts/smoke_test.py` to run a mini pipeline: generate two realizations, compute metrics, and build CSV/PDF outputs, printing summary statuses for QA.\n- Ensure tests run quickly by downsampling arrays where possible and mocking heavy PDF generation in unit tests (while leaving one integration test for real file I/O using tmp_path).\n- Wire the new tests into CI (pytest collection picks up automatically) and document how to run them locally (`pytest tests/test_estuarine_*`).\n- Collect coverage data for generator/interactive/stats/reporting to ensure critical branches are exercised; update QA checklist accordingly.",
        "testStrategy": "- Pytest suites verifying deterministic outputs, slider validation, QC flag boundaries, reporting file creation, and smoke pipeline success.\n- Use hypothesis/property tests for parameter clamping edge cases.\n- Add CI badge/update if coverage thresholds met; ensure `pre-commit run --all-files` stays clean after new notebooks/docstrings are included.",
        "priority": "medium",
        "dependencies": [
          4,
          5,
          7,
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create estuarine pytest modules for generators, metrics, and reporting",
            "description": "Add dedicated pytest modules for the estuarine pipeline covering generator outputs, metrics, and reporting artifacts across key estuarine regimes.",
            "dependencies": [],
            "details": "Create `tests/test_estuarine_generator.py`, `tests/test_estuarine_metrics.py`, and `tests/test_reporting_estuarine.py`. In the generator tests, exercise at least default, tide-dominated, and wave-dominated presets using fixed RNG seeds, asserting deterministic gray output, stable masks, and expected QC flags. In the metrics tests, verify that core estuarine metrics (e.g., bar geometries, shoreline orientation statistics, mudflat coverage) are computed without errors and obey PRD-defined bounds. In the reporting tests, verify that estuarine runs emit expected CSV rows and minimal PDF artifacts (mocking heavy rendering where appropriate) and that any per-environment legends, palette mappings, and QC flags appear in the outputs.",
            "status": "pending",
            "testStrategy": "Run `pytest tests/test_estuarine_generator.py tests/test_estuarine_metrics.py tests/test_reporting_estuarine.py` and confirm all tests pass, focusing on RNG reproducibility (same seed yields identical arrays), metric value ranges matching PRD limits, and presence of expected CSV/PDF files or mocked artifacts in a temporary directory."
          },
          {
            "id": 2,
            "title": "Extend smoke_test.py to cover estuarine mini-pipeline",
            "description": "Expand the smoke testing script to run an end-to-end estuarine pipeline producing metrics and reporting artifacts for QA.",
            "dependencies": [],
            "details": "Modify `scripts/smoke_test.py` to add an estuarine-focused smoke path that generates at least two realizations (e.g., tide-heavy and wave-heavy), computes metrics, and writes CSV and PDF outputs into a temporary or configured output folder. Ensure the script prints concise summary lines for each realization, including generator success, metric summary, and reporting status, plus a final overall PASS/FAIL code for CI. Where possible, downsample arrays and reuse existing helpers so the smoke path runs within seconds on a laptop while still exercising generator, metrics, and reporting stacks end to end.",
            "status": "pending",
            "testStrategy": "Manually invoke `python scripts/smoke_test.py --env estuarine` (or equivalent flag) and verify it exits with code 0 on success, prints clear status lines, and produces expected CSV/PDF outputs. Optionally wrap the script in a lightweight pytest (marked as smoke) that calls the main entry with `tmp_path` to ensure the command succeeds and all expected files are created."
          },
          {
            "id": 3,
            "title": "Optimize estuarine tests for speed, CI wiring, and coverage tracking",
            "description": "Ensure estuarine tests are fast, wired into CI, and contribute to coverage and QA checklists without adding excessive runtime.",
            "dependencies": [],
            "details": "Review newly added estuarine tests to downsample large arrays and use fixtures or parametrization to minimize redundant work, and mock heavy PDF generation except for a single integration-style test that writes real files via `tmp_path`. Confirm that pytest auto-discovers the new `tests/test_estuarine_*.py` modules and that the CI configuration (e.g., GitHub Actions or local pipeline) runs `pytest tests/test_estuarine_*` as part of the standard test job. Enable or update coverage collection so generator, interactive/slider plumbing, stats, and reporting modules used by the estuarine pipeline are included, and update the QA checklist and documentation to reference the new estuarine coverage and smoke steps.",
            "status": "pending",
            "testStrategy": "Run the full CI-equivalent command locally (for example `pytest --maxfail=1 --disable-warnings --cov=analog_image_generator tests/test_estuarine_*`) and verify the runtime stays within acceptable limits while coverage reports show meaningful lines executed in generator, slider plumbing, metrics, and reporting code. Confirm the CI pipeline configuration includes these tests and that the QA checklist references both the smoke script and the coverage targets for the estuarine pipeline."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-11-16T17:37:36.511Z",
      "updated": "2025-11-16T17:37:36.512Z",
      "description": "Tasks for estuarine-v1 context"
    }
  },
  "fluvial-v1-demo": {
    "tasks": [
      {
        "id": 1,
        "title": "Align fluvial demo notebook skeletons with GEOLOGIC_RULES anchors",
        "description": "Audit existing fluvial notebooks and craft a repeatable intro/checklist cell pattern that ties every section back to the documented anchors.",
        "details": "- Inventory notebooks under `notebooks/fluvial_*` plus `fluvial_sanity.ipynb` and map each required section (intro, presets, generator runs, metrics, reporting, debug) to anchor IDs defined in `docs/GEOLOGIC_RULES.md`.\n- Create/refresh intro markdown blocks per environment (meandering, braided, anastomosing, stacked, reporting, stats) referencing anchors like ``analog_image_generator.geologic_generators.generate_meandering`` and `anchor-fluvial-overview` in `docs/GEOLOGIC_RULES.md`.\n- Add a standardized checklist markdown cell (✅/⬜) enumerating PRD bullets so QA can tick each feature before sharing with the professor.\n- Pseudo-code:\n```\nfor env in (\"meandering\",\"braided\",\"anastomosing\",\"stacked\",\"reporting\",\"stats\"):\n    anchor_id = f\"anchor-{env}-overview\"\n    write_markdown_cell(f\"### {env.title()} demo\\nAnchors: GEOLOGIC_RULES#{anchor_id}\")\n```\n- Ensure the new anchors are backfilled into `docs/GEOLOGIC_RULES.md` and cross-linked from notebook markdown cells.",
        "testStrategy": "Run `python scripts/validate_geo_anchors.py` to confirm notebook anchors map to GEOLOGIC_RULES entries, then open each notebook in nbconvert preview to verify markdown renders and the checklist references all PRD bullets.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Inventory fluvial notebooks and anchor requirements",
            "description": "List every `notebooks/fluvial_*.ipynb` file plus `notebooks/fluvial_sanity.ipynb`, documenting required sections (intro, presets, generator runs, metrics, reporting, debug).",
            "dependencies": [],
            "details": "Open each notebook in Jupyter or nbconvert, note section order, existing markdown anchors, and any missing intro/checklist cells so later edits cover meandering, braided, anastomosing, stacked, reporting, and stats demos.",
            "status": "done",
            "testStrategy": "Use `nbconvert --to notebook --execute` in dry-run mode to ensure notebooks load without execution errors during inventory.",
            "updatedAt": "2025-11-20T18:29:42.203Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Map notebook sections to GEOLOGIC_RULES anchors",
            "description": "Cross-reference the inventoried sections with `docs/GEOLOGIC_RULES.md`, capturing exact anchor IDs for each environment and section.",
            "dependencies": [
              1
            ],
            "details": "Build a mapping table linking intro/preset/run/metrics/reporting/debug cells to anchors such as `analog_image_generator.geologic_generators.generate_meandering` and `anchor-fluvial-overview`, noting any missing anchors needing backfill.",
            "status": "done",
            "testStrategy": "Review `docs/GEOLOGIC_RULES.md` after mapping to confirm every referenced anchor exists and follows `anchor-<env>-<principle>` naming.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:30:56.128Z"
          },
          {
            "id": 3,
            "title": "Refresh intro and checklist markdown cells",
            "description": "Insert or update markdown cells per notebook so each environment’s intro and checklist reference the mapped anchors and PRD bullets.",
            "dependencies": [
              2
            ],
            "details": "Implement the standardized pattern (`### {Env} demo`, anchor references, ✅/⬜ checklist) ensuring markdown cells include proper metadata IDs like `anchor-meandering-overview` and mention Task Master tags where required.",
            "status": "done",
            "testStrategy": "Render the modified cells via nbconvert preview to confirm headings, anchor references, and checkbox formatting appear correctly.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:30:56.141Z"
          },
          {
            "id": 4,
            "title": "Backfill GEOLOGIC_RULES entries for new anchors",
            "description": "Update `docs/GEOLOGIC_RULES.md` with any missing anchor descriptions introduced in notebooks, keeping anchors synchronized with code references.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add or edit GEOLOGIC_RULES rows so every notebook anchor points to a documented principle, referencing fully qualified functions (`analog_image_generator.geologic_generators.*`) and ensuring notebook metadata IDs match the documentation.",
            "status": "done",
            "testStrategy": "Manually verify each notebook anchor string exists in `docs/GEOLOGIC_RULES.md` by searching with `rg` or a custom validation snippet.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:34:22.488Z"
          },
          {
            "id": 5,
            "title": "Run anchor validation and notebook previews",
            "description": "Execute repository validation commands to confirm anchors and markdown render correctly after edits.",
            "dependencies": [
              3,
              4
            ],
            "details": "Run `python scripts/validate_geo_anchors.py` and nbconvert previews for all updated notebooks, capturing logs/screenshots for QA and ensuring no regressions in anchor coverage.",
            "status": "done",
            "testStrategy": "Record output from `python scripts/validate_geo_anchors.py` plus each nbconvert run; all should exit successfully without warnings.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:34:22.499Z"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Inventory `notebooks/fluvial_*.ipynb` plus `notebooks/fluvial_sanity.ipynb`, map each required section (intro, presets, generator runs, metrics, reporting, debug) to anchors already listed in `docs/GEOLOGIC_RULES.md`, then refresh the intro/checklist markdown cells per environment (meandering, braided, anastomosing, stacked, reporting, stats) so they reference the exact anchor IDs. Update or add any missing entries back into `docs/GEOLOGIC_RULES.md`, ensure notebook cell metadata ids follow `anchor-<env>-<principle>`, and finish by running `python scripts/validate_geo_anchors.py` plus nbconvert previews to confirm the new pattern renders properly.",
        "updatedAt": "2025-11-20T18:34:22.499Z"
      },
      {
        "id": 2,
        "title": "Add parameter preset cells referencing generator defaults",
        "description": "Expose baseline/extreme parameter presets for each fluvial environment so demos can reproduce single-belt and stacked behaviors on demand.",
        "details": "- Pull defaults from `analog_image_generator.geologic_generators` (`_MEANDER_DEFAULTS`, `_BRAIDED_DEFAULTS`, `_ANASTO_DEFAULTS`) and stacked controls from `stacked_channels.build_stacked_fluvial`.\n- For each notebook, add code cells defining dictionaries such as `baseline_params`, `extreme_scroll_params`, `stacked_package_mix`, each with inline comments referencing GEOLOGIC_RULES anchors.\n- Include Task Master tag metadata (e.g., `params[\"task_master_tag\"] = \"fluvial-v1\"`) when preparing reporting hooks.\n- Pseudo-code:\n```\nfrom analog_image_generator import geologic_generators as gg\nbaseline_meander = {**gg._MEANDER_DEFAULTS, \"seed\": 42}\nextreme_meander = {**baseline_meander, \"drift_fraction\": 0.18, \"mode\": \"single\"}\nstacked_presets = {\n    \"mode\": \"stacked\",\n    \"package_count\": 3,\n    \"package_styles\": [\"meandering\",\"braided\",\"anastomosing\"],\n    \"package_relief_px\": [18, 12, 24],\n}\n```\n- Embed markdown explaining when to use each preset and how it maps to the PRD scenarios.",
        "testStrategy": "Execute preset cells inside each notebook to ensure dictionaries render without NameErrors, then leverage lightweight smoke snippets (`analog_image_generator.preview.run_param_batch`) to verify the presets generate outputs before wiring visualization cells.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit fluvial notebooks for preset entry points",
            "description": "Review each fluvial notebook to locate or create preset sections and note required GEOLOGIC_RULES anchors.",
            "dependencies": [],
            "details": "Inventory notebooks under `notebooks/fluvial_*` plus sanity/demo files, confirm they import `analog_image_generator.geologic_generators as gg`, and record where baseline/extreme/stacked presets and markdown guidance must reside so later edits stay aligned with GEOLOGIC_RULES anchors.",
            "status": "done",
            "testStrategy": "Open notebooks in Jupyter/Lab, ensure preset sections exist or add empty cells without execution errors.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:38:18.661Z"
          },
          {
            "id": 2,
            "title": "Implement baseline and extreme single-belt preset cells",
            "description": "Clone generator defaults into preset dictionaries with metadata for single-belt demos.",
            "dependencies": [
              1
            ],
            "details": "For each notebook, add code cells that copy `_MEANDER_DEFAULTS`, `_BRAIDED_DEFAULTS`, `_ANASTO_DEFAULTS`, override seeds/mode/drift parameters, inject `params['task_master_tag'] = 'fluvial-v1'`, and pair markdown describing baseline vs extreme usage while citing appropriate GEOLOGIC_RULES anchors.",
            "status": "done",
            "testStrategy": "Run the preset-defining cells in each notebook to ensure dictionaries build without NameErrors and display expected keys.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:38:18.673Z"
          },
          {
            "id": 3,
            "title": "Define stacked package mix preset dictionaries",
            "description": "Introduce stacked-mode dictionaries referencing stacked channel controls and metadata.",
            "dependencies": [
              1
            ],
            "details": "Import `analog_image_generator.stacked_channels` where required, clone controls from `build_stacked_fluvial`, set `mode='stacked'`, package counts/styles/relief arrays, embed mix-specific markdown guidance, and note Task Master tag metadata for reporting hooks.",
            "status": "done",
            "testStrategy": "Execute stacked preset cells ensuring `stacked_channels` imports resolve and resulting dicts contain package-related keys and metadata.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:38:18.684Z"
          },
          {
            "id": 4,
            "title": "Validate preset execution and usage documentation",
            "description": "Run smoke executions and finalize markdown instructions for all presets.",
            "dependencies": [
              2,
              3
            ],
            "details": "Execute every preset cell end-to-end (baseline, extreme, stacked) within notebooks, fix any missing imports, add markdown explaining when each preset should be run (single vs stacked) tied to PRD scenarios, and save notebooks after confirming clean runs.",
            "status": "done",
            "testStrategy": "Use lightweight smoke snippets such as `analog_image_generator.preview.run_param_batch` (or equivalent) to confirm presets generate outputs and visually inspect quick plots/logs for regressions.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:38:18.696Z"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 4,
        "expansionPrompt": "For each fluvial notebook import `analog_image_generator.geologic_generators as gg` and, where needed, `analog_image_generator.stacked_channels`, then author preset dictionaries (baseline, extreme, stacked mixes) by cloning `_MEANDER_DEFAULTS`, `_BRAIDED_DEFAULTS`, `_ANASTO_DEFAULTS`, and the stacked controls defined near `build_stacked_fluvial`. Add metadata like `task_master_tag = \"fluvial-v1\"`, include markdown describing when to use each preset (single vs stacked), and verify the preset cells execute without NameErrors before saving the notebooks.",
        "updatedAt": "2025-11-20T18:38:18.696Z"
      },
      {
        "id": 3,
        "title": "Implement generator run cells for single and stacked fluvial belts",
        "description": "Wire notebook cells that call the existing generator and stacked channel builders to produce gray images and masks for each preset.",
        "details": "- Add helper cell per notebook that imports `analog_image_generator.geologic_generators.generate_fluvial` and `stacked_channels.build_stacked_fluvial`.\n- Write callable utility `run_fluvial_case(params)` returning `(gray, masks)` to avoid duplication; detect `params[\"mode\"] == \"stacked\"` to route to `build_stacked_fluvial`.\n- Capture masks (`channel`, `levee`, `scroll_bar`, `upper_surface_mask`, `package_id_map`, etc.) for downstream visualization.\n- Persist quick artifacts under `outputs/notebooks/<env>/seed_{seed}/` for later reporting hooks.\n- Pseudo-code:\n```\nfrom analog_image_generator import geologic_generators as gg\nfrom analog_image_generator import stacked_channels\n\ndef run_fluvial_case(params):\n    if params.get(\"mode\") == \"stacked\":\n        return stacked_channels.build_stacked_fluvial(params)\n    return gg.generate_fluvial(params)\n\ngray, masks = run_fluvial_case(baseline_meander)\n```\n- Include markdown callouts describing the expected `gray`/`masks` tuple per PRD.",
        "testStrategy": "Execute each generator cell with deterministic seeds, confirm shapes match `params[\"height\"], params[\"width\"]`, and visually inspect quick inline plots to ensure single vs stacked parity before continuing.",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create reusable `run_fluvial_case` helper",
            "description": "Add a notebook helper cell that imports existing generator modules and routes params to the stacked builder when needed.",
            "dependencies": [],
            "details": "Define `run_fluvial_case(params)` that imports `analog_image_generator.geologic_generators.generate_fluvial` and `analog_image_generator.stacked_channels.build_stacked_fluvial`, checks `params.get(\"mode\")`, and returns `(gray, masks)` for both pathways.",
            "status": "done",
            "testStrategy": "Run helper with representative single and stacked params to confirm it returns the tuple without errors.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:44:14.254Z"
          },
          {
            "id": 2,
            "title": "Wire helper into each fluvial notebook preset block",
            "description": "Insert execution cells after Task 2 presets so every baseline/extreme dictionary invokes the helper with deterministic seeds.",
            "dependencies": [
              1
            ],
            "details": "Iterate through Task 2 preset dictionaries (meander, braided, anastomosing, stacked) and call `run_fluvial_case` for each, ensuring seeds are set and outputs stored in variables for downstream visualization.",
            "status": "done",
            "testStrategy": "Execute notebooks up to the new cells and verify each preset run completes using the helper.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:44:14.267Z"
          },
          {
            "id": 3,
            "title": "Persist artifacts under `outputs/notebooks/<env>/seed_{seed}`",
            "description": "Write filesystem routines that save gray arrays and mask dictionaries for each run into the specified directory layout.",
            "dependencies": [
              2
            ],
            "details": "After each helper call, create `outputs/notebooks/<env>/seed_{seed}/` and serialize gray images plus masks (`channel`, `levee`, `scroll_bar`, `upper_surface_mask`, `package_id_map`, etc.) so later reporting hooks can load them.",
            "status": "done",
            "testStrategy": "Inspect output directories after notebook runs to confirm files exist and contain expected dimensions.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:44:14.278Z"
          },
          {
            "id": 4,
            "title": "Document expected `(gray, masks)` tuples with PRD anchors",
            "description": "Add nearby markdown cells referencing GEOLOGIC_RULES anchors to explain the structure of returned data for single and stacked runs.",
            "dependencies": [
              2
            ],
            "details": "Write markdown callouts per notebook describing gray image semantics, mask keys, and stacked-specific maps, linking to the relevant PRD anchors and noting Task Master tags for traceability.",
            "status": "done",
            "testStrategy": "Render markdown in notebook preview to ensure anchors and descriptions appear correctly.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:44:14.289Z"
          },
          {
            "id": 5,
            "title": "Validate outputs and capture quick visual QA",
            "description": "Execute generator cells end-to-end, confirm array shapes match `params['height']`/`width`, and create inline plots for sanity checks.",
            "dependencies": [
              3,
              4
            ],
            "details": "Run each notebook section, verify gray/mask shapes, compare single vs stacked visuals, and log results for QA readiness per parent task test strategy.",
            "status": "done",
            "testStrategy": "Use deterministic seeds to rerun cells, check `gray.shape` and each mask shape, and visually inspect inline plots for expected differences.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:44:14.300Z"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Create a reusable notebook helper (e.g., `run_fluvial_case(params)`) that imports `analog_image_generator.geologic_generators.generate_fluvial` and `analog_image_generator.stacked_channels.build_stacked_fluvial`, routes to the stacked builder when `params[\"mode\"] == \"stacked\"`, and persists `(gray, masks)` artifacts under `outputs/notebooks/<env>/seed_{seed}/`. Execute the helper for each preset added in Task 2, capture returned masks (channel/levee/scroll_bar/upper_surface_mask/package_id_map/etc.), and describe expected tuples in nearby markdown blocks referencing PRD anchors.",
        "updatedAt": "2025-11-20T18:44:14.300Z"
      },
      {
        "id": 4,
        "title": "Build visualization cells with palette legends and stacked overlays",
        "description": "Render matplotlib grids that combine gray images, mask composites, stacked boundary overlays, and palette legends per PRD requirements.",
        "details": "- Use `matplotlib.pyplot` plus `matplotlib.colors.ListedColormap` with palette definitions from `docs/PALETTES.md` to colorize masks.\n- Add dedicated functions for stacked overlays: `plot_boundary_overlay(gray, masks[\"upper_surface_mask\"], masks[\"package_id_map\"])` returning axes with alpha-blended colors.\n- Embed legends showing palette colors for `channel`, `levee`, `scroll_bar`, stacked boundary IDs, and QA flags.\n- Provide comparison grids showing `single` vs `stacked` outputs side-by-side with consistent axes titles (seed, preset name, anchor IDs).\n- Pseudo-code:\n```\nfrom matplotlib import pyplot as plt\nfrom docs_palettes import CHANNEL_PALETTE\n\nfig, axes = plt.subplots(1, 3, figsize=(14,4))\naxes[0].imshow(gray, cmap=\"gray\")\naxes[1].imshow(masks[\"channel\"], cmap=CHANNEL_PALETTE)\naxes[2].imshow(masks[\"package_id_map\"], cmap=STACKED_ID_CMAP, alpha=0.7)\n```\n- Save snapshots into `outputs/notebooks/...` for inclusion in README/reporting preview sections.",
        "testStrategy": "Run notebook sections to ensure figures render without warnings, verify colorbars/legends match `docs/PALETTES.md`, and capture nbconvert-generated PNGs for review attachments.",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Map palette colors and legend entries",
            "description": "Review palette definitions to ensure every facies and QA flag has an explicit color mapping plus legend label.",
            "dependencies": [],
            "details": "Read `docs/PALETTES.md` and supporting utilities to build a consolidated dictionary covering channel, levee, scroll_bar, stacked boundary IDs, and QA markers that downstream plots can ingest.",
            "status": "done",
            "testStrategy": "Validate the palette map by instantiating each `ListedColormap` and confirming expected hex colors via a quick matplotlib swatch cell.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:48:28.707Z"
          },
          {
            "id": 2,
            "title": "Create grayscale and facies composite plot helpers",
            "description": "Implement reusable matplotlib helpers for displaying the gray analog image and facies mask composites with consistent styling.",
            "dependencies": [
              1
            ],
            "details": "Add functions such as `plot_gray_panel(gray, ax)` and `plot_facies_composite(masks, ax, palettes)` that handle axis labels, aspect ratio, and annotate preset metadata while pulling colormaps from the palette dictionary.",
            "status": "done",
            "testStrategy": "Execute the helpers on a representative `(gray, masks)` tuple inside a notebook cell to ensure they render without warnings and respect the palette entries.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:48:28.718Z"
          },
          {
            "id": 3,
            "title": "Implement stacked boundary overlay renderer",
            "description": "Provide a dedicated function to blend upper-surface and package-id masks over the gray image with legends.",
            "dependencies": [
              1
            ],
            "details": "Write `plot_boundary_overlay(gray, upper_surface_mask, package_id_map, ax)` using alpha-blended `ListedColormap`s, add legend patches for stacked IDs/QA flags, and expose knobs for transparency and boundary outlines.",
            "status": "done",
            "testStrategy": "Run the overlay helper on sample masks with distinct package IDs to confirm colors follow the stacked palette and legends show all entries.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:48:28.729Z"
          },
          {
            "id": 4,
            "title": "Assemble single vs stacked comparison grid builder",
            "description": "Combine the helper panels into a grid figure comparing single-belt and stacked outputs side-by-side with shared annotations.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement a wrapper (e.g., `render_fluvial_comparison(gray_single, masks_single, gray_stacked, masks_stacked, metadata)`) that sets up matplotlib grids, calls the existing helpers, enforces axis titles (seed, preset, anchor IDs), and returns the figure/axes for reuse.",
            "status": "done",
            "testStrategy": "Invoke the comparison builder with deterministic single/stacked cases and verify the resulting figure has aligned axes titles plus legends in each panel.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:48:28.740Z"
          },
          {
            "id": 5,
            "title": "Integrate visualization helpers into notebooks and export snapshots",
            "description": "Wire the new plotting utilities into each fluvial notebook to generate visualization cells and persist PNG outputs.",
            "dependencies": [
              4
            ],
            "details": "Insert cells that call the comparison builder for each preset, annotate GEOLOGIC_RULES anchors in markdown, and save figures under `outputs/notebooks/<env>/` for README/reporting reuse while ensuring Task Master tags are recorded.",
            "status": "done",
            "testStrategy": "Run the updated notebook sections end-to-end, confirm PNG files land in the outputs directory, and spot-check nbconvert previews for correct legends and annotations.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:48:28.751Z"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Leverage `matplotlib.pyplot`, `matplotlib.colors.ListedColormap`, and palette data from `docs/PALETTES.md`/`src/analog_image_generator/utils.py` to build helper functions that render (1) grayscale analogs, (2) facies mask composites, (3) stacked overlays combining `masks[\"upper_surface_mask\"]` and `masks[\"package_id_map\"]`, plus accompanying legends. Wire those helpers into each notebook to show single vs stacked side-by-side grids (annotated with preset names, seeds, and anchor IDs) and save PNG snapshots into `outputs/notebooks/...` for later reporting reuse.",
        "updatedAt": "2025-11-20T18:48:28.751Z"
      },
      {
        "id": 5,
        "title": "Add metrics summary cells invoking stats compute pipelines",
        "description": "Summarize β/D/H, PSD, and QA flags inside notebooks by calling the existing stats utilities and presenting tables/plots.",
        "details": "- Import `analog_image_generator.stats.compute_metrics` and `stats.preview_metrics`; pass `gray`, `masks`, and `env` derived from presets.\n- Display tabular metrics (β_iso vs β_dir, β_seg_x/y, entropy, PSD anisotropy, topology counts) plus QA flags in a pandas DataFrame.\n- Plot PSD anisotropy curves and highlight β/D/H comparisons inline with Matplotlib.\n- Provide in-cell markdown referencing GEOLOGIC_RULES metrics anchors and linking to stats notebook sections.\n- Pseudo-code:\n```\nfrom analog_image_generator import stats\nmetrics = stats.compute_metrics(gray, masks, env=\"fluvial\")\npd.DataFrame([metrics])[metric_columns + qa_columns]\n```\n- Cache metrics dictionaries per realization for reuse by reporting cells.",
        "testStrategy": "Execute metrics cells for each preset, assert expected keys using `assert set(metrics).issuperset({...})`, and run `pytest tests/test_stats.py -k compute_metrics` afterwards to ensure no regressions.",
        "priority": "high",
        "dependencies": [
          "1",
          "2",
          "3",
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Inventory fluvial notebooks and preset contexts for metrics cells",
            "description": "Determine every fluvial notebook and preset realization that requires metrics summaries before coding any cells.",
            "dependencies": [],
            "details": "Open the fluvial notebooks, list the preset runs offered in each one, and note where metrics/QA coverage must appear to satisfy GEOLOGIC_RULES anchors.",
            "status": "done",
            "testStrategy": "Cross-reference the collected notebook list against docs/GEOLOGIC_RULES.md anchors to ensure no environment is missed.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:54:46.702Z"
          },
          {
            "id": 2,
            "title": "Inject compute_metrics/preview_metrics execution blocks per preset",
            "description": "Add reusable code cells that import stats utilities, call compute_metrics and preview_metrics for each preset, and cache their outputs.",
            "dependencies": [
              1
            ],
            "details": "Implement notebook cells that load gray/masks/env values, call analog_image_generator.stats.compute_metrics and stats.preview_metrics, then store the resulting dicts in a cache keyed by preset for downstream reuse.",
            "status": "done",
            "testStrategy": "Execute each new cell to confirm metrics dicts populate without errors and contain expected keys like beta_iso, beta_dir, psd_anisotropy.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:54:46.714Z"
          },
          {
            "id": 3,
            "title": "Render pandas tables covering β/D/H, PSD, topology, QA flags",
            "description": "Format the computed metrics into comprehensive DataFrame tables covering required numeric and QA fields for every preset.",
            "dependencies": [
              2
            ],
            "details": "Use pandas to assemble data rows from cached metrics, include beta_iso vs beta_dir, beta_seg axes, entropy, PSD anisotropy, topology counts, and QA flags, and display them inline with rich formatting.",
            "status": "done",
            "testStrategy": "Spot-check DataFrame columns for completeness and verify values update when rerunning presets.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:54:46.725Z"
          },
          {
            "id": 4,
            "title": "Produce PSD anisotropy plots and β/D/H comparisons with markdown anchors",
            "description": "Add Matplotlib visualizations of PSD curves and β/D/H comparisons accompanied by markdown referencing GEOLOGIC_RULES anchors and stats notebook sections.",
            "dependencies": [
              2,
              3
            ],
            "details": "Generate per-preset PSD anisotropy plots plus overlays highlighting beta vs D vs H relationships, insert markdown cells linking to the correct anchor IDs, and ensure previews run cleanly.",
            "status": "done",
            "testStrategy": "Execute visualization cells to confirm plots render without warnings and markdown links resolve to the documented anchors.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:54:46.736Z"
          },
          {
            "id": 5,
            "title": "Document caching behavior and validate via targeted tests",
            "description": "Explain metrics caching usage in markdown, ensure downstream cells reuse the cached dicts, and run the specified pytest target.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Add markdown describing how cached metrics feed later reporting cells, include lightweight assertions verifying required keys, and finish by running pytest tests/test_stats.py -k compute_metrics to guard against regressions.",
            "status": "done",
            "testStrategy": "Confirm assertions pass within the notebook and run pytest tests/test_stats.py -k compute_metrics successfully.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:54:46.747Z"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Import `analog_image_generator.stats` into every fluvial notebook, call `compute_metrics(gray, masks, env=\"fluvial\")` and `preview_metrics` for each preset realization, then present β/D/H, PSD, topology, and QA flag values in pandas tables plus supporting Matplotlib plots (e.g., PSD anisotropy curves). Reference the relevant GEOLOGIC_RULES metric anchors in markdown, cache the metrics dictionaries for later reporting cells, and finish with `pytest tests/test_stats.py -k compute_metrics` to ensure no regressions.",
        "updatedAt": "2025-11-20T18:54:46.747Z"
      },
      {
        "id": 6,
        "title": "Integrate reporting hooks and artifact previews in notebooks",
        "description": "Expose the reporting pipeline outputs directly inside demo notebooks, linking to smoke artifacts and showing CSV/PDF previews.",
        "details": "- Import `analog_image_generator.reporting.build_reports` and reuse metrics rows produced in Task 5; include metadata such as `env`, `seed`, `task_master_tag`, `stacked_packages`.\n- Create notebook cells that trigger reporting in a scratch directory (e.g., `outputs/notebooks/reporting_demo/`) while also referencing `outputs/smoke_report/` generated by `scripts/smoke_test.py`.\n- Embed PDF/CSV previews via nbconvert-friendly widgets (e.g., display CSV head with pandas, embed PDF using `IPython.display.IFrame`).\n- Provide instructions linking these outputs to the professor-ready artifacts and Task Master tag `fluvial-v1`.\n- Pseudo-code:\n```\nrows = [{\"env\": \"fluvial\", \"seed\": params[\"seed\"], **metrics} for metrics in collected_metrics]\nreporting.build_reports(rows, output_dir=\"outputs/notebooks/reporting_demo\")\ndisplay(pd.read_csv(\"outputs/notebooks/reporting_demo/metrics.csv\").head())\n```\n- Update markdown to explain how to regenerate full smoke artifacts (`python scripts/smoke_test.py`).",
        "testStrategy": "Run notebook cell to invoke `build_reports`, confirm CSV + per-env PDFs + master PDF appear, and manually open resulting files (or embed inline) to validate legibility; then re-run `python scripts/smoke_test.py` to ensure reporting changes remain compatible.",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "3",
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Load cached metrics rows with metadata for reporting",
            "description": "Gather Task 5 metrics rows and augment each record with env, seed, task_master_tag, and stacked_packages metadata before invoking the reporting pipeline.",
            "dependencies": [],
            "details": "Implement notebook cell that imports analog_image_generator.reporting.build_reports, collects cached metrics rows, and ensures every dict includes env, seed, task_master_tag=fluvial-v1, stacked_packages, and any downstream-required metadata prior to reuse.",
            "status": "done",
            "testStrategy": "Run the preparatory cell and inspect a sample row to confirm all metadata keys exist.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:58:13.132Z"
          },
          {
            "id": 2,
            "title": "Trigger build_reports into outputs/notebooks/reporting_demo",
            "description": "Create notebook helper cells that call build_reports with the enriched rows and write artifacts into the scratch outputs/notebooks/reporting_demo directory while referencing smoke outputs.",
            "dependencies": [
              1
            ],
            "details": "Add filesystem-setup cell that ensures outputs/notebooks/reporting_demo exists, then invoke reporting.build_reports(rows, output_dir=...) and note path parity with outputs/smoke_report/ generated by scripts/smoke_test.py for validation.",
            "status": "done",
            "testStrategy": "Execute the reporting cell and verify metrics.csv, per-env PDFs, and master PDF appear in the scratch directory.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:58:13.143Z"
          },
          {
            "id": 3,
            "title": "Embed CSV head and PDF previews inside notebooks",
            "description": "Display inline previews of generated metrics CSV and PDFs using pandas DataFrame head and IPython.display.IFrame, also linking to smoke artifacts for comparison.",
            "dependencies": [
              2
            ],
            "details": "Insert nbconvert-safe visualization cells that read outputs/notebooks/reporting_demo/metrics.csv via pandas head(), embed PDF pages via IPython.display.IFrame, and add text cells referencing outputs/smoke_report/ artifacts for parity checks.",
            "status": "done",
            "testStrategy": "Run notebook rendering cells and confirm CSV table and PDF iframe display correctly when exporting or running interactively.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:58:13.154Z"
          },
          {
            "id": 4,
            "title": "Document regeneration workflow and smoke linkage",
            "description": "Update markdown cells explaining how reporting hooks tie into PRD/Task Master (tag fluvial-v1) and remind readers to rerun python scripts/smoke_test.py to regenerate professor-ready artifacts.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Add instructional markdown covering step-by-step commands, mention Task Master tag context, describe how scratch previews relate to outputs/smoke_report/, and highlight the smoke test command for full artifact regeneration.",
            "status": "done",
            "testStrategy": "Render markdown in the notebook to ensure instructions appear clearly and reference the correct commands and directories.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T18:58:13.166Z"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Use the cached metrics rows from Task 5 to call `analog_image_generator.reporting.build_reports` inside each demo notebook, targeting a scratch folder like `outputs/notebooks/reporting_demo/`. Include metadata columns (`env`, `seed`, `task_master_tag`, `stacked_packages`, etc.), embed CSV previews via pandas `head()` and show PDF snippets (e.g., `IPython.display.IFrame`) while cross-referencing smoke artifacts under `outputs/smoke_report/`. Document how these hooks connect to the PRD/Task Master workflow and remind readers to re-run `python scripts/smoke_test.py`.",
        "updatedAt": "2025-11-20T18:58:13.166Z"
      },
      {
        "id": 7,
        "title": "Add debug hooks, seed overrides, and metadata tables",
        "description": "Provide notebook sections that expose tuning controls (seed overrides, stacked toggles, metadata display) plus a final checklist asserting all PRD features were demonstrated.",
        "details": "- Implement UI helper cells that let users set `CURRENT_SEED`, `STACK_MODE`, and `DEBUG_FLAGS` using ipywidgets toggles or manual variable assignments.\n- Print metadata tables from `masks[\"realization_metadata\"]` (especially `stacked_packages`, `erosion_relief_px`, `package_count`) and expose JSON dumps for Task Master logging.\n- Add cell toggles (can leverage `ipywidgets.Accordion`) to show/hide raw masks, random seeds, and parameter dictionaries.\n- Provide a markdown checklist (matching Task 1) that the engineer can tick after verifying generator/metrics/reporting sections, ensuring compliance with QA definition of done.\n- Pseudo-code:\n```\nimport ipywidgets as widgets\nseed_box = widgets.IntText(value=42, description=\"Seed override\")\nstack_toggle = widgets.ToggleButtons(options=[\"single\",\"stacked\"], description=\"Mode\")\ndisplay(seed_box, stack_toggle)\n```\n- Document how to forward overrides into the generator helper from Task 3 before execution.",
        "testStrategy": "Interactively change seeds/modes in the notebook and ensure `run_fluvial_case` re-runs with the overrides; verify metadata tables update accordingly and the checklist references all toggles; capture screenshots for README updates.",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit notebooks for required override hooks",
            "description": "Review each fluvial demo notebook to locate where seed/stack/mode overrides and debug controls should live before adding UI cells.",
            "dependencies": [],
            "details": "Catalog existing generator helper cells, note how Task 3’s helper is invoked, and document where CURRENT_SEED, STACK_MODE, and DEBUG_FLAGS need to be injected so later steps can wire ipywidgets correctly.\n<info added on 2025-11-20T19:15:19.713Z>\nNotebook audit summary: `notebooks/fluvial_meandering.ipynb` cell 24 now resolves `.taskmaster/tasks/tasks.json` by walking `Path.cwd()` parents before loading `tasks_snapshot`, so the Task Master snapshot cell is root-safe and ready for widget-driven logging; cells 26/22/24 in the meandering/braided/anastomosing notebooks already centralize `OVERRIDE_STATE[\"seed\"]`, `[\"stack_mode\"]`, and shared `DEBUG_FLAGS` plus `render_metadata_table()`/`build_debug_accordion()` helpers, so those cells are the injection points where the upcoming `CURRENT_SEED`/`STACK_MODE`/`DEBUG_FLAGS` ipywidgets will write overrides before calling `run_fluvial_override`; the trailing “Debug + QA checklist” markdown cells (indices 28/24/26 in the respective notebooks) remain unchanged and are where we’ll surface the new override + metadata tasks once the widgets are wired, keeping the Task 7 checklist requirements visible.\n</info added on 2025-11-20T19:15:19.713Z>",
            "status": "done",
            "testStrategy": "Open notebooks to confirm all relevant sections were inventoried and annotated for upcoming insertions.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:15:24.858Z"
          },
          {
            "id": 2,
            "title": "Implement ipywidget controls for overrides",
            "description": "Create notebook cells introducing IntText/Toggle widgets (or fallback variables) that update CURRENT_SEED, STACK_MODE, and DEBUG_FLAGS ahead of generator execution.",
            "dependencies": [
              1
            ],
            "details": "Build reusable UI helper code using widgets.IntText, ToggleButtons, and Checkbox or dictionary editors, ensure values propagate into the generator helper from Task 3 via explicit assignments, and document how to pass overrides when re-running runs.\n<info added on 2025-11-20T19:16:27.528Z>\nImplemented shared “Debug + seed override controls” cells in `notebooks/fluvial_meandering.ipynb`, `notebooks/fluvial_braided.ipynb`, and `notebooks/fluvial_anastomosing.ipynb` that define `OVERRIDE_STATE`/`DEBUG_FLAGS`, reuse `run_fluvial_override()` (routing to `analog_image_generator.geologic_generators.generate_fluvial` vs `stacked_channels.build_stacked_fluvial`), and draw ipywidgets dropdown + ToggleButtons + IntText controls so the Run override button (or `_run_override`) updates `OVERRIDE_STATE` before re-running the helper while exposing single vs stacked package toggles, seed overrides, and a widgets-is-None fallback for manual editing.\n</info added on 2025-11-20T19:16:27.528Z>",
            "status": "done",
            "testStrategy": "Interactively change widget values, rerun the generator helper, and confirm override variables update the rendered realization.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:16:32.538Z"
          },
          {
            "id": 3,
            "title": "Display realization metadata tables and JSON dumps",
            "description": "Add cells that pull masks[\"realization_metadata\"] to present stacked package summaries and serialized payloads for Task Master logging.",
            "dependencies": [
              2
            ],
            "details": "Format tables highlighting stacked_packages, package_count, erosion_relief_px, and produce accompanying JSON output (e.g., via json.dumps) saved/printed so engineers can copy logs.\n<info added on 2025-11-20T19:17:25.402Z>\nIn notebooks/fluvial_meandering.ipynb, notebooks/fluvial_braided.ipynb, and notebooks/fluvial_anastomosing.ipynb the `_run_override` callback now always invokes `render_metadata_table(masks, params)` so every override prints the pandas summary row (preset, stack_mode, seed, package_count, erosion_relief_px, qa_flags), any stacked package DataFrames, and the JSON dump via `json.dumps(metadata, indent=2)` for Task Master capture, and `build_debug_accordion` adds a metadata `widgets.Output` pane that mirrors the same JSON so accordion logs include package_count/erosion_relief_px/qa flags alongside previews and parameter dictionaries.\n</info added on 2025-11-20T19:17:25.402Z>",
            "status": "done",
            "testStrategy": "Run cells after generator execution to verify tables populate with current run data and JSON reflects the same metadata.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:17:31.975Z"
          },
          {
            "id": 4,
            "title": "Add accordion toggles for raw masks and parameter dictionaries",
            "description": "Introduce ipywidgets.Accordion or similar toggles allowing users to reveal raw mask visualizations, random seed readouts, and parameter dictionaries on demand.",
            "dependencies": [
              3
            ],
            "details": "Create accordion sections with children outputs (plots, pprint data) wired to existing mask arrays/params, ensuring toggles render cleanly and don’t re-run heavy computations unnecessarily.\n<info added on 2025-11-20T19:18:28.448Z>\n`build_debug_accordion()` in `notebooks/fluvial_meandering.ipynb`, `notebooks/fluvial_braided.ipynb`, and `notebooks/fluvial_anastomosing.ipynb` instantiates three `widgets.Output` containers that call `_plot_masks(analog, masks, f\"{STYLE_LABEL} override preview\")`, `pprint(params)`, and `json.dumps(masks.get(\"realization_metadata\", {}), indent=2)` before passing them to `widgets.Accordion([...])`, giving per-override toggles for raw mask previews, parameter dictionaries, and metadata payloads without rerunning the expensive plotting code.\n</info added on 2025-11-20T19:18:28.448Z>",
            "status": "done",
            "testStrategy": "Expand/collapse each accordion to confirm contents display correctly and remain synchronized with the current realization data.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:18:34.625Z"
          },
          {
            "id": 5,
            "title": "Embed standardized QA checklist markdown at notebook end",
            "description": "Append the Task 1-compliant QA checklist markdown so engineers can confirm generators, metrics, reporting, and logging features were demonstrated.",
            "dependencies": [
              4
            ],
            "details": "Reuse the checklist template defined under Task 1, ensure items cover seed overrides, stack toggles, metadata table, debug accordions, and provide instructions for ticking items post-validation.\n<info added on 2025-11-20T19:19:17.022Z>\nAdded a closing “### Debug + QA checklist” markdown cell to `notebooks/fluvial_meandering.ipynb`, `notebooks/fluvial_braided.ipynb`, and `notebooks/fluvial_anastomosing.ipynb` that reuses the Task 1 template so QA must capture the seed override + stack toggle screenshot, log the `masks[\"realization_metadata\"]` table output into `.taskmaster/tasks/tasks.json`, expand the debug accordion to inspect raw mask/mode parameter dictionaries, rerun `python scripts/validate_geo_anchors.py` and `python scripts/smoke_test.py`, and only tick the “checked into git” box once those validations pass before calling the Task Master subtask done.\n</info added on 2025-11-20T19:19:17.022Z>",
            "status": "done",
            "testStrategy": "Render notebook preview to verify the checklist mirrors Task 1, includes all new features, and can be manually checked off during QA review.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:19:22.312Z"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Introduce ipywidget-based controls (or explicit override variables) for `CURRENT_SEED`, `STACK_MODE`, and `DEBUG_FLAGS`, weave them into the generator helper from Task 3 before execution, and surface tables sourced from `masks[\"realization_metadata\"]` (especially stacked package summaries). Provide toggles/accordions to show raw masks, parameter dictionaries, and JSON payloads for Task Master logging, and end each notebook with the standardized QA checklist from Task 1 so engineers can tick off the demonstrated features.",
        "updatedAt": "2025-11-20T19:19:22.312Z"
      },
      {
        "id": 8,
        "title": "Embed interactive panel & slider overrides in `v20a_interactive_rebuild.ipynb`",
        "description": "Enhance the interactive notebook to showcase `interactive.build_interactive_ui` usage, slider overrides, and preview exports per PRD.",
        "details": "- Import `analog_image_generator.interactive.build_interactive_ui`, `build_sliders`, `preview_sequence`, and `run_param_batch`.\n- Demonstrate constructing the UI panel via code, overriding slider values programmatically (e.g., `panel.slider_controls[\"drift_fraction\"].value = 0.15`).\n- Show how to plug slider outputs into `preview_sequence` to render preview mosaics plus placeholder β/D/H metrics and how to queue `run_param_batch` for batch exports tied to Task Master tags.\n- Include markdown referencing anchors `notebooks/v20a_interactive_rebuild.ipynb#anchor-fluvial-preview-sequence` and GEOLOGIC_RULES interactive entries.\n- Pseudo-code:\n```\nfrom analog_image_generator import interactive\npanel = interactive.build_interactive_ui(\"fluvial\")\npanel.slider_controls[\"package_count\"].value = 3\npreview = interactive.preview_sequence(\"fluvial\", panel.slider_state(), seeds=[42,77])\npreview.display()\n```\n- Add debug cell pointing to slider schema JSON for reuse in Task Master automation.",
        "testStrategy": "Run notebook cells to ensure ipywidgets render, `preview_sequence` returns expected dataclass, and that slider overrides propagate to generated previews; run `pytest tests/test_interactive.py` afterwards to confirm module invariants still hold.",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Confirm interactive imports and notebook anchors",
            "description": "Open the v20a_interactive_rebuild notebook and ensure it imports build_interactive_ui, build_sliders, preview_sequence, and run_param_batch while locating the anchor sections that need updates.",
            "dependencies": [],
            "details": "Audit the notebook cells, insert missing imports from analog_image_generator.interactive at the top, and outline where anchor references to GEOLOGIC_RULES.md and notebook anchors must appear before modifying any UI code.\n<info added on 2025-11-20T19:28:05.843Z>\nInserted a top-level code cell in notebooks/v20a_interactive_rebuild.ipynb that explicitly imports build_sliders, build_interactive_ui, preview_sequence, and run_param_batch from analog_image_generator.interactive immediately after the GEOLOGIC_RULES anchor markdown (docs/GEOLOGIC_RULES.md#anchor-fluvial-interactive-ui), ensuring SLIDER_LIBRARY/build_interactive_ui initialization and the later preview_sequence/run_param_batch cells resolve dependencies before UI wiring.\n</info added on 2025-11-20T19:28:05.843Z>",
            "status": "done",
            "testStrategy": "Run the initial cells to confirm imports succeed without NameError and anchors resolve in markdown preview.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:28:12.503Z"
          },
          {
            "id": 2,
            "title": "Instantiate fluvial interactive panel and show slider overrides",
            "description": "Create a code cell that builds the fluvial control panel and sets example slider overrides like drift_fraction and package_count before rendering the panel.",
            "dependencies": [
              1
            ],
            "details": "Use interactive.build_interactive_ui(\"fluvial\") to get the panel, expose slider_controls dict access, programmatically assign new values, and ensure the widget render reflects overrides with explanatory comments for Task Master readers.\n<info added on 2025-11-20T19:29:05.214Z>\nCode cell in notebooks/v20a_interactive_rebuild.ipynb now calls interactive_panel = interactive.build_interactive_ui(\"fluvial\"), maps slider widgets into SLIDER_CONTROLS, sets SLIDER_CONTROLS[\"drift_fraction\"].value = 0.18 and SLIDER_CONTROLS[\"package_count\"].value = 3, flips interactive_panel.widgets[\"mode\"] to \"stacked\" with interactive_panel.widgets[\"package_mix\"] = (\"Meandering\", \"Braided\", \"Anastomosing\"), and re-renders interactive_panel.layout with a Markdown summary so reviewers can trace Task Master toggle overrides directly to the visible sliders.\n</info added on 2025-11-20T19:29:05.214Z>",
            "status": "done",
            "testStrategy": "Execute the cell to confirm the panel renders with updated slider values and no widget errors are thrown.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:29:10.684Z"
          },
          {
            "id": 3,
            "title": "Render preview mosaics and placeholder metrics from slider state",
            "description": "Demonstrate capturing panel.slider_state(), calling preview_sequence with sample seeds, and displaying previews alongside stub β/D/H metrics that reflect the override values.",
            "dependencies": [
              2
            ],
            "details": "Add cells that call interactive.preview_sequence with the fluvial slider state, iterate returned mosaics, display them, and compute placeholder metrics (e.g., from masks metadata) to show how UI outputs feed into reporting hooks.\n<info added on 2025-11-20T19:30:02.279Z>\nNotebook cell in notebooks/v20a_interactive_rebuild.ipynb now snapshots slider_state(panel) via current_generator_params into base_params before launching interactive.preview_sequence for paired single and stacked seed lists, iterates the returned frames to display both mosaics, and accumulates preview_records annotated with TASK_TAG=\"fluvial-v1-demo\" so preview_df captures the β/entropy/fractal placeholders QA expects from the override flow.\n</info added on 2025-11-20T19:30:02.279Z>",
            "status": "done",
            "testStrategy": "Run the preview cell to ensure preview_sequence executes, returns expected structures, and displayed metrics update when slider overrides change.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:30:09.303Z"
          },
          {
            "id": 4,
            "title": "Queue run_param_batch exports tagged fluvial-v1",
            "description": "Show how to pass slider-derived parameter dictionaries into run_param_batch with Task Master metadata including the fluvial-v1 tag and optional export paths.",
            "dependencies": [
              3
            ],
            "details": "Create an example list of runs using slider_state plus manual tweaks, invoke interactive.run_param_batch with tag metadata, and log/print queued export info so notebook readers know how to launch batch processing.\n<info added on 2025-11-20T19:31:20.085Z>\nNotebook cell “Queue run_param_batch exports…” inside `notebooks/v20a_interactive_rebuild.ipynb` now calls `interactive.run_param_batch(\"fluvial\", SLIDER_LIBRARY, seeds=[311, 313], output_dir=Path(\"outputs/interactive_batch/fluvial-v1-demo\"), style=current_state[\"style\"], mode=current_state.get(\"mode\", \"single\"), extra_params=current_state)` so the queued batch reuses the live `slider_state` overrides; the cell displays “Saved 2 previews to outputs/interactive_batch/fluvial-v1-demo” plus a pandas table of the analog/color PNG paths per seed, giving Task Master reviewers a concrete `fluvial-v1-demo` tag example that ties the batch exports and earlier TASK_TAG logging together.\n</info added on 2025-11-20T19:31:20.085Z>",
            "status": "done",
            "testStrategy": "Execute the batch cell (with dry-run safeguards if available) to ensure run_param_batch accepts inputs without errors and confirms queued jobs in the notebook output.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:31:25.785Z"
          },
          {
            "id": 5,
            "title": "Document anchors and expose slider schema debug cell",
            "description": "Add markdown referencing notebooks/v20a_interactive_rebuild anchors and GEOLOGIC_RULES entries plus a code cell dumping slider schema JSON for automation reuse.",
            "dependencies": [
              3,
              4
            ],
            "details": "Insert markdown tying the implemented principles to anchor-fluvial-preview-sequence and relevant GEOLOGIC_RULES keys, and add a debug cell that loads or constructs slider schema JSON (e.g., via build_sliders) and prints it for Task Master automation hooks.\n<info added on 2025-11-20T19:34:19.978Z>\nDocumented the anchor tie-in inside `notebooks/v20a_interactive_rebuild.ipynb` so the markdown block for `anchor-fluvial-preview-sequence` explicitly references the UX rows in `docs/GEOLOGIC_RULES.md` mapped to `analog_image_generator.interactive.build_interactive_ui(...)` and `interactive.preview_sequence(...)`, making it clear this notebook cell is the canonical reference for those anchors. Added a debug/export cell in the same notebook that reuses `SLIDER_LIBRARY = interactive.build_sliders(\"fluvial\")` (see `src/analog_image_generator/interactive.py:428`) to build a `schema_export` dict of group labels and slider definitions, writes it to `outputs/interactive_batch/fluvial_slider_schema.json`, and prints the resulting path so Task Master automation hooks can ingest the slider schema artifact.\n</info added on 2025-11-20T19:34:19.978Z>",
            "status": "done",
            "testStrategy": "Render the markdown in Jupyter to verify anchor links, execute the debug cell to ensure the schema JSON prints/exports correctly for downstream processes.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:34:25.203Z"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Enhance `notebooks/v20a_interactive_rebuild.ipynb` to import `analog_image_generator.interactive.build_interactive_ui`, `build_sliders`, `preview_sequence`, and `run_param_batch`; instantiate the panel, demonstrate programmatic slider overrides (e.g., tweak `drift_fraction`, `package_count`), and show how slider states feed `preview_sequence` mosaics plus `run_param_batch` exports tagged with `fluvial-v1`. Add markdown linking to the appropriate GEOLOGIC_RULES anchors and include a debug cell exposing the slider schema JSON for automation hooks.",
        "updatedAt": "2025-11-20T19:34:25.203Z"
      },
      {
        "id": 9,
        "title": "Expand stats notebook with PSD anisotropy and β_dir vs β_iso comparisons",
        "description": "Update `notebooks/fluvial_stats.ipynb` to run multiple realizations through `stats.compute_metrics`, visualize PSD anisotropy, and compare directional metrics as required.",
        "details": "- Add loop generating several realizations (baseline meander, extreme meander, stacked) and collect metrics via `compute_metrics`.\n- Plot PSD power spectra slices along dir vs iso orientation, annotate β_dir vs β_iso differences, and include discussion referencing PRD requirements.\n- Visualize β vs D vs H scatter for the runs and highlight QA flags.\n- Provide summary tables comparing Phase 1 vs Phase 2 metrics, ensuring coverage of PSD anisotropy, variograms, topology, and stacked metadata rows.\n- Pseudo-code:\n```\nresults = []\nfor preset in (baseline_meander, extreme_meander, stacked_presets):\n    gray, masks = run_fluvial_case(preset)\n    metrics = stats.compute_metrics(gray, masks, env=\"fluvial\")\n    results.append({\"preset\": preset[\"name\"], **metrics})\nplot_psd_anisotropy(results)\n```\n- Cross-link new sections to GEOLOGIC_RULES anchors and note how outputs feed reporting.",
        "testStrategy": "Execute the stats notebook top-to-bottom, verifying PSD plots render and `compute_metrics` returns consistent values; rerun `pytest tests/test_stats.py` plus targeted `python -m pytest tests -k psd` (if available) to ensure no regressions.",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "3",
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate preset execution loop in fluvial_stats notebook",
            "description": "Add notebook cells looping through baseline, extreme, and stacked presets using the Task 3 run helper to gather gray images and masks.",
            "dependencies": [],
            "details": "Create a results accumulator, call run_fluvial_case(params) for each preset, capture seeds/metadata, and store gray plus masks for downstream metrics and visualizations while referencing GEOLOGIC_RULES anchors in markdown.\n<info added on 2025-11-20T19:39:43.938Z>\nNotebook cell 8 in notebooks/fluvial_stats.ipynb now defines BASELINE_MEANDER, EXTREME_SINUOSITY, and STACKED_PACKAGE_SWEEP presets sourced from gg._MEANDER_DEFAULTS, cell 9 adds run_fluvial_case() that toggles gg.generate_fluvial vs sc.build_stacked_fluvial while capturing metadata/variograms/PSD, and cell 10 loops PRESET_DEFINITIONS so RUN_RESULTS collects the three realizations before the metrics_df aggregation drives ag_stats.compute_metrics for the PSD/β comparisons.\n</info added on 2025-11-20T19:39:43.938Z>",
            "status": "done",
            "testStrategy": "Execute the new loop cell to confirm three realizations complete without errors and persist intermediate outputs for later cells.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:39:50.882Z"
          },
          {
            "id": 2,
            "title": "Collect stats.compute_metrics payloads into structured table",
            "description": "Invoke stats.compute_metrics for each realization and normalize the returned metrics into a pandas DataFrame or list of dicts for easy plotting.",
            "dependencies": [
              1
            ],
            "details": "Iterate through loop outputs, call stats.compute_metrics(gray, masks, env=\"fluvial\"), merge preset names, Phase tags, PSD/variogram/topology fields, and stacked metadata, then cache both JSON-like structures and tabular form for reuse across plots and summary tables.\n<info added on 2025-11-20T19:40:45.594Z>\nLoop over the three RUN_RESULTS entries (baseline_meander, extreme_meander, stacked_packages) in notebooks/fluvial_stats.ipynb, call ag_stats.compute_metrics for each, append dictionaries that merge preset, mode, and inferred Phase labels with every flattened payload from src/analog_image_generator/stats.py (beta_iso, beta_dir_*, beta_seg*, h0, psd_aspect/theta, entropy_global, fractal_dimension, topology_* fields, qa_* flags, stacked_package_count), and hydrate a single `metrics_df = pd.DataFrame.from_records(metrics_rows)` that downstream β_dir vs β_iso plots, PSD views, QA summaries, and exports all reference; keep both metrics_rows and metrics_df cached in notebook scope (and optional JSON dump) so later cells never recompute metrics.\n</info added on 2025-11-20T19:40:45.594Z>",
            "status": "done",
            "testStrategy": "Add quick asserts in the notebook verifying expected metric keys exist per row and display the head of the aggregated table to ensure correct formatting.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:40:52.195Z"
          },
          {
            "id": 3,
            "title": "Visualize PSD anisotropy with β_dir vs β_iso comparisons",
            "description": "Build PSD plotting cell(s) that slice spectra along directional vs isotropic orientations and annotate β_dir minus β_iso differences per preset.",
            "dependencies": [
              2
            ],
            "details": "Use matplotlib/plotly helpers to overlay PSD curves for direction/isotropic spectra, annotate beta values, add markdown interpreting anisotropy behavior relative to PRD requirements, and link to the proper GEOLOGIC_RULES anchors.\n<info added on 2025-11-20T19:41:40.946Z>\nDocumented that `notebooks/fluvial_stats.ipynb#anchor-fluvial-variogram` now caches `variogram = ag_stats.compute_variogram(configure_gray(analog), VARIO_DIRECTIONS)` within `run_fluvial_case`, and the plotting cell immediately after the `metrics_df` build overlays each preset’s isotropic vs directional curves with per-panel `β_iso` text pulled from `metrics_df.loc[...]` so PSD anisotropy context is shown next to the spectra. Added the follow-on delta section that melts the `beta_dir_*` columns into `long_df`, computes `delta = beta_dir - beta_iso`, and draws the β_dir−β_iso line chart per preset, giving explicit annotations of the anisotropy deltas before the QA scatter discussion.\n</info added on 2025-11-20T19:41:40.946Z>",
            "status": "done",
            "testStrategy": "Render the PSD figures in the notebook, verifying annotations appear and that curve legends distinguish presets and orientations clearly.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:41:46.858Z"
          },
          {
            "id": 4,
            "title": "Plot β/D/H scatter highlighting QA flags",
            "description": "Produce scatter/ternary-style visualizations relating β, fractal dimension D, and Hurst exponent H, flagging any runs with QA issues and noting stacked metadata.",
            "dependencies": [
              2
            ],
            "details": "Create a plotting cell that encodes QA flag state via color/markers, overlays stacked vs single-belt presets, and includes inline markdown discussing observed trends and how QA flags should be interpreted for reporting feeds.\n<info added on 2025-11-20T19:42:21.052Z>\nAdded a β_iso vs fractal_dimension scatter cell in notebooks/fluvial_stats.ipynb where metrics_df rows are colored by qa_psd_anisotropy_warning, preset names are annotated directly on the points to distinguish stacked versus single runs, and the inline table metrics_df[[preset, h0, qa_psd_anisotropy_warning, qa_channel_area_warning]] surfaces the H intercept and QA columns so reviewers can correlate β/D/H and flag states without leaving the plot context.\n</info added on 2025-11-20T19:42:21.052Z>",
            "status": "done",
            "testStrategy": "Display the scatter plot and confirm tooltips/legends highlight QA-flagged runs; spot-check values against the aggregated metrics table.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:42:28.641Z"
          },
          {
            "id": 5,
            "title": "Summarize Phase 1 vs Phase 2 metrics and validate notebook",
            "description": "Add tables comparing Phase 1 and Phase 2 metrics (PSD, variograms, topology, stacked metadata), cross-link sections to GEOLOGIC_RULES anchors, and validate the notebook plus stats tests.",
            "dependencies": [
              3,
              4
            ],
            "details": "Use pandas styling or markdown tables to contrast metrics per phase, add commentary tying outputs to anchors and reporting requirements, then run the notebook end-to-end followed by pytest tests/test_stats.py (and PSD-focused subsets) to ensure reproducibility.\n<info added on 2025-11-20T19:56:43.066Z>\nAdded a “Phase 1 vs Phase 2 summary” section in notebooks/fluvial_stats.ipynb that builds phase_group = metrics_df.groupby(\"phase\")[beta_iso, psd_aspect, entropy_global, fractal_dimension, topology_channel_area_fraction].agg([\"mean\", \"std\"]) with anchors noted (anchor-fluvial-metrics, anchor-fluvial-variogram, anchor-fluvial-psd-topology) and validation commentary; executed the stats notebook end-to-end and exported metrics_df to outputs/stats/fluvial-demo/fluvial_metrics_table.csv for reporting/Task Master ingestion.\n</info added on 2025-11-20T19:56:43.066Z>",
            "status": "done",
            "testStrategy": "Execute the entire notebook without interruption, export updated outputs if needed, and run `pytest tests/test_stats.py` (plus `python -m pytest tests -k psd` if available) to confirm changes introduce no regressions.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T19:56:48.449Z"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Update `notebooks/fluvial_stats.ipynb` to iterate over multiple presets (baseline, extreme, stacked) using the run helper from Task 3, call `stats.compute_metrics` for each, and visualize PSD anisotropy curves plus β_dir vs β_iso comparisons. Produce β/D/H scatter plots, highlight QA flags, and summarize Phase 1 vs Phase 2 metrics in tables tied back to GEOLOGIC_RULES anchors. Validate by executing the notebook end-to-end and running `pytest tests/test_stats.py` (and any PSD-focused subsets).",
        "updatedAt": "2025-11-20T19:56:48.449Z"
      },
      {
        "id": 10,
        "title": "Update reporting notebook, documentation, and automation hooks",
        "description": "Refresh `notebooks/reporting.ipynb`, README, docs/WORKFLOW, docs/CODEX_RUNBOOK, and meeting recap to reflect the new demo workflow while enforcing automation requirements.",
        "details": "- Enhance `notebooks/reporting.ipynb` to demonstrate building `metrics_rows`, calling `reporting.build_reports`, and previewing CSV/PDF artifacts with notes about `outputs/smoke_report/` parity.\n- Update `README.md` with demo screenshots/instructions, `docs/WORKFLOW.md` with a demo-day checklist, and `docs/CODEX_RUNBOOK.md` with pre-demo commands (run pytest, geo anchor validation, smoke test, Task Master notes).\n- Add a meeting recap entry (using `docs/MEETING_RECAP_TEMPLATE.md`) summarizing readiness.\n- Embed automation reminders in docs (and optionally notebook markdown) to rerun `pytest`, `python scripts/validate_geo_anchors.py`, and `python scripts/smoke_test.py` after edits; wire these commands into a `make demo-check` helper if needed.\n- Pseudo-code for automation cell:\n```\n!pytest -q\n!python scripts/validate_geo_anchors.py\n!python scripts/smoke_test.py --fast\n```\n- Ensure updates reference Task Master tag `fluvial-v1` for traceability.",
        "testStrategy": "Execute the reporting notebook to regenerate artifacts, run the documented automation commands locally (pytest, validate_geo_anchors, smoke_test) to confirm they succeed, and review documentation diffs for accuracy.",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit reporting notebook baseline and smoke artifacts",
            "description": "Open `notebooks/reporting.ipynb` plus `outputs/smoke_report/` to capture current metrics, report cells, and automation notes that must be refreshed for the new workflow.",
            "dependencies": [],
            "details": "Review existing notebook sections, note where metrics_rows are built, confirm how `analog_image_generator.reporting.build_reports` is (or isn’t) invoked, and document required updates so later edits can reference Task Master tag `fluvial-v1`.\n<info added on 2025-11-20T20:08:01.597Z>\nAudit complete: notebooks/reporting.ipynb now carries the anchors block (anchor-reporting-pipeline/anchor-reporting-mosaics referencing analog_image_generator.reporting.build_reports) plus a demo-ready checklist, and smoke artifacts are confirmed in outputs/smoke_report/ (csv + per-env PDFs + master PDF) for baseline parity.\n</info added on 2025-11-20T20:08:01.597Z>",
            "status": "done",
            "testStrategy": "Manually step through the current notebook to ensure cells execute and note any failing cells before modification.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T20:08:11.281Z"
          },
          {
            "id": 2,
            "title": "Implement refreshed metrics_rows and `build_reports` demo cells",
            "description": "Revise `notebooks/reporting.ipynb` to build metrics_rows, call `reporting.build_reports`, preview CSV/PDF outputs, and mention `outputs/smoke_report/` parity plus automation commands.",
            "dependencies": [
              1
            ],
            "details": "Add code and markdown cells showing the metrics construction pipeline, invocation of `analog_image_generator.reporting.build_reports`, and inline previews (CSV head, PDF embed) while referencing `fluvial-v1` for traceability and highlighting pseudo-code automation commands.\n<info added on 2025-11-20T20:11:40.471Z>\nAdd to notebooks/reporting.ipynb: construct fresh metrics_rows for both a baseline meander and stacked mixture using gg.generate_fluvial vs gg.build_stacked_fluvial with build_row (ag_stats.compute_metrics → reporting.CSV_COLUMNS, env fluvial), then call reporting.build_reports(metrics_rows, Path(\"outputs/reporting_demo/fluvial-v1-demo\")), print the artifacts manifest, and display both a CSV head and embedded PDF preview; include a markdown cell summarizing artifact metadata/anchors (e.g., anchor-reporting-pipeline/anchor-reporting-mosaics) and tag the demo explicitly as fluvial-v1 for traceability.\n</info added on 2025-11-20T20:11:40.471Z>",
            "status": "done",
            "testStrategy": "Run the updated notebook end-to-end to confirm artifacts are generated in the demo directory and inspect CSV/PDF previews render without errors.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T20:11:46.903Z"
          },
          {
            "id": 3,
            "title": "Update README and WORKFLOW docs with demo guidance",
            "description": "Refresh `README.md` and `docs/WORKFLOW.md` to include new demo screenshots/instructions, a demo-day checklist, Task Master references, and explicit automation reminders for pytest, anchor validation, and smoke tests.",
            "dependencies": [
              2
            ],
            "details": "Edit both documents so the narrative matches the revised reporting notebook, highlight commands (`pytest`, `python scripts/validate_geo_anchors.py`, `python scripts/smoke_test.py`), and ensure every new section cites the `fluvial-v1` tag plus mentions `outputs/smoke_report/`.\n<info added on 2025-11-20T20:12:16.927Z>\nRevise docs/WORKFLOW.md reporting exports to require running `jupyter nbconvert --to notebook --inplace --execute notebooks/reporting.ipynb` so `outputs/reporting_demo/fluvial-v1-demo/` demo artifacts regenerate before attaching refreshed CSV/PDF deliverables (cite fluvial-v1 and note smoke_report parity).\n</info added on 2025-11-20T20:12:16.927Z>",
            "status": "done",
            "testStrategy": "Review rendered Markdown (via markdown preview or `mdformat --check` if available) and manually verify all command snippets render correctly and reference the right files.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T20:12:24.166Z"
          },
          {
            "id": 4,
            "title": "Revise docs/CODEX_RUNBOOK with automation hooks",
            "description": "Expand `docs/CODEX_RUNBOOK.md` to list pre-demo commands, Task Master notes, and automation pseudo-code while reinforcing re-run requirements for pytest, anchor validation, and smoke tests.",
            "dependencies": [
              2
            ],
            "details": "Add a pre-demo checklist, embed the provided pseudo-code snippet, ensure `fluvial-v1` tagging is explicit, and mention the tentative `make demo-check` helper so operators have a single command path.\n<info added on 2025-11-20T20:13:06.617Z>\nAdded Codex-Max demo-ready automation block in docs/CODEX_RUNBOOK.md covering `python -m pytest`, `python scripts/validate_geo_anchors.py`, `python scripts/smoke_test.py`, and `jupyter nbconvert --to notebook --inplace --execute notebooks/reporting.ipynb`, reinforcing fluvial-v1 tagging and the optional `make demo-check` single-command path before demos.\n</info added on 2025-11-20T20:13:06.617Z>",
            "status": "done",
            "testStrategy": "Lint or preview the runbook file to ensure formatting remains consistent (e.g., `markdownlint` if available) and double-check command blocks copy-paste cleanly.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T20:13:19.584Z"
          },
          {
            "id": 5,
            "title": "Create latest meeting recap from template",
            "description": "Copy `docs/MEETING_RECAP_TEMPLATE.md` to a dated recap, summarize demo readiness, automation steps, and references to updated artifacts while naming Task Master tag `fluvial-v1`.",
            "dependencies": [
              3,
              4
            ],
            "details": "Populate sections covering context, decisions, action items, and automation reminders (pytest, validate_geo_anchors, smoke_test) so stakeholders see the refreshed workflow and reporting outputs.\n<info added on 2025-11-20T20:13:46.127Z>\nAdded meeting recap entry in docs/MEETING_RECAP_2025-11-20.md capturing fluvial demo readiness (Tasks 1–9 done, Task 10 docs/reporting still in-flight), Codex default set to gpt-5.1-codex-max, reporting artifacts paths at outputs/reporting_demo/fluvial-v1-demo/ and outputs/smoke_report/, and remaining doc/automation tasks (close Task 10, attach CSV/PDFs, rerun pytest + scripts/validate_geo_anchors.py + scripts/smoke_test.py + nbconvert on notebooks/reporting.ipynb).\n</info added on 2025-11-20T20:13:46.127Z>",
            "status": "done",
            "testStrategy": "Spell-check or read-through the recap to confirm all required sections are filled and links/paths point to the correct notebook/docs entries.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T20:13:54.894Z"
          },
          {
            "id": 6,
            "title": "Document automation helper and verification steps",
            "description": "Embed automation reminders across docs/notebook and add (or update) a `make demo-check` helper so rerunning pytest, geo anchor validation, and smoke tests is streamlined, ensuring all references mention `fluvial-v1`.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Update relevant Makefile or documentation snippets to describe the helper target, confirm notebook markdown mentions the commands, and ensure links back to Task Master automation requirements and `outputs/smoke_report/` parity.\n<info added on 2025-11-20T20:15:03.625Z>\nAdded a demo-check helper note spanning the Makefile and docs/CODEX_RUNBOOK.md with the exact chain pytest -q → python scripts/validate_geo_anchors.py → python scripts/smoke_test.py (outputs/smoke_report) → jupyter nbconvert --to notebook --inplace --execute notebooks/reporting.ipynb, and mirrored the requirement inside reporting.ipynb markdown so fluvial-v1 demo presenters must run it before sharing artifacts and stay aligned with Task Master automation expectations.\n</info added on 2025-11-20T20:15:03.625Z>",
            "status": "done",
            "testStrategy": "Invoke `make demo-check` (or dry-run if new) to ensure the commands chain correctly, or manually run listed commands to validate instructions match actual behavior.",
            "parentId": "undefined",
            "updatedAt": "2025-11-20T20:15:11.193Z"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Refresh `notebooks/reporting.ipynb` to showcase building `metrics_rows` and invoking `analog_image_generator.reporting.build_reports`, then update `README.md`, `docs/WORKFLOW.md`, `docs/CODEX_RUNBOOK.md`, and the latest meeting recap (copy `docs/MEETING_RECAP_TEMPLATE.md`) with the new demo workflow, automation checklist, and Task Master references. Document commands such as `pytest`, `python scripts/validate_geo_anchors.py`, `python scripts/smoke_test.py`, and optionally add a `make demo-check` helper. Ensure every update references the `fluvial-v1` tag for traceability and aligns with the reporting artifacts saved under `outputs/smoke_report/`.",
        "updatedAt": "2025-11-20T20:15:11.193Z"
      },
      {
        "id": 11,
        "title": "Overhaul interactive sliders with real-time preview",
        "description": "Expose all fluvial parameters as sliders with debounced live preview; add status/busy UI; ensure stacked/single parity and quick metrics refresh.",
        "details": "Implement: 1) Expand build_sliders(fluvial) to include all major geologic params (meander/braided/anasto + stacked); 2) Refactor v20a_interactive_rebuild.ipynb UI cell to render full slider panel with debounced on-change preview (gray+facies), quick metrics (preview_metrics), metadata/QA table, status label; 3) Add batch helper button for run_param_batch seeds; 4) Update anchors/docs (GEOLOGIC_RULES, notebook markdown, runbook) with new UI behavior; 5) Add a smoke-ish test to ensure slider config and preview calls work; 6) Run validation loop (pytest, validate_geo_anchors, smoke_test, nbconvert).",
        "testStrategy": "",
        "status": "in-progress",
        "dependencies": [],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-11-20T20:53:24.061Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-11-20T20:53:24.066Z",
      "taskCount": 11,
      "completedCount": 10,
      "tags": [
        "fluvial-v1-demo"
      ],
      "created": "2025-11-20T20:57:25.499Z",
      "description": "Tasks for fluvial-v1-demo context",
      "updated": "2025-11-20T20:57:25.499Z"
    }
  }
}